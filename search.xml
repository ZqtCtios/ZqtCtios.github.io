<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>02_逻辑回归_Logistic_Regression</title>
      <link href="2020/12/03/machinelearning/wu-en-da-ji-qi-xue-xi/logistic-regression/"/>
      <url>2020/12/03/machinelearning/wu-en-da-ji-qi-xue-xi/logistic-regression/</url>
      
        <content type="html"><![CDATA[<h1 id="逻辑回归（Logistic-Regression）"><a href="#逻辑回归（Logistic-Regression）" class="headerlink" title="逻辑回归（Logistic Regression）"></a>逻辑回归（Logistic Regression）</h1><h2 id="1-分类问题（Classification）"><a href="#1-分类问题（Classification）" class="headerlink" title="1.分类问题（Classification）"></a>1.分类问题（Classification）</h2><p>分类问题中，需要预测的变量y是离散的值</p><h2 id="2-假说表示"><a href="#2-假说表示" class="headerlink" title="2.假说表示"></a>2.假说表示</h2><p>逻辑回归模型的假设是: $\quad h_{\theta}(x)=g\left(\theta^{T} X\right)$ 其中: $\quad X$ 代表特征向量 $g$ 代表逻辑函数（logistic function)是一个常用的逻辑函数为 S 形函数(Sigmoid function)，公式为:</p><script type="math/tex; mode=display">g(z)=\frac{1}{1+e^{-z}}</script><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np <span class="token keyword">def</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>z<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">return</span> <span class="token number">1</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>z<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>$h_{\theta}(x)$ 的作用是，对于给定的输入变量，根据选择的参数计算输出变量=1 的可能性(estimated probablity) 即 $h_{\theta}(x)=P(y=1 \mid x ; \theta)$<br>例如，如果对于给定的 $x,$ 通过已经确定的参数计算得出 $h_{\theta}(x)=0.7,$ 则表示有 $70 \%$ 的几率 $y$ 为正向类，相应地 $y$ 为负向类的几率为 1-0.7=0.3。</p><h2 id="3-判定边界Decision-Boundary"><a href="#3-判定边界Decision-Boundary" class="headerlink" title="3.判定边界Decision Boundary"></a>3.判定边界Decision Boundary</h2><p>现在假设我们有一个模型</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201203163952.png" alt="image-20201203163952519"></p><p>并且参数 $\theta$ 是向量[-3,1,1]。 则当-3 $+x_{1}+x_{2} \geq 0,$ 即 $x_{1}+x_{2} \geq 3$ 时，模型将预测 $y=1$。 我们可以绘制直线 $x_{1}+x_{2}=3$, 这条线便是我们模型的分界线，将预测为 1 的区域和预测为 0 的区域分隔开。</p><h2 id="4-代价函数-Cost-Function"><a href="#4-代价函数-Cost-Function" class="headerlink" title="4 代价函数 Cost Function"></a>4 代价函数 Cost Function</h2><p> 定义用来 拟合参数的优化目标或者叫代价函数，这便是监督学习问题中的逻辑回归模型的拟合问题.</p><p>对于线性回归模型，我们定义的代价函数是所有模型误差的平方和。理论上来说，我们也可以对逻辑回归模型沿用这个定义，但是问题在于，当我们将 $h_{\theta}(x)=\frac{1}{1+e^{-\theta^{T} X}}$ 带入到这样定义了的代价函数中时，我们得到的代价函数将是一个非凸函数（non-convexfunction）</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201204160007.png" alt="image-20201204160007361"></p><p>这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。<br>线性回归的代价函数为:</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{m} \sum_{i=1}^{m} \frac{1}{2}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}</script><p>我们重新定义逻辑回归的代价函数为: </p><script type="math/tex; mode=display">J(\theta)=\frac{1}{m} \sum_{i=1}^{m} \operatorname{cost}\left(h_{\theta}\left(x^{(i)}\right), y^{(i)}\right)</script><p>其中:</p><script type="math/tex; mode=display">\operatorname{cost}\left(h_{\theta}(x), y\right)=\left\{\begin{aligned}-\log \left(h_{\theta}(x)\right) & \text { if } y=1 \\-\log \left(1-h_{\theta}(x)\right) & \text { if } y=0\end{aligned}\right.</script><p>$h_{\theta}(x)$ 与 $\operatorname{cost}\left(h_{\theta}(x), y\right)$ 之间的关系如下图所示:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201204160831.png" alt="image-20201204160831708"></p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201205002159.png" alt="Snipaste_2020-12-05_00-21-41"></p>]]></content>
      
      
      <categories>
          
          <category> 吴恩达——机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 吴恩达 </tag>
            
            <tag> 逻辑回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>01_线性回归_Linear_Regression</title>
      <link href="2020/12/02/machinelearning/wu-en-da-ji-qi-xue-xi/linear-regression/"/>
      <url>2020/12/02/machinelearning/wu-en-da-ji-qi-xue-xi/linear-regression/</url>
      
        <content type="html"><![CDATA[<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>回归本质上是利用离散的值来试着推测出一个连续值的结果，对其他值进行预测</p><h2 id="1-单变量线性回归（Linear-Regression-with-One-Variable）"><a href="#1-单变量线性回归（Linear-Regression-with-One-Variable）" class="headerlink" title="1 单变量线性回归（Linear Regression with One Variable）"></a>1 单变量线性回归（Linear Regression with One Variable）</h2><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201202232210.png" alt="image-20201202232210402"></p><h3 id="1-1-例子房价预测问题："><a href="#1-1-例子房价预测问题：" class="headerlink" title="1.1 例子房价预测问题："></a>1.1 例子房价预测问题：</h3><p>假使回归问题的训练集(Training Set)如下表所示</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201202232330.png" alt="image-20201202232330802"></p><p>我们将要用来描述这个回归问题的标记如下: </p><ul><li><p>$𝑚$代表训练集中实例的数量</p></li><li><p>$𝑥$ 代表特征/输入变量</p></li><li>$𝑦$ 代表目标变量/输出变量</li><li>$(𝑥,𝑦)$ 代表训练集中的实例</li><li>$(𝑥^{(𝑖)},𝑦^{(𝑖)})$代表第$𝑖$ 个观察实例</li><li>$h$ 代表学习算法的解决方案或函数也称为假设(hypothesis)</li></ul><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201202232830.png" alt="image-20201202232830677"></p><p>我们该如何表达 h?</p><p>一种可能的表达方式为:$h_𝜃(𝑥) = 𝜃_0 + 𝜃_1𝑥$，因为只含有一个特征/输入变量，因此这样 的问题叫作单变量线性回归问题。</p><h3 id="1-2-代价函数"><a href="#1-2-代价函数" class="headerlink" title="1.2 代价函数"></a>1.2 代价函数</h3><h4 id="误差定义："><a href="#误差定义：" class="headerlink" title="误差定义："></a>误差定义：</h4><p>预测值与实际值的的差距。$h_𝜃(x)-y$</p><h4 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h4><p>对于$h_𝜃(𝑥) = 𝜃_0 + 𝜃_1𝑥$，目的是找到两个变量$𝜃_0,𝜃_1$来使得整体误差的值最小，即误差平方和最小，代价公式为：</p><script type="math/tex; mode=display">MSE=J\left(\theta_{0}, \theta_{1}\right)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}</script><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201202233929.png" alt="image-20201202233929481"></p><h3 id="1-3-梯度下降"><a href="#1-3-梯度下降" class="headerlink" title="1.3 梯度下降"></a>1.3 梯度下降</h3><p>批量梯度下降(batch gradient descent):</p><script type="math/tex; mode=display">\begin{aligned}&\text { repeat until convergence \{}\\&\theta_{j}:=\theta_{j}-\alpha \frac{\partial}{\partial \theta_{j}} J\left(\theta_{0}, \theta_{1}\right) \quad(\text { for } j=0 \text { and } j=1)\\&\text{\}}\end{aligned}</script><p>其中$𝑎$是学习率(learning rate)，它决定了我们沿着能让代价函数下降程度最大的方向 向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数，参数更新的步骤如下</p><script type="math/tex; mode=display">\begin{array}{l}\text { temp } 0:=\theta_{0}-\alpha \frac{\partial}{\partial \theta_{0}} J\left(\theta_{0}, \theta_{1}\right) \\\text { templ }:=\theta_{1}-\alpha \frac{\partial}{\partial \theta_{1}} J\left(\theta_{0}, \theta_{1}\right) \\\theta_{0}:=\text { temp } 0 \\\theta_{1}:=\text { temp } 1\end{array}</script><ul><li>$a$太小收敛速度太慢</li><li>$a$太大可能不收敛</li></ul><h3 id="1-4-梯度下降的线性回归"><a href="#1-4-梯度下降的线性回归" class="headerlink" title="1.4 梯度下降的线性回归"></a>1.4 梯度下降的线性回归</h3><script type="math/tex; mode=display">\begin{array}{c}\frac{\partial}{\partial \theta_{j}} J\left(\theta_{0}, \theta_{1}\right)=\frac{\partial}{\partial \theta_{j}} \frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2} \\j=0 \text { 时: } \frac{\partial}{\partial \theta_{0}} J\left(\theta_{0}, \theta_{1}\right)=\frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) \\j=1 \text { 时: } \quad \frac{\partial}{\partial \theta_{1}} J\left(\theta_{0}, \theta_{1}\right)=\frac{1}{m} \sum_{i=1}^{m}\left(\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) \cdot x^{(i)}\right)\end{array}</script><h2 id="2-多变量线性回归-Linear-Regression-with-Multiple-Variables"><a href="#2-多变量线性回归-Linear-Regression-with-Multiple-Variables" class="headerlink" title="2 多变量线性回归(Linear Regression with Multiple Variables)"></a>2 多变量线性回归(Linear Regression with Multiple Variables)</h2><p>对上面的模型增加更多的特征，模型中的特征为$(x_1,x_2,….,x_n)$</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201203163630.png" alt="image-20201203150333545"></p><p>增添更多特征后，我们引入一系列新的注释:</p><ul><li><p>$𝑛$ 代表特征的数量</p></li><li><p>$𝑥^{(𝑖)}$代表第 $𝑖$ 个训练实例，是特征矩阵中的第$𝑖$行，是一个向量(vector)。</p></li><li><p>比方说，上图的</p><script type="math/tex; mode=display">x^{(2)}=\left[\begin{array}{c}1416 \\3 \\2 \\40\end{array}\right]</script></li><li><p>$𝑥^{(𝑖)}_j$代表特征矩阵中第$ 𝑖$ 行的第$𝑗$个特征，也就是第 𝑖 个训练实例的第 𝑗 个特征。</p></li><li><p>如上图的$\mid x_{2}^{(2)}=3, x_{3}^{(2)}=2$</p></li><li><p>支持多变量的假设 h 表示为:$h_{\theta}(x)=\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}+\ldots+\theta_{n} x_{n}$</p></li><li><p>这个公式中有𝑛 + 1个参数和𝑛个变量，为了使得公式能够简化一些，引入$𝑥_0 = 1$，则公 式转化为:$h_{\theta}(x)=\theta_{0} x_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}+\ldots+\theta_{n} x_{n}$</p></li><li><p>此时模型中的参数是一个$𝑛 + 1$维的向量，任何一个训练实例也都是𝑛 + 1维的向量，特 征矩阵𝑋的维度是 $𝑚 ∗ (𝑛 + 1)$。 因此公式可以简化为:$h_𝜃(𝑥) = 𝜃^𝑇𝑋$，其中上标𝑇代表矩阵转置。</p></li></ul><h3 id="2-1-多变量梯度下降Gradient-Descent-for-Multiple-Variables"><a href="#2-1-多变量梯度下降Gradient-Descent-for-Multiple-Variables" class="headerlink" title="2.1 多变量梯度下降Gradient Descent for Multiple Variables"></a>2.1 多变量梯度下降Gradient Descent for Multiple Variables</h3><p>与单变量线性回归类似，在多变量线性回归中，我们也构建一个代价函数，则这个代价 函数是所有建模误差的平方和，即:</p><script type="math/tex; mode=display">J\left(\theta_{0}, \theta_{1} \ldots \theta_{n}\right)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}</script><p>梯度下降算法求导后：</p><p>Repeat：</p><script type="math/tex; mode=display">\theta_{j}:=\theta_{j}-\alpha \frac{1}{m} \sum_{i=1}^{m}\left(\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) \cdot x_{j}^{(i)}\right)</script><p>( simultaneously update  $\theta_{j}$$ \text { for } \mathrm{j}=0,1, \ldots, \mathrm{n})$</p><p>计算Cost函数Python代码：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">computeCost</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> theta<span class="token punctuation">)</span><span class="token punctuation">:</span>  inner <span class="token operator">=</span> np<span class="token punctuation">.</span>power<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">(</span>X <span class="token operator">*</span> theta<span class="token punctuation">.</span>T<span class="token punctuation">)</span> <span class="token operator">-</span> y<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>   <span class="token keyword">return</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>inner<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="2-2-梯度下降法实践-1-特征缩放"><a href="#2-2-梯度下降法实践-1-特征缩放" class="headerlink" title="2.2 梯度下降法实践 1-特征缩放"></a>2.2 梯度下降法实践 1-特征缩放</h3><p>Gradient Descent in Practice I - Feature Scaling</p><p>在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛，解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201203153342.png" alt="image-20201203153342751"></p><p>最简单的方法是令:$x_{n}=\frac{x_{n}-\mu_{n}}{s_{n}},$ 其中 $\mu_{n}$ 是平均值, $s_{n}$ 是标准差。</p><h3 id="2-3-梯度下降法实践-2-学习率"><a href="#2-3-梯度下降法实践-2-学习率" class="headerlink" title="2.3 梯度下降法实践 2-学习率"></a>2.3 梯度下降法实践 2-学习率</h3><p>Gradient Descent in Practice II - Learning Rate</p><p>梯度下降算法的每次迭代受到学习率的影响，如果学习率𝑎过小，则达到收敛所需的迭 代次数会非常高;如果学习率𝑎过大，每次迭代可能不会减小代价函数，可能会越过局部最 小值导致无法收敛。</p><h3 id="2-4-特征和多项式回归"><a href="#2-4-特征和多项式回归" class="headerlink" title="2.4 特征和多项式回归"></a>2.4 特征和多项式回归</h3><p>线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，比如一个二次方模型: $h_{\theta}(x)=\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}^{2}$<br>或者三次方模型: $\quad h_{\theta}(x)=\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}^{2}+\theta_{3} x_{3}^{3}$</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201203155639.png" alt="image-20201203155639868"></p><p>通常我们需要先观察数据然后再决定准备尝试怎样的模型。 另外，我们可以令：$x_{2}=x_{2}^{2}, x_{3}=x_{3}^{3},$ 从而将模型转化为线性回归模型。</p><p>从而将模型转化为线性回归模型。</p><h3 id="2-5-正规方程"><a href="#2-5-正规方程" class="headerlink" title="2.5 正规方程"></a>2.5 正规方程</h3><p>正规方程是通过求解下面的方程来找出使得代价函数最小的参数的 $： \frac{\partial}{\partial \theta_{j}} J\left(\theta_{j}\right)=0$，假设我们的训练集特征矩阵为 X（包含了 $x_{0}=1$ ）并且我们的训练集结果为向量 y，则利用正规方程解出向量 $\theta=\left(X^{T} X\right)^{-1} X^{T} y$ 。上标 T 代表矩阵转置，上标-1 代表矩阵的逆。设矩阵 $A=X^{T} X,$ 则: $\left(X^{T} X\right)^{-1}=A^{-1}$</p><p> 梯度下降与正规方程的比较:</p><div class="table-container"><table><thead><tr><th style="text-align:left">梯度下降</th><th>正规方程</th></tr></thead><tbody><tr><td style="text-align:left">需要选择学习率𝛼</td><td>不需要</td></tr><tr><td style="text-align:left">需要多次迭代</td><td>一次运算得出</td></tr><tr><td style="text-align:left">当特征数量𝑛大时也能较好适用</td><td>需要计算 $\left(X^{T} X\right)^{-1}$ 如果特征数量 $n$ 较大则运算代价大, 因为矩阵逆的计算时间复杂度 为 $O\left(n^{3}\right),$ 通常来说当n小于10000 时还是 可以接受的</td></tr><tr><td style="text-align:left">适用于各种类型的模型</td><td>只适用于线性模型，不适合逻辑回归模型等 其他模型</td></tr></tbody></table></div><p>正规方程的python实现：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np <span class="token keyword">def</span> <span class="token function">normalEqn</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>  theta <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>inv<span class="token punctuation">(</span>X<span class="token punctuation">.</span>T@X<span class="token punctuation">)</span>@X<span class="token punctuation">.</span>T@y <span class="token comment">#X.T@X 等价于 X.T.dot(X) </span>  <span class="token keyword">return</span> theta<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 吴恩达——机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 吴恩达 </tag>
            
            <tag> 线性回归 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
