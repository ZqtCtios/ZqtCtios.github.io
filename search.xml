<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>04_深度学习的实践层面（Practical aspects of Deep Learning）</title>
      <link href="2020/12/16/machinelearning/andrew-ng-deeplearning/04-practical-aspects-of-deep-learning/"/>
      <url>2020/12/16/machinelearning/andrew-ng-deeplearning/04-practical-aspects-of-deep-learning/</url>
      
        <content type="html"><![CDATA[<h1 id="深度学习的实践层面（Practical-aspects-of-Deep-Learning）"><a href="#深度学习的实践层面（Practical-aspects-of-Deep-Learning）" class="headerlink" title="深度学习的实践层面（Practical aspects of Deep Learning）"></a>深度学习的实践层面（Practical aspects of Deep Learning）</h1><h2 id="1-训练，验证，测试集（Train-Dev-Test-sets）"><a href="#1-训练，验证，测试集（Train-Dev-Test-sets）" class="headerlink" title="1.训练，验证，测试集（Train / Dev / Test sets）"></a>1.训练，验证，测试集（Train / Dev / Test sets）</h2><p>在机器学习发展的小数据量时代，常见做法是将所有数据三七分，就是人们常说的 70% 验证集，30%测试集，如果没有明确设置验证集，也可以按照 60%训练，20%验证和 20%测试集来划分。这是前几年机器学习领域普遍认可的最好的实践方法。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/image-20201216145010169.png" alt="data set"></p><p>如果只有 100 条，1000 条或者 1 万条数据，那么上述比例划分是非常合理的。</p><p>但是在大数据时代，我们现在的数据量可能是百万级别，那么验证集和测试集占数据总量的比例会趋向于变得更小。因为验证集的目的就是验证不同的算法，检验哪种算法更有效， 因此，验证集要足够大才能评估，比如 2 个甚至10 个不同算法，并迅速判断出哪种算法更 有效。我们可能不需要拿出 20%的数据作为验证集。</p><p>比如我们有 100 万条数据，那么取 1 万条数据便足以进行评估，找出其中表现最好的 1-2 种算法。同样地，根据最终选择的分类器，测试集的主要目的是正确评估分类器的性能，所以，如果拥有百万数据，我们只需要 1000 条数据，便足以评估单个分类器，并且准确评估该分类器的性能。</p><p>假设我们有100 万条数据，其中 1 万条作为验证集，1 万条作为测试集， 100 万里取 1万，比例是 1%，即:训练集占 98%，验证集和测试集各占 1%。对于数据量过百万的应用，训练集可以占到 99.5%，验证和测试集各占 0.25%，或者验证集占 0.4%，测试集占 0.1%。</p><p>总结一下，在机器学习中，我们通常将样本分成训练集，验证集和测试集三部分，数据 集规模相对较小，适用传统的划分比例，数据集规模较大的，验证集和测试集要小于数据总量的 20%或 10%。</p><h2 id="2-偏差，方差（Bias-Variance）"><a href="#2-偏差，方差（Bias-Variance）" class="headerlink" title="2.偏差，方差（Bias /Variance）"></a>2.偏差，方差（Bias /Variance）</h2><p>分析在训练集上训练算法产生的误差和验证集上验证算法产生的误差来诊断算法是否存在高偏差和高方差</p><h2 id="3-正则化（Regularization）"><a href="#3-正则化（Regularization）" class="headerlink" title="3.正则化（Regularization）"></a>3.正则化（Regularization）</h2><p>深度学习可能存在过拟合问题——高方差，有两个解决方法，一个是正则化，另一个是准备更多的数据。</p><p>逻辑回归CostFunction：</p><script type="math/tex; mode=display">J(w, b)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)</script><p>L1正则化：</p><script type="math/tex; mode=display">J(w, b)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)+\frac{\lambda}{m}\sum_{j=1}^{n_{x}}|w|</script><p>L2正则化：</p><script type="math/tex; mode=display">J(w, b)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)+\frac{\lambda}{2 m}\left\|W\right\|^{2}</script><p>$\frac{\lambda}{2 m}$ 乘以 $w$ 范数的平方， w欧几里德范数的平方等于 $w_{j}\left(j\right.$ 值从 1 到 $\left.n_{x}\right)$ 平方的和，也可表示为 $w^{T} w$, 也就是向量参数 $w$ 的欧几里德范数（2 范数）的平方，此方法称为L2正则化。因为这里用了欧几里德法线，被称为向量参数w的L2范数。L2正则化是最常见的正则化类型。</p><h3 id="如何在神经网络中实现𝐿2正则化"><a href="#如何在神经网络中实现𝐿2正则化" class="headerlink" title="如何在神经网络中实现𝐿2正则化"></a>如何在神经网络中实现𝐿2正则化</h3><p>神经网络含有一个成本函数，该函数包含 $W^{[1]}, b^{[1]}$ 到 $W^{[l]}, b^{[l]}$ 所有参数，字母L是神经网络所含的层数，因此成本函数等于 $m$个训练样本损失函数的总和乘以$\frac{1}{m}$，正则项为$\frac{\lambda}{2 m} \sum_{1}^{L}\left|W^{[l]}\right|^{2},$ 我们称$\left|W^{[l]}\right|^{2}$ 为范数平方，这个矩阵范数 $\left|W^{[l]}\right|^{2}$ (即平方范数 $),$ 被定义为矩阵中所有元素的平方求和。</p><p>该矩阵范数被称作“弗罗贝尼乌斯范数”，用下标𝐹标注，鉴于线性代数中一些神秘晦涩的原因，我们不称之为“矩阵𝐿2范数”，而称它为“弗罗贝尼乌斯范数”，矩阵𝐿2范数听起来更自然，但鉴于一些大家无须知道的特殊原因，按照惯例，我们称之为“弗罗贝尼乌斯范数”， 它表示一个矩阵中所有元素的平方和。</p><h3 id="该如何使用该范数实现梯度下降"><a href="#该如何使用该范数实现梯度下降" class="headerlink" title="该如何使用该范数实现梯度下降"></a>该如何使用该范数实现梯度下降</h3><p>用 backprop 计算出 $d W$ 的值, backprop 会给出J对 $W$ 的偏导数，实际上是 $W^{[l]},$ 把 $W^{[l]}$替换为 $W^{[l]}$ 减 去 学 习 率 乘 以 $d W$。</p><p>这就是之前我们额外增加的正则化项，既然已经增加了这个正则项，现在我们要做的就是给 $d W$ 加上这一项 $\frac{1}{m}W^{[l]},$ 然后计算这个更新项，使用新定义的 $d W^{[l]},$ 它的定义含有相关参数代价函数导数和，以及最后添加的额外正则项, 这也是L2正则化有时被称为“权重衰减”的原因。</p><p>我们用 $d W^{[l]}$ 的定义替换此处的 $d W^{[l]},$ 可以看到， $W^{[l]}$ 的定义被更新为 $W^{[l]}$ 减 去 学 习 率 $a$ 乘以 backprop 再加上 $\frac{1}{m}W^{[l]} 。$</p><p>该正则项说明，不论 $W^{[l]}$ 是什么，我们都试图让它变得更小，实际上，相当于我们给矩阵 $\mathrm{W}$ 乘以 $\left(1-a \frac{\lambda}{m}\right)$ 倍的权重，矩阵 $W$ 减去 $\alpha \frac{\lambda}{m}$ 倍的它，也就是用这个系数 $\left(1-a \frac{\lambda}{m}\right)$ 乘以矩阵W，该系数小于 1，因此L2范数正则化也被称为“权重衰减”，因为它就像一般的梯度下降,$W$ 被更新为少了 $a$ 乘以 backprop 输出的最初梯度值, 同时 $W$ 也乘以了这个系数，这个系数小于 1，因此L2正则化也被称为“权重衰减”。</p><h3 id="为什么正则化有利于预防过拟合"><a href="#为什么正则化有利于预防过拟合" class="headerlink" title="为什么正则化有利于预防过拟合"></a>为什么正则化有利于预防过拟合</h3><p>为什么正则化有利于预防过拟合呢?为什么它可以减少方差问题?我们通过两个例子来直观体会一下。左图是高偏差，右图是高方差，中间是 Just Right，这几张图我们在前面课程中看到过。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/image-20201216154639378.png" alt="samples"></p><p>直观理解就是𝜆增加到足够大，𝑊会接近于 0，实际上是不会发生这种情况的，我们尝试消除或至少减少许多隐藏单元的影响，最终这个网络会变得更简单，这个神经网络越来越 接近逻辑回归，我们直觉上认为大量隐藏单元被完全消除了，其实不然，实际上是该神经网 络的所有隐藏单元依然存在，但是它们的影响变得更小了。神经网络变得更简单了，貌似这样更不容易发生过拟合，因此我不确定这个直觉经验是否有用，不过在编程中执行正则化时， 你实际看到一些方差减少的结果。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/image-20201216155326921.png" alt="samples"></p><h2 id="4-dropout-正则化（DropoutRegularization）"><a href="#4-dropout-正则化（DropoutRegularization）" class="headerlink" title="4.dropout 正则化（DropoutRegularization）"></a>4.dropout 正则化（DropoutRegularization）</h2><p>除了𝐿2正则化，还有一个非常实用的正则化方法——“Dropout（随机失活）”，我们来看看它的工作原理。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/image-20201216155512971.png" alt="过拟合网络"></p><p>假设你在训练上图这样的神经网络，它存在过拟合，这就是 dropout 所要处理的，我们 复制这个神经网络，dropout 会遍历网络的每一层，并设置消除神经网络中节点的概率。假 设网络中的每一层，每个节点都以抛硬币的方式设置概率，每个节点得以保留和消除的概率 都是 0.5，设置完节点概率，我们会消除一些节点，然后删除掉从该节点进出的连线，最后 得到一个节点更少，规模更小的网络，然后用 backprop 方法进行训练。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/image-20201216155557069.png" alt="dropout"></p><p>如何实施 dropout 呢?方法有几种，接下来我要讲的是最常用的方法，即 inverted dropout（反向随机失活），出于完整性考虑，我们用一个三层$(𝑙 = 3)$网络来举例说明。 编码中会有很多涉及到 的地方。我只举例说明如何在某一层中实施 dropout。首先要定义向量$𝑑$，$𝑑^{[3]}$表示一个三层的 dropout 向量:</p><p>然后看它是否小于某数，我们称之为 keep-prob，keep-prob 是一个具体数字，上个示例中它是 0.5，而本例中它是 0.8，它表示保留某个隐藏单元的概率，此处 keep-prob 等于0.8， 它意味着消除任意一个隐藏单元的概率是0.2，它的作用就是生成随机矩阵，如果对$𝑎^{[3]}$进行因子分解，效果也是一样的。$𝑑^{[3]}$是一个矩阵，每个样本和每个隐藏单元，其中$𝑑^{[3]}$中的对 值为 1 的概率都是0.8，对应为0的概率是0.2，随机数字小于0.8。它等于 1 的概率是 0.8， 等于 0 的概率是 0.2。</p><p>接下来要做的就是从第三层中获取激活函数，这里我们叫它$𝑎^{[3]}$，$𝑎^{[3]}$含有要计算的激活 函数，$𝑎^{[3]}$等于上面的$𝑎^{[3]}$乘以$𝑑^{[3]}$，a3 =np.multiply(a3,d3)，这里是元素相乘，也可写 为𝑎3*= 𝑑3，它的作用就是让$𝑑^{[3]}$中所有等于 0 的元素(输出)，而各个元素等于 0 的概率只有 20%，乘法运算最终把$𝑑^{[3]}$中相应元素输出，即让$𝑑^{[3]}$中 0 元素与$a^{[3]}$中相对元素归零。</p><p>如果用 python 实现该算法的话，$𝑑^{[3]}$则是一个布尔型数组，值为 true 和 false，而不是 1 和 0，乘法运算依然有效，python 会把 true 和 false 翻译为 1 和 0</p><h2 id="5-其他正则化方法-Other-regularization-methods"><a href="#5-其他正则化方法-Other-regularization-methods" class="headerlink" title="5.其他正则化方法(Other regularization methods)"></a>5.其他正则化方法(Other regularization methods)</h2><ol><li><p>数据扩增</p><p>例如：随意翻转和裁剪图片，我们可以增大数据集，额外生成假训练数据。和全新的，独立的猫咪图片数据相比，这些额外的假的数据无法包含像全新数据那么多的信息，但我们这 么做基本没有花费，代价几乎为零，除了一些对抗性代价。以这种方式扩增算法数据，进而正则化数据集，减少过拟合比较廉价。</p></li><li><p>early stopping</p></li><li><p>Momentum</p></li><li><p>RMSprop </p></li><li><p>Adam</p></li></ol><h2 id="6-归一化输入-Normalizing-inputs"><a href="#6-归一化输入-Normalizing-inputs" class="headerlink" title="6.归一化输入(Normalizing inputs)"></a>6.归一化输入(Normalizing inputs)</h2><p>训练神经网络，其中一个加速训练的方法就是归一化输入。假设一个训练集有两个特征， 输入特征为 2 维，归一化需要两个步骤:</p><p>1.第一步是零均值化， $\mu=\frac{1}{m} \sum_{i=1}^{m} x^{(i)},$ 它是一个向量， $x$ 等于每个训练数据 $x$ 减去 $\mu,$ 意思是移动训练集，直到它完成零均值化。</p><p>2.第二步是归一化方差，注意特征 $x_{1}$ 的方差比特征 $x_{2}$ 的方差要大得多，我们要做的是给 $\sigma$赋值， $\sigma^{2}=\frac{1}{m} \sum_{i=1}^{m}\left(x^{(i)}\right)^{2},$ 这是节点 $y$ 的平方， $\sigma^{2}$ 是一个向量，它的每个特征都有方差，注意，我们已经完成零值均化, $\left(x^{(i)}\right)^{2}$ 元素 $y^{2}$ 就是方差，我们把所有数据除以向量 $\sigma^{2}$, 最后变成上图形式。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/image-20201216164125855.png" alt="Normalizing inputs"></p><h2 id="7-梯度消失-梯度爆炸（Vanishing-Exploding-gradients）"><a href="#7-梯度消失-梯度爆炸（Vanishing-Exploding-gradients）" class="headerlink" title="7. 梯度消失/梯度爆炸（Vanishing / Exploding gradients）"></a>7. 梯度消失/梯度爆炸（Vanishing / Exploding gradients）</h2><p>训练神经网络，尤其是深度神经所面临的一个问题就是梯度消失或梯度爆炸，也就是你训练神经网络的时候，导数或坡度有时会变得非常大，或者非常小，甚至于以指数方式变小， 这加大了训练的难度。</p><p>假设你正在训练这样一个极深的神经网络，这个神经网络会有参数 $W^{[1]}, W^{[2]}, W^{[3] } $等 等 ， 直 到$W^{[l]},$ 为了简单起见，假设我们使用激活函数 $g(z)=z$，也就是线性激活函数，我们忽略 $b$，假设 $b^{[l]}=0$， 如果那样的话，输出:</p><script type="math/tex; mode=display">y=W^{[l]} W^{[L-1]} W^{[L-2]} \ldots W^{[3]} W^{[2]} W^{[1]} x</script><p> $W^{[1]} x=z^{[1]},$ 因为 $b=0,$ 所以我想 $z^{[1]}=W^{[1]} x, a^{[1]}=g\left(z^{[1]}\right)$，因为我们使用了一个线性激活函数，它等于 $z^{[1]},$ 所以第一项 $W^{[1]} x=a^{[1]},$ 通过推理，你会得出 $W^{[2]} W^{[1]} x=a^{[2]},$ 因为 $a^{[2]}=g\left(z^{[2]}\right),$ 还等于 $g\left(W^{[2]} a^{[1]}\right),$ 可以用 $W^{[1]} x$ 替换$a^{[1]},$ 所以这一项就等于 $a^{[2]},$ 这个就是 $a^{[3]}\left(W^{[3]} W^{[2]} W^{[1]} x\right)_{\circ}$</p><p>所有这些矩阵数据传递的协议将给出𝑦而不是𝑦的值。</p><p>假设每个权重矩阵 $W^{[l]}=\left[\begin{array}{cc}1.5 &amp; 0 \ 0 &amp; 1.5\end{array}\right],$ 从技术上来讲，最后一项有不同维度，可能它就是余下的权重矩阵， $y=W^{[1]}\left[\begin{array}{cc}1.5 &amp; 0 \ 0 &amp; 1.5\end{array}\right]^{(L-1)} x,$ 因为我们假设所有矩阵都等于它，它是 1.5倍的单位矩阵，最后的计算结果就是金, $\hat{y}$ 也就是等于1.5 $^{(L-1)} x_{\circ}$ 如果对于一个深度神经网络来说L值较大，那么的值也会非常大，实际上它呈指数级增长的，它增长的比率是1.5 $^{L},$ 因此对于一个深度神经网络，𝑦的值将爆炸式增长。</p><p>相反的，如果权重是 0.5, $W^{[l]}=\left[\begin{array}{cc}0.5 &amp; 0 \ 0 &amp; 0.5\end{array}\right],$ 它比 1 小，这项也就变成了 $0.5^{L},$ 矩阵 $y=W^{[1]}\left[\begin{array}{cc}0.5 &amp; 0 \ 0 &amp; 0.5\end{array}\right]^{(L-1)} x,$ 再次忽略 $W^{[L]},$ 因此每个矩阵都小于 $1,$ 假设 $x_{1}$ 和 $x_{2}$ 都是 $1,$ 激活函数将变成 $\frac{1}{2}, \frac{1}{2}, \frac{1}{4}, \frac{1}{4}, \frac{1}{8}, \frac{1}{8}$ 等，直到最后一项变成 $\frac{1}{2^{L}},$ 所以作为自定义函数，激活函数的值将以指数级下降, 它是与网络层数数量L相关的函数, 在深度网络中, 激活函数以指数级递减。权重W只比 1 略大一点，或者说只是比单位矩阵大一点,深度神经网络的激活函数将爆炸式增长，如果W比 1 略小一点，可能是 $\left[\begin{array}{cc}0.9 &amp; 0 \ 0 &amp; 0.9\end{array}\right]$ 。</p><h2 id="8-神经网络的权重初始化（Weight-Initialization-for-Deep-Networks）"><a href="#8-神经网络的权重初始化（Weight-Initialization-for-Deep-Networks）" class="headerlink" title="8.神经网络的权重初始化（Weight Initialization for Deep Networks）"></a>8.神经网络的权重初始化（Weight Initialization for Deep Networks）</h2><p>单个神经元可能有 4 个输入特征，从 $x_{1}$ 到 $x_{4},$ 经过 $a=g(z)$ 处理，最终得到金，稍后讲深度网络时，这些输入表示为 $a^{[l]},$ 暂时我们用x表示。<br>$z=w_{1} x_{1}+w_{2} x_{2}+\cdots+w_{n} x_{n}, b=0,$ 暂时忽略 $b,$ 为了预防z值过大或过小，你可以看到n越大，你希望 $w_{i}$ 越小，因为z是 $w_{i} x_{i}$ 的和，如果你把很多此类项相加，希望每项值更小,最合理的方法就是设置 $w_{i}=\frac{1}{n}, n$ 表示神经元的输入特征数量，实际上，你要做的就是设置某层权重矩阵 $w^{[l]}=n p . random. randn( shape ) * np. \operatorname{sqrt}\left(\frac{1}{n^{[l-1]}}\right)$，$n^{[l-1]}$ 就是第l层神经单元的数量（即第l - 1层神经元数量）。</p><p>使用 Relu 激活函数时, $g^{[l]}(z)=\operatorname{Relu}(z)$,它取决于你对随机变量的熟悉程度，这是高斯随机变量，然后乘以它的平方根，也就是引用这个方差二。这里，我用<br>的是 $n^{[l-1]}$, 因为本例中，逻辑回归的特征是不变的。但一般情况下l层上的每个神经元都有$n^{[l-1] }$个输入 如果激活函数的输入特征被零均值和标准方差化，方差是1 ， z也会调整到相似范围，这就没解决问题（梯度消失和爆炸问题）。但它确实降低了梯度消失和爆炸问题，因为它给权重矩阵w设置了合理值，你也知道，它不能比 1 大很多，也不能比 1 小很多，所以梯度没有爆炸或消失过快。</p><p>对于如 tanh 激活函数，有篇论文提到，常量 1 比常量 2的效率更高，对于 tanh 函数来说，它是 $\sqrt{\frac{1}{n^{[l-1]}}},$ 这里平方根的作用与这个公式作用相同<br>$np. sqrt \left(\frac{1}{n[l-1]}\right)$, 它适用于 tanh 激活函数，被称为 Xavier 初始化。Yoshua Bengio 和他的同事还提出另一种方法，你可能在一些论文中看到过，它们使用的是公式 $\sqrt{\frac{2}{n^{[l-1]}+n^{[l]}}}$ 其它理论已对此证明，但如果你想用 Relu 激活函数，也就是最常用的激活函数，我会用这个公式<br> $np. sqrt \left(\frac{1}{n[l-1]}\right)$，如果使用 $\tanh$ 函数，可以用公式 $\sqrt{\frac{1}{n[l-1]}},$ 有些作者也会使用这个函数。</p><h2 id="9-梯度检验（Gradient-checking）"><a href="#9-梯度检验（Gradient-checking）" class="headerlink" title="9.梯度检验（Gradient checking）"></a>9.梯度检验（Gradient checking）</h2><script type="math/tex; mode=display">d \theta_{\text {approx }}[i]=\frac{J\left(\theta_{1}, \theta_{2}, \ldots \theta_{i}+\varepsilon, \ldots\right)-J\left(\theta_{1}, \theta_{2}, \ldots \theta_{i}-\varepsilon, \ldots\right)}{2 \varepsilon}</script><p>这个值$d \theta_{\text {approx }}[i]$应该逼近$d \theta[i]=\frac{\partial J}{\partial \theta_{i}}, d \theta[i]$是代价函数的偏导数，然后你需要对$𝑖$的每个值都执行这个运算，最后得到两个向量，得到$d \theta$的逼近值$d \theta_{\text {approx }}$，它与𝑑𝜃具有相同维度，它们两个与𝜃具有相同维度，你要做的就是验证这些向量在实施神经网络时，我经常需要执行 foreprop 和 backprop，然后我可能发现这个梯度 检验有一个相对较大的值，我会怀疑存在 bug，然后开始调试，调试，调试，调试一段时间 后，我得到一个很小的梯度检验值，现在我可以很自信的说，神经网络实施是正确的。是否彼此接近。</p>]]></content>
      
      
      <categories>
          
          <category> 吴恩达——深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 吴恩达 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>03_深层神经网络（Deep L-layer neural network）</title>
      <link href="2020/12/15/machinelearning/andrew-ng-deeplearning/03-deep-l-layer-neural-network/"/>
      <url>2020/12/15/machinelearning/andrew-ng-deeplearning/03-deep-l-layer-neural-network/</url>
      
        <content type="html"><![CDATA[<h1 id="深层神经网络（Deep-L-layer-neural-network）"><a href="#深层神经网络（Deep-L-layer-neural-network）" class="headerlink" title="深层神经网络（Deep L-layer neural network）"></a>深层神经网络（Deep L-layer neural network）</h1><h2 id="1-符号定义"><a href="#1-符号定义" class="headerlink" title="1.符号定义"></a>1.符号定义</h2><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/image-20201216141148480.png" alt="neural network"></p><p>上图是一个四层的神经网络，有三个隐藏层。我们可以看到，第一层（即左边数过去第二层，因为输入层是第 0 层 $)$ 有 5 个神经元数目，第二层 5 个, 第三层 3 个。</p><p>我们用 L表示层数，上图: $L=4$ ，输入层的索引为“0”，第一个隐藏层 $n^{[1]}=5,$ 表示有 5个隐藏神经元，同理 $n^{[2]}=5, n^{[3]}=3, n^{[4]}=n^{[L]}=1($ 输出单元为 1$)$ 。而输入层， $n^{[0]}=n_{x}=3$。</p><p>在不同层所拥有的神经元的数目，对于每层 $I$ 都用 $a^{[l]}$ 来记作 $I$ 层激活后结果，我们会在后面看到在正向传播时，最终能你会计算出 $a^{[l]}$。</p><p>通过用激活函数 $g$ 计算 $z^{[l]},$ 激活函数也被索引为层数 $l,$ 然后我们用 $w^{[l]}$ 来记作在 1 层计算 $z^{[l]}$ 值的权重。类似的， $z^{[l]}$ 里的方程 $b^{[l]}$ 也一样。</p><p>最后总结下符号约定:</p><p>输入的特征记作 $x,$ 但是 $x$ 同样也是 0 层的激活函数，所以 $x=a^{[0]}$ 。最后一层的激活函数，所以 $a^{[L]}$ 是等于这个神经网络所预测的输出结果。</p><h2 id="2-深层网络中的前向传播（Forward-propagation-in-a-Deep-Network）"><a href="#2-深层网络中的前向传播（Forward-propagation-in-a-Deep-Network）" class="headerlink" title="2. 深层网络中的前向传播（Forward propagation in a Deep Network）"></a>2. 深层网络中的前向传播（Forward propagation in a Deep Network）</h2><p>我们先来看对其中一个训练样本 x 如何应用前向传播，之后讨论向量化的版本。</p><p>第一层需要计算 $z^{[1]}=w^{[1]} x+b^{[1]}, a^{[1]}=g^{[1]}\left(z^{[1]}\right)$，（x 可以看做 $a^{[0]}$）</p><p>第二层需要计算 $z^{[2]}=w^{[2]} a^{[1]}+b^{[2]}, a^{[2]}=g^{[2]}\left(z^{[2]}\right)$</p><p>以此类推，</p><p>第四层为 $z^{[4]}=w^{[4]} a^{[3]}+b^{[4]}, \quad a^{[4]}=g^{[4]}\left(z^{[4]}\right)$</p><p>前向传播可以归纳为多次迭代 $z^{[l]}=w^{[l]} a^{[l-1]}+b^{[l]}, a^{[l]}=g^{[l]}\left(z^{[l]}\right)$ 。</p><p>向量化实现过程可以写成:</p><script type="math/tex; mode=display">Z^{[l]}=W^{[l]} a^{[l-1]}+b^{[l]}, \quad A^{[l]}=g^{[l]}\left(Z^{[l]}\right)\left(A^{[0]}=X\right)</script><h2 id="3-搭建神经网络块（Building-blocks-of-deep-neural-networks）"><a href="#3-搭建神经网络块（Building-blocks-of-deep-neural-networks）" class="headerlink" title="3.搭建神经网络块（Building blocks of deep neural networks）"></a>3.搭建神经网络块（Building blocks of deep neural networks）</h2><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/image-20201216143340801.png" alt="neural networks"></p><p>这是一个层数较少的神经网络, 我们选择其中一层(方框部分 $),$ 从这一层的计算着手。在第l层你有参数 $W^{[l]}$ 和 $b^{[l]},$ 正向传播里有输入的激活函数, 输入是前一层 $a^{[l-1]},$ 输出是 $a^{[l]},$我们之前讲过 $z^{[l]}=W^{[l]} a^{[l-1]}+b^{[l]}, a^{[l]}=g^{[l]}\left(z^{[l]}\right),$ 那么这就是你如何从输入 $a^{[l-1]}$ 走到输出的 $a^{[l]}$ 。之后你就可以把 $z^{[l]}$ 的值缓存起来，我在这里也会把这包括在缓存中，因为缓存的$z^{[i]}$ 对以后的正向反向传播的步骤非常有用。</p><p>然后是反向步骤或者说反向传播步骤，同样也是第l层的计算，你会需要实现一个函数输入为 $d a^{[l]},$ 输出 $d a^{[l-1]}$ 的函数。一个小细节需要注意，输入在这里其实是 $d a^{[l]}$ 以及所缓存的 $z^{[l]}$ 值，之前计算好的 $z^{[l]}$ 值, 除了输出 $d a^{[l-1]}$ 的值以外，也需要输出你需要的梯度 $d W^{[l]}$ 和$d b^{[l]},$ 这是为了实现梯度下降学习。</p><p>这就是基本的正向步骤的结构, 我把它成为称为正向函数，类似的在反向步骤中会称为反向函数。总结起来就是，在 I 层，你会有正向函数，输入 $a^{[l-1]}$ 并且输出 $a^{[l]},$ 为了计算结果你需要用 $W^{[l]}$ 和 $b^{[l]},$ 以及输出到缓存的 $z^{[l]} 。$ 然后用作反向传播的反向函数，是另一个函数, 输入 $d a^{[l]},$ 输出 $d a^{[l-1]},$ 你就会得到对激活函数的导数, 也就是希望的导数值 $d a^{[l]}$。$a^{[l-1]}$是会变的，前一层算出的激活函数导数。在这个方块 (第二个) 里你需要 $W^{[l]}$ 和 $b^{[l]},$ 最后你要算的是$𝑑𝑧^{[𝑙]}$。然后这个方块(第三个)中，这个反向函数可以计算输出$𝑑𝑊^{[𝑙]}$和$𝑑𝑏^{[𝑙]}$。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/image-20201216143523413.png" alt="sample"></p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/image-20201216143646495.png" alt="neural networks"></p><p>神经网络的一步训练包含了, 从 $a^{[0]}$ 开始, 也就是 $x$ 然后经过一系列正向传播计算得到$\hat{y},$ 之后再用输出值计算这个（第二行最后方块），再实现反向传播。现在你就有所有的导数项了， $W$ 也会在每一层被更新为 $W=W-\alpha d W, b$ 也一样， $b=b-\alpha d b,$ 反向传播就都计算完毕，我们有所有的导数值，那么这是神经网络一个梯度下降循环。</p><h2 id="4-参数-VS-超参数（Parameters-vs-Hyperparameters）"><a href="#4-参数-VS-超参数（Parameters-vs-Hyperparameters）" class="headerlink" title="4.参数 VS 超参数（Parameters vs Hyperparameters）"></a>4.参数 VS 超参数（Parameters vs Hyperparameters）</h2><h3 id="4-1-什么是超参数"><a href="#4-1-什么是超参数" class="headerlink" title="4.1 什么是超参数?"></a>4.1 什么是超参数?</h3><p>比如算法中的 learning rate $a$ (学习率) 、iterations(梯度下降法循环的数量)、L（隐藏层数目）、n $^{[l]}$ (隐藏层单元数目) 、 choice of activation function（激活函数的选择）都需要你来设置，这些数字实际上控制了最后的参数 $W$ 和 $b$ 的值，所以它们被称作超参数。</p><p>实际上深度学习有很多不同的超参数，之后我们也会介绍一些其他的超参数，如momentum、mini batch size $、$ regularization parameters 等等。</p><h3 id="4-2-如何寻找超参数的最优值"><a href="#4-2-如何寻找超参数的最优值" class="headerlink" title="4.2 如何寻找超参数的最优值?"></a>4.2 如何寻找超参数的最优值?</h3><p>走 Idea—Code—Experiment—Idea 这个循环，尝试各种不同的参数，实现模型并观察是 否成功，然后再迭代。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/image-20201216144843534.png" alt="Idea—Code—Experiment—Idea"></p>]]></content>
      
      
      <categories>
          
          <category> 吴恩达——深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 吴恩达 </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>02_浅层神经网络(Shallow neural networks)</title>
      <link href="2020/12/14/machinelearning/andrew-ng-deeplearning/02-shallow-neural-networks/"/>
      <url>2020/12/14/machinelearning/andrew-ng-deeplearning/02-shallow-neural-networks/</url>
      
        <content type="html"><![CDATA[<h1 id="浅层神经网络（Shallow-neural-networks）"><a href="#浅层神经网络（Shallow-neural-networks）" class="headerlink" title="浅层神经网络（Shallow neural networks）"></a>浅层神经网络（Shallow neural networks）</h1><h2 id="1-神经网络概述（Neural-Network-Overview）"><a href="#1-神经网络概述（Neural-Network-Overview）" class="headerlink" title="1.神经网络概述（Neural Network Overview）"></a>1.神经网络概述（Neural Network Overview）</h2><p>神经网络看起来是如下这个样子</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/image-20201214164209143.png" alt="Neural Network"></p><p>你可以把许多sigmoid 单元堆叠起来形成一个神经网络。对于图中的节点，它包含了之前讲的计算的两个步骤</p><ul><li><p>首先通过计算出值$𝑧$</p></li><li><p>然后通过$\sigma(z)$计算值$𝑎$。</p><script type="math/tex; mode=display">\left.\begin{array}{l}x \\w \\b\end{array}\right\} \Longrightarrow z=w^{T} x+b \Longrightarrow a=\sigma(z)</script></li></ul><h2 id="2-神经网络的表示（Neural-Network-Representation）"><a href="#2-神经网络的表示（Neural-Network-Representation）" class="headerlink" title="2.神经网络的表示（Neural Network Representation）"></a>2.神经网络的表示（Neural Network Representation）</h2><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/image-20201214164452683.png" alt="Neural Network"></p><ul><li>输入层：包含了神经网络的输入，例如途中输入特征$𝑥_1$ 、$𝑥_2$ 、$𝑥_3$ ，它们被竖直地堆叠起来</li><li>隐藏层：输入层与输出层之间节点</li><li>输出层：负责产生预测值</li></ul><p>符号表示：</p><ul><li>$𝑥$：表示输入特征，可以用$𝑎^{[0]}$表示</li><li>$a_j^{[i]}$：第$i$层节点的第$j$个节点</li><li>参数$𝑊$ 和$𝑏$ ：$𝑊$是一个 4x3 的矩阵，而$𝑏$是一个 4x1 的向量</li></ul><h2 id="3-计算一个神经网络的输出（ComputingaNeuralNetwork’s-output）"><a href="#3-计算一个神经网络的输出（ComputingaNeuralNetwork’s-output）" class="headerlink" title="3.计算一个神经网络的输出（ComputingaNeuralNetwork’s output）"></a>3.计算一个神经网络的输出（ComputingaNeuralNetwork’s output）</h2><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/image-20201214165233518.png" alt="两层网络结构"></p><p>𝑥表示输入特征，𝑎表示每个神经元的输出，𝑊表示特征的权重，上标表示神经网络的层数，下标表示该层的第几个神经元。这是神经网络的符号惯例，下同。</p><h3 id="神经网络的计算"><a href="#神经网络的计算" class="headerlink" title="神经网络的计算"></a>神经网络的计算</h3><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/image-20201214165343752.png" alt="image-20201214165343752"></p><p>回到两层的神经网络，我们从隐藏层的第一个神经元开始计算，如上图第一个最上面的箭头所指。从上图可以看出，输入与逻辑回归相似，这个神经元的计算与逻辑回归一样分为两步，小圆圈代表了计算的两个步骤。</p><ul><li>第一步， 计算 $z_{1}^{[1]}, z_{1}^{[1]}=w_{1}^{[1] T} x+b_{1}^{[1]}$</li><li>第二步，通过激活函数计算 $a_{1}^{[1]}, a_{1}^{[1]}=\sigma\left(z_{1}^{[1]}\right)$ 。</li></ul><p>隐藏层的第二个以及后面两个神经元的计算过程一样，只是注意符号表示不同，最终分别得到 $a_{2}^{[1]}, a_{3}^{[1]}, a_{4}^{[1]},$ 详细结果见下:</p><script type="math/tex; mode=display">\begin{array}{l}z_{1}^{[1]}=w_{1}^{[1] T} x+b_{1}^{[1]}, a_{1}^{[1]}=\sigma\left(z_{1}^{[1]}\right) \\z_{2}^{[1]}=w_{2}^{[1] T} x+b_{2}^{[1]}, a_{2}^{[1]}=\sigma\left(z_{2}^{[1]}\right) \\z_{3}^{[1]}=w_{3}^{[1] T} x+b_{3}^{[1]}, a_{3}^{[1]}=\sigma\left(z_{3}^{[1]}\right) \\z_{4}^{[1]}=w_{4}^{[1] T} x+b_{4}^{[1]}, a_{4}^{[1]}=\sigma\left(z_{4}^{[1]}\right)\end{array}</script><h3 id="向量化计算"><a href="#向量化计算" class="headerlink" title="向量化计算"></a>向量化计算</h3><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/image-20201214165605526.png" alt="Z"></p><p>Given input $\mathrm{x}$：</p><script type="math/tex; mode=display">\begin{array}{l}z^{[1]}=W^{[1]} x+b^{[1]} \\a^{[1]}=\sigma\left(z^{[1]}\right) \\z^{[2]}=W^{[2]} a^{[1]}+b^{[2]} \\a^{[2]}=\sigma\left(z^{[2]}\right)\end{array}</script><h2 id="4-多样本向量化（Vectorizing-across-multiple-examples）"><a href="#4-多样本向量化（Vectorizing-across-multiple-examples）" class="headerlink" title="4.多样本向量化（Vectorizing across multiple examples）"></a>4.多样本向量化（Vectorizing across multiple examples）</h2><p>公式 3.12 :</p><script type="math/tex; mode=display">x=\left[\begin{array}{llll}\vdots & \vdots & \vdots & \vdots \\x^{(1)} & x^{(2)} & \cdots & x^{(m)} \\\vdots & \vdots & \vdots & \vdots\end{array}\right]</script><p>公式 3.13 :</p><script type="math/tex; mode=display">Z^{[1]}=\left[\begin{array}{clll}\vdots & \vdots & \vdots & \vdots \\z^{[1](1)} & z^{[1](2)} & \cdots & z^{[1](m)} \\\vdots & \vdots & \vdots & \vdots\end{array}\right]</script><p>公式 3.14 :</p><script type="math/tex; mode=display">A^{[1]}=\left[\begin{array}{clll}\vdots & \vdots & \vdots & \vdots \\\alpha^{[1](1)} & \alpha^{[1](2)} & \cdots & \alpha^{[1](m)} \\\vdots & \vdots & \vdots & \vdots\end{array}\right]</script><p>公式 3.15 :</p><script type="math/tex; mode=display">\left.\begin{array}{rl}z^{[1](i)}=W^{[1](i)} x^{(i)}+b^{[1]} \\\alpha^{[1](i)}=\sigma\left(z^{[1](i)}\right) \\z^{[2](i)}=W^{[2](i)} \alpha^{[1](i)}+b^{[2]} \\\alpha^{[2](i)}=\sigma\left(z^{[2](i)}\right)\end{array}\right\} \Rightarrow\left\{\begin{array}{l}A^{[1]}=\sigma\left(z^{[1]}\right) \\z^{[2]}=W^{[2]} A^{[1]}+b^{[2]} \\A^{[2]}=\sigma\left(z^{[2]}\right)\end{array}\right.</script><p>定义矩阵X等于训练样本，将它们组合成矩阵的各列，形成一个n维或n乘以m维矩阵。接下来计算见公式 3.15:</p><p>以此类推，从小写的向量x到这个大写的矩阵 $X,$ 只是通过组合 $x$ 向量在矩阵的各列中。</p><p>同理， $z^{<a href="1">1</a>}, z^{<a href="2">1</a>}$等等都是$z^{<a href="m">1</a>}$的列向量，将所有𝑚都组合在各列中，就的到矩阵$Z^{[1]}$</p><p>同理， $a^{<a href="1">1</a>}, a^{<a href="2">1</a>}, \ldots \ldots, a^{<a href="m">1</a>}$ 将其组合在矩阵各列中，如同从向量x到矩阵 $X,$ 以及从向量z到矩阵 $Z$ 一样，就能得到矩阵 $A^{[1]}$ 。</p><p>同样的，对于 $Z^{[2]}$ 和 $A^{[2]},$ 也是这样得到。</p><h2 id="5-激活函数（Activationfunctions）"><a href="#5-激活函数（Activationfunctions）" class="headerlink" title="5.激活函数（Activationfunctions）"></a>5.激活函数（Activationfunctions）</h2><p>使用一个神经网络时，需要决定使用哪种激活函数用隐藏层上，哪种用在输出节点上。到目前为止, 之前只用过 sigmoid 激活函数, 但是, 有时其他的激活函数效果会更好。<br>在神经网路的前向传播中, $a^{[1]}=\sigma\left(z^{[1]}\right)$ 和 $a^{[2]}=\sigma\left(z^{[2]}\right)$ 这两步会使用到 sigmoid 函数。sigmoid 函数在这里被称为激活函数。</p><script type="math/tex; mode=display">\quad a=\sigma(z)=\frac{1}{1+e^{-z}}</script><p>更通常的情况下，使用不同的函数 $g\left(z^{[1]}\right), g$ 可以是除了 sigmoid 函数意外的非线性函数。tanh 函数或者双曲正切函数是总体上都优于 sigmoid 函数的激活函数。$a=\tan (z)$ 的值域是位于+1 和-1 之间。</p><script type="math/tex; mode=display">\quad a=\tanh (z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}</script><p>值域介于+1 和-1 之间。结果表明，如果在隐藏层上使用函数$\quad g\left(z^{[1]}\right)=\tanh \left(z^{[1]}\right)$ 效果总是优于 sigmoid 函数。因为函数值域在-1 和+1的激活函数，其均值是更接近零均值的。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/TanhReal.gif" alt="tanx"></p><p>在训练一个算法模型时，如果使用 tanh 函数代替sigmoid 函数中心化数据，使得数据的平均值更接近 0 而不是 $0.5 .$</p><p>有一个例外:在二分类的问题中，对于输出层，因为𝑦的值是 0 或 1，所以想让𝑦的数值介于 0 和 1 之间，而不是在-1 和+1 之间。所以需要使用 sigmoid 激活函数。</p><h3 id="修正线性单元的函数-ReLu"><a href="#修正线性单元的函数-ReLu" class="headerlink" title="修正线性单元的函数(ReLu)"></a>修正线性单元的函数(ReLu)</h3><p>ReLu 函数图像是如下图。 </p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/0*g9ypL5M3k-f7EW85.png" alt="ReLu"></p><script type="math/tex; mode=display">𝑎 = 𝑚𝑎𝑥(0, 𝑧)</script><p>所以，只要𝑧是正值的情况下，导数恒等于 1，当𝑧是负值的时候，导数恒等于 0。</p><p>从实际上来说，当使用𝑧的导数时，𝑧=0 的导数是没有定义的。但是当编程实现的时候，𝑧的取值刚好等于 0.00000001，这个值相当小，所以，在实践中，不需要担心这个值，𝑧是等于 0 的时候，假设一个导数是 1 或者 0 效果都可以。</p><h3 id="选择激活函数的经验法则"><a href="#选择激活函数的经验法则" class="headerlink" title="选择激活函数的经验法则"></a>选择激活函数的经验法则</h3><ul><li>sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。</li><li>tanh激活函数：tanh是非常优秀的，几乎适合所有场合。</li><li>ReLu激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用 ReLu 或者Leaky ReLu。</li></ul><h2 id="6-激活函数的导数（Derivatives-of-activation-functions）"><a href="#6-激活函数的导数（Derivatives-of-activation-functions）" class="headerlink" title="6.激活函数的导数（Derivatives of activation functions）"></a>6.激活函数的导数（Derivatives of activation functions）</h2><p>针对以下四种激活，求其导数如下：</p><h3 id="6-1-sigmoid-activation-function"><a href="#6-1-sigmoid-activation-function" class="headerlink" title="6.1 sigmoid  activation function"></a>6.1 sigmoid  activation function</h3><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/image-20201214172035239.png" alt="sigmoid function"></p><script type="math/tex; mode=display">\frac{d}{d z} g(z)=\frac{1}{1+e^{-z}}\left(1-\frac{1}{1+e^{-z}}\right)=g(z)(1-g(z))=a(1-a)</script><h3 id="6-2-Tanh-activation-function"><a href="#6-2-Tanh-activation-function" class="headerlink" title="6.2 Tanh activation function"></a>6.2 Tanh activation function</h3><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/image-20201214172250352.png" alt="Tanh activation function"></p><script type="math/tex; mode=display">g(z)=\tanh (z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}</script><script type="math/tex; mode=display">\frac{d}{\mathrm{~d} z} g(z)=1-(\tanh (z))^{2}</script><h3 id="6-3-Rectified-Linear-Unit"><a href="#6-3-Rectified-Linear-Unit" class="headerlink" title="6.3 Rectified Linear Unit"></a>6.3 Rectified Linear Unit</h3><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/image-20201214172357648.png" alt="Rectified Linear Unit"></p><script type="math/tex; mode=display">\begin{array}{c}g(z)=\max (0, z) \\0 \\g(z)^{\prime}=\left\{\begin{array}{ll}0 & \text { if } \quad z<0 \\\text { undefined } & \text { if } \quad z>0 \\\text { if } \quad z & =0\end{array}\right.\end{array}</script><h3 id="6-4-Leaky-linear-unit-（Leaky-ReLU）"><a href="#6-4-Leaky-linear-unit-（Leaky-ReLU）" class="headerlink" title="6.4 Leaky linear unit （Leaky ReLU）"></a>6.4 Leaky linear unit （Leaky ReLU）</h3><p>与 ReLU 类似:</p><script type="math/tex; mode=display">\begin{array}{c}g(z)=\max (0.01 z, z) \\g(z)^{\prime}=\left\{\begin{array}{ll}0.01 & \text { if } \quad z<0 \\1 & \text { if } \quad z>0 \\u n d e f i n e d & \text { if } \quad z=0\end{array}\right.\end{array}</script>]]></content>
      
      
      <categories>
          
          <category> 吴恩达——深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 吴恩达 </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>01_神经网络的编程基础（Basics of Neural Network programming）</title>
      <link href="2020/12/12/machinelearning/andrew-ng-deeplearning/01-basics-of-neural-network/"/>
      <url>2020/12/12/machinelearning/andrew-ng-deeplearning/01-basics-of-neural-network/</url>
      
        <content type="html"><![CDATA[<h1 id="神经网络的编程基础（Basics-of-Neural-Network-programming）"><a href="#神经网络的编程基础（Basics-of-Neural-Network-programming）" class="headerlink" title="神经网络的编程基础（Basics of Neural Network programming）"></a>神经网络的编程基础（Basics of Neural Network programming）</h1><h2 id="1-二分类（Binary-Classification）"><a href="#1-二分类（Binary-Classification）" class="headerlink" title="1.二分类（Binary Classification）"></a>1.二分类（Binary Classification）</h2><p>这周我们将学习神经网络的基础知识，其中需要注意的是，当实现一个神经网络的时候， 我们需要知道一些非常重要的技术和技巧。例如有一个包含𝑚个样本的训练集，你很可能习 惯于用一个 for 循环来遍历训练集中的每个样本，但是当实现一个神经网络的时候，我们通 常不直接使用 for 循环来遍历整个训练集，所以在这周的课程中你将学会如何处理训练集。</p><p>另外在神经网络的计算中，通常先有一个叫做前向暂停（forward pause）或叫做前向传播 （foward propagation）的步骤，接着有一个叫做反向暂停（backward pause） 或叫做反向传播 （backward propagation）的步骤。所以这周我也会向你介绍为什么神经网络的训练过程可以分为前向传播和反向传播两个独立的部分。</p><p>在课程中我将使用逻辑回归（logistic regression）来传达这些想法，以使大家能够更加容易地理解这些概念。即使你之前了解过逻辑回归，我认为这里还是有些新的、有趣的东西等着你去发现和了解，所以现在开始进入正题。</p><p>逻辑回归是一个用于二分类（binary classification）的算法。首先我们从一个问题开始说起， 这里有一个二分类问题的例子，假如你有一张图片作为输入，比如这只猫，如果识别这张图片为猫，则输出标签 1 作为结果;如果识别出不是猫，那么输出标签 0 作为结果。现在我们可以用字母𝑦来表示输出的结果标签，如下图所示:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201210163758.png" alt="image-20201210163758119"></p><p>我们来看看一张图片在计算机中是如何表示的，为了保存一张图片，需要保存三个矩阵， 它们分别对应图片中的红、绿、蓝三种颜色通道，如果你的图片大小为 64x64 像素，那么你 就有三个规模为 64x64 的矩阵，分别对应图片中红、绿、蓝三种像素的强度值。为了便于表 示，这里我画了三个很小的矩阵，注意它们的规模为 5x4 而不是 64x64，如下图所示:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201210163827.png" alt="image-20201210163827520"></p><p>为了把这些像素值放到一个特征向量中，我们需要把这些像素值提取出来，然后放入一个特征向量𝑥。为了把这些像素值转换为特征向量 𝑥，我们需要像下面这样定义一个特征向量 𝑥 来表示这张图片，我们把所有的像素都取出来，例如 255、231 等等，直到取完所有的红色像素，接着最后是 255、134、…、255、134 等等，直到得到一个特征向量，把图片中所有的红、绿、蓝像素值都列出来。如果图片的大小为 64x64 像素，那么向量 𝑥 的总维度， 将是 64 乘以 64 乘以 3，这是三个像素矩阵中像素的总量。在这个例子中结果为 12,288。现 在我们用$𝑛_𝑥 = 12288$，来表示输入特征向量的维度，有时候为了简洁，我会直接用小写的𝑛 来表示输入特征向量𝑥的维度。所以在二分类问题中，我们的目标就是习得一个分类器，它以图片的特征向量作为输入，然后预测输出结果𝑦为 1 还是 0，也就是预测图片中是否有猫:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201210163923.png" alt="image-20201210163923881"></p><p>接下来我们说明一些在余下课程中，需要用到的一些符号。</p><ul><li>$x:$ 表示一个 $n_{x}$ 维数据，为输入数据，维度为 $\left(n_{x}, 1\right) ;$</li><li>$y:$ 表示输出结果，取值为 (0,1)$;$</li><li>$\left(x^{(i)}, y^{(i)}\right):$ 表示第i组数据，可能是训练数据，也可能是测试数据，此处默认为训练数据;</li><li>$X=\left[x^{(1)}, x^{(2)}, \ldots, x^{(m)}\right]:$ 表示所有的训练数据集的输入值, 放在一个 $n_{x} \times m$ 的矩阵中其中 $m$ 表示样本数目;</li><li>$Y=\left[y^{(1)}, y^{(2)}, \ldots, y^{(m)}\right]:$ 对应表示所有训练数据集的输出值，维度为1 $\times m_{\circ}$</li></ul><p>用一对 $(x, y)$ 来表示一个单独的样本，x代表 $n_{x}$ 维的特征向量， $y$ 表示标签(输出结果)只能为 0 或 1 。而训练集将由 $m$ 个训练样本组成，其中 $\left(x^{(1)}, y^{(1)}\right)$ 表示第一个样本的输入和输出 $,\left(x^{(2)}, y^{(2)}\right)$ 表示第二个样本的输入和输出，直到最后一个样本 $\left(x^{(m)}, y^{(m)}\right),$ 然后所有的这些一起表示整个训练集。有时候为了强调这是训练样本的个数，会写作 $M_{t r a i n},$ 当涉及到测试集的时候，我们会使用 $M_{t e s t}$ 来表示测试集的样本数，所以这是测试集的样本数:</p><script type="math/tex; mode=display">(x, y) \quad x \in \mathbb{R}^{n_{x}}, y \in\{0,1\}</script><script type="math/tex; mode=display">\left.\left\{\left(x^{(1)} \cdot y^{(1)}\right),\left(x^{(2}, y^{(2)}\right), \ldots, \ldots, \left(x^{(m)}, y^{(m)}\right)\right)\right\}</script><p>最后为了能把训练集表示得更紧奏一点，我们会定义一个矩阵用大写X的表示，它由输入向量 $x^{(1)} 、 x^{(2)}$ 等组成，如下图放在矩阵的列中，所以现在我们把 $x^{(1)}$ 作为第一列放在矩阵中， $x^{(2)}$ 作为第二列， $x^{(m)}$ 放到第m列，然后我们就得到了训练集矩阵 $X$ 。所以这个矩阵有 $m$列， $m$ 是训练集的样本数量，然后这个矩阵的高度记为 $n_{x},$ 注意有时候可能因为其他某些原因，矩阵 $X$ 会由训练样本按照行堆叠起来而不是列，如下图所示: $x^{(1)}$ 的转置直到 $x^{(m)}$ 的转置,但是在实现神经网络的时候,使用左边的这种形式,会让整个实现的过程变得更加简单:</p><p>那么输出标签y呢？同样的道理，为了能更加容易地实现一个神经网络, 将标签y放在列中将会使得后续计算非常方便，所以我们定义大写的Y等于 $y^{(1)}, y^{(m)}, \ldots, y^{(m)},$ 所以在这里是一个规模为 1 乘以m的矩阵。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201210164925.png" alt="image-20201210164925141"></p><h2 id="2-逻辑回归（Logistic-Regression）"><a href="#2-逻辑回归（Logistic-Regression）" class="headerlink" title="2. 逻辑回归（Logistic Regression）"></a>2. 逻辑回归（Logistic Regression）</h2><p>逻辑回归是监督学习问题中，当输出𝑦全部为0或1时使用的一种学习算法。 </p><p>逻辑回归的目标是最大程度地减少其预测与训练数据之间的误差。</p><p>示例：识别图像是否为猫：<br>给定一个由特征向量表示的图像，该算法将评估这张图像是猫图像的概率。</p><script type="math/tex; mode=display">\text { Given } x, \hat{y}=P(y=1 \mid x), \text { where } 0 \leq \hat{y} \leq 1</script><p>逻辑回归中常用的参数和表示:</p><ul><li>输入特征向量：$x \in \mathbb{R}^{n_{x}}$， 其中 $n_{x}$ 是特征的数量</li><li>实际值:：$y \in 0,1$</li><li>参数（权重）： $w \in \mathbb{R}^{n_{x}}$，其中 $n_{x}$ 是特征的数量</li><li>偏差（阈值）： $b \in \mathbb{R}$</li><li>输出： $\hat{y}=\sigma\left(w^{T} x+b\right)$</li><li>sigmoid函数： $s=\sigma\left(w^{T} x+b\right)=\sigma(z)=\frac{1}{1+e^{-z}}$</li></ul><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201211150522.png" alt="sigmoid function"></p><ul><li>当$z \gg 0$时$\sigma(z)=1$</li><li><p>当$z \ll 0$时 $\sigma(z)=0$</p></li><li><p>当 $z=0$时$\sigma(z)=0.5$</p></li></ul><h2 id="3-逻辑回归的代价函数（Cost-Function）"><a href="#3-逻辑回归的代价函数（Cost-Function）" class="headerlink" title="3.逻辑回归的代价函数（Cost Function）"></a>3.逻辑回归的代价函数（Cost Function）</h2><p>为了训练参数w，b，我们需要定义一下的代价函数：</p><script type="math/tex; mode=display">\begin{aligned}&\text { Recap: }\\&\hat{y}^{(i)}=\sigma\left(w^{T} x^{(i)}+b\right), \text { where } \sigma\left(z^{(i)}\right)=\frac{1}{1+e^{-z}(t)}\\&\text {Given }\left\{\left(x^{(1)}, y^{(1)}\right), \cdots,\left(x^{(m)}, y^{(m)}\right)\right\}, \text { we want } \hat{y}^{(i)} \approx y^{(i)}\end{aligned}</script><p>损失函数用于度量预测$\left(\hat{y}^{(i)}\right)$与期望输出$\left(y^{(i)}\right)$之间的差异 ，损失函数为单个训练示例计算误差。<br>$L\left(\hat{y}^{(i)}, y^{(i)}\right)=\frac{1}{2}\left(\hat{y}^{(i)}-y^{(i)}\right)^{2}$<br>$L\left(\hat{y}^{(i)}, y^{(i)}\right)=-\left(y^{(i)} \log \left(\hat{y}^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-\hat{y}^{(i)}\right)\right.$</p><ul><li>If $y^{(i)}=1: L\left(\hat{y}^{(i)}, y^{(i)}\right)=-\log \left(\hat{y}^{(i)}\right)$ where $\log \left(\hat{y}^{(i)}\right)$ and $\hat{y}^{(i)}$ should be close to 1</li><li>If $y^{(i)}=0: L\left(\hat{y}^{(i)}, y^{(i)}\right)=-\log \left(1-\hat{y}^{(i)}\right)$ where $\log \left(1-\hat{y}^{(i)}\right)$ and $\hat{y}^{(i)}$ should be close to 0</li></ul><p>代价函数:</p><script type="math/tex; mode=display">J(w, b)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)=\frac{1}{m} \sum_{i=1}^{m}\left(-y^{(i)} \log \hat{y}^{(i)}-\left(1-y^{(i)}\right) \log \left(1-\hat{y}^{(i)}\right)\right)</script><h2 id="4-梯度下降法（Gradient-Descent）"><a href="#4-梯度下降法（Gradient-Descent）" class="headerlink" title="4.梯度下降法（Gradient Descent）"></a>4.梯度下降法（Gradient Descent）</h2><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201211144252.png" alt="Gradient Descent"></p><p>整个梯度下降法的迭代过程就是将$J(w,b)$从一个初始点朝着最小值点方向走。</p><script type="math/tex; mode=display">w:=w-a \frac{\partial J(w, b)}{\partial w} \quad b:=b-a \frac{\partial J(w, b)}{\partial b}</script><p>其中$a$为步长（step）</p><h2 id="5-计算图（Computation-Graph）"><a href="#5-计算图（Computation-Graph）" class="headerlink" title="5.计算图（Computation Graph）"></a>5.计算图（Computation Graph）</h2><p>例如函数$J(a,b,c)=3(a+bc)$可以分解为以下计算图：</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201211154440.png" alt="Computation Graph"></p><h2 id="6-使用计算图求导数（computing-derivatives）"><a href="#6-使用计算图求导数（computing-derivatives）" class="headerlink" title="6.使用计算图求导数（computing derivatives）"></a>6.使用计算图求导数（computing derivatives）</h2><p>下面用到的公式:</p><script type="math/tex; mode=display">\frac{d J}{d u}=\frac{d J}{d v} \frac{d v}{d u}, \quad \frac{d J}{d b}=\frac{d J}{d u} \frac{d u}{d b}, \quad \frac{d J}{d a}=\frac{d J}{d u} \frac{d u}{d a}</script><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201211155940.png" alt="computing derivatives"></p><p>反向从最后往前计算导数，比如先计算$\frac{d J}{d u}$，然后通过链式求导法则往前求导</p><h2 id="7-逻辑回归中的梯度下降（Logistic-Regression-Gradient-Descent）"><a href="#7-逻辑回归中的梯度下降（Logistic-Regression-Gradient-Descent）" class="headerlink" title="7.逻辑回归中的梯度下降（Logistic Regression Gradient Descent）"></a>7.逻辑回归中的梯度下降（Logistic Regression Gradient Descent）</h2><p>假设样本只有两个特征 $x_{1}$ 和 $x_{2}$, 为了计算 $z$ ，我们需要输入参数 $w_{1} 、 w_{2}$ 和 $b$, 除此之外还有特征值 $x_{1}$ 和 $x_{2}$。 因此z的计算公式为 ：$z=w_{1} x_{1}+w_{2} x_{2}+b$<br>回想一下逻辑回归的公式定义如下: $\quad \hat{y}=a=\sigma(z)$ 其中 $z=w^{T} x+b, \quad \sigma(z)=\frac{1}{1+e^{-z}}$<br>损失函数: </p><script type="math/tex; mode=display">L\left(\hat{y}^{(i)}, y^{(i)}\right)=-y^{(i)} \log \hat{y}^{(i)}-\left(1-y^{(i)}\right) \log \left(1-\hat{y}^{(i)}\right)</script><p>代价函数：</p><script type="math/tex; mode=display">J(w, b)=\frac{1}{m} \sum_{i}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)</script><p>假设现在只考虑单个样本的情况，单个样本的代价函数定义如下:</p><script type="math/tex; mode=display">L(a, y)=-(y \log (a)+(1-y) \log (1-a))</script><p>其中 $a$ 是逻辑回归的输出, $y$ 是样本的标签值。现在让我们画出表示这个计算的计算图。<br>这里先复习下梯度下降法， $w$ 和 $b$ 的修正量可以表达如下:</p><script type="math/tex; mode=display">w:=w-a \frac{\partial J(w, b)}{\partial w}, \quad b:=b-a \frac{\partial J(w, b)}{\partial b}</script><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201211160530.png" alt="Computation Graph"></p><p>一步一步往前计算首先对a求偏导数：</p><script type="math/tex; mode=display">\frac{d L(a, y)}{d a}=-y / a+(1-y) /(1-a)</script><p>因为 $\frac{d L(a, y)}{d z}=\frac{d L}{d z}=\left(\frac{d L}{d a}\right) \cdot\left(\frac{d a}{d z}\right), \quad$ 并且 $\frac{d a}{d z}=a \cdot(1-a), \quad$ 而 $\frac{d L}{d a}=\left(-\frac{y}{a}+\frac{(1-y)}{(1-a)}\right),$<br>因此将这两项相乘，得到:</p><script type="math/tex; mode=display">d z=\frac{d L(a, y)}{d z}=\frac{d L}{d z}=\left(\frac{d L}{d a}\right) \cdot\left(\frac{d a}{d z}\right)=\left(-\frac{y}{a}+\frac{(1-y)}{(1-a)}\right) \cdot a(1-a)=a-y</script><p>现在进行最后一步反向推导，也就是计算𝑤和𝑏变化对代价函数𝐿的影响</p><script type="math/tex; mode=display">\frac{\partial L}{\partial w_{1}}=x_{1} \cdot d z,\quad \frac{\partial L}{\partial w_{2}}=x_{2} \cdot d z, \quad d b=d z</script><p>对于m个样本：</p><script type="math/tex; mode=display">\begin{array}{l}d w_{j}=\frac{1}{m} \sum_{i}^{m} x_{j}^{(i)}\left(a^{(i)}-y^{(i)}\right) \\d b=\frac{1}{m} \sum_{i}^{m}\left(a^{(i)}-y^{(i)}\right)\end{array}</script><p>伪代码：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">J<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>dw1<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>dw2<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>db<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span><span class="token keyword">for</span> i <span class="token operator">=</span> <span class="token number">1</span> to m  z<span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token operator">=</span> wx<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token operator">+</span>b<span class="token punctuation">;</span>  a<span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>z<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  J <span class="token operator">+=</span> <span class="token operator">-</span><span class="token punctuation">[</span>y<span class="token punctuation">(</span>i<span class="token punctuation">)</span>log<span class="token punctuation">(</span>a<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">+</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>y<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span>log<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>a<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> dz<span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token operator">=</span> a<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token operator">-</span>y<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>  dw1 <span class="token operator">+=</span> x1<span class="token punctuation">(</span>i<span class="token punctuation">)</span>dz<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>  dw2 <span class="token operator">+=</span> x2<span class="token punctuation">(</span>i<span class="token punctuation">)</span>dz<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>  db <span class="token operator">+=</span> dz<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>  J<span class="token operator">/=</span> m<span class="token punctuation">;</span>  dw1<span class="token operator">/=</span> m<span class="token punctuation">;</span>  dw2<span class="token operator">/=</span> m<span class="token punctuation">;</span>  db<span class="token operator">/=</span> m<span class="token punctuation">;</span>  w<span class="token operator">=</span>w<span class="token operator">-</span>alpha<span class="token operator">*</span>dw  b<span class="token operator">=</span>b<span class="token operator">-</span>alpha<span class="token operator">*</span>db<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="8-向量化"><a href="#8-向量化" class="headerlink" title="8.向量化"></a>8.向量化</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np <span class="token comment">#导入 numpy 库</span>a <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment">#创建一个数据 </span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token comment"># [1 2 3 4]</span><span class="token keyword">import</span> time <span class="token comment">#导入时间库</span>a <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">1000000</span><span class="token punctuation">)</span>b <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">1000000</span><span class="token punctuation">)</span> <span class="token comment">#通过 round 随机得到两个一百万维度的数组</span>tic <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#现在测量一下当前时间</span><span class="token comment">#向量化的版本</span>c <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span>toc <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>“Vectorized version<span class="token punctuation">:</span>” <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token operator">*</span><span class="token punctuation">(</span>toc<span class="token operator">-</span>tic<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span>”ms”<span class="token punctuation">)</span> <span class="token comment">#打印一下向量化的版本的时间</span><span class="token comment">#继续增加非向量化的版本</span>c<span class="token operator">=</span><span class="token number">0</span>tic <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1000000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>c <span class="token operator">+=</span> a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">*</span>b<span class="token punctuation">[</span>i<span class="token punctuation">]</span>toc <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>c<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>“For loop<span class="token punctuation">:</span>” <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token operator">*</span><span class="token punctuation">(</span>toc<span class="token operator">-</span>tic<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> “ms”<span class="token punctuation">)</span><span class="token comment">#打印 for循环的版本的时间</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>for太慢，上面的例子相差300倍，深度学习代码中尽量不使用for，使用向量</p><h2 id="9-向量化逻辑回归（Vectorizing-Logistic-Regression）"><a href="#9-向量化逻辑回归（Vectorizing-Logistic-Regression）" class="headerlink" title="9.向量化逻辑回归（Vectorizing Logistic Regression）"></a>9.向量化逻辑回归（Vectorizing Logistic Regression）</h2><p>我们的目标是不使用 for 循环，而是向量，我们可以这么做:</p><script type="math/tex; mode=display">Z=w^{T} X+b=n p . \operatorname{dot}(w . T, X)+b</script><script type="math/tex; mode=display">A=\sigma(Z)</script><script type="math/tex; mode=display">d Z=A-Y</script><script type="math/tex; mode=display">d w=\frac{1}{m} * X * d z^{T}​</script><script type="math/tex; mode=display">d b=\frac{1}{m} * n p . \operatorname{sum}(d Z)​</script><script type="math/tex; mode=display">w:=w-a * d w​</script><script type="math/tex; mode=display">b:=b-a * d b​</script><h2 id="10-Python-中的广播（Broadcasting-in-Python）"><a href="#10-Python-中的广播（Broadcasting-in-Python）" class="headerlink" title="10.Python 中的广播（Broadcasting in Python）"></a>10.Python 中的广播（Broadcasting in Python）</h2><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/image-20201214143246086.png" alt="Broadcasting in Python"></p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/image-20201214143943638.png" alt="image-20201214143943638"></p><h2 id="11-logistic-损失函数的解释（Explanationoflogistic-regression-cost-function）"><a href="#11-logistic-损失函数的解释（Explanationoflogistic-regression-cost-function）" class="headerlink" title="11.logistic 损失函数的解释（Explanationoflogistic regression cost function）"></a>11.logistic 损失函数的解释（Explanationoflogistic regression cost function）</h2><p>回想一下，在逻辑回归中，需要预测的结果金,可以表示为 $\hat{y}=\sigma\left(w^{T} x+b\right), \sigma$ 是我们熟给定训练样本 $x$ 条件下 $y$ 等于 1 的概率。<br>换句话说，如果 $y=1,$ 在给定训练样本 $x$ 条件下 $\mathrm{y}=\hat{y}$<br>反过来说，如果 $y=0,$ 在给定训练样本 $x$ 条件下 $(y=1-\hat{y})$,<br>因此，如果 $\hat{y}$ 代表 $y=1$ 的概率，那么1 $-\hat{y}$ 就是 $y=0$ 的概率。<br>接下来，我们就来分析这两个条件概率公式。<br>If $y=1: \quad p(y \mid x)=\hat{y}$<br>If $y=0: \quad p(y \mid x)=1-\hat{y}$</p><p>上述的两个条件概率公式可以合并成如下公式:</p><script type="math/tex; mode=display">p(y \mid x)=\hat{y}^{y}(1-\hat{y})^{(1-y)}</script><p>由于 $\log$函数是严格单调递增的函数，最大化 $\log (p(y \mid x))$ 等价于最大化 $p(y \mid x)$ 并且地计算$p(y \mid x)$ 的 $\log$ 对数, 就是计算 $\log \left(\hat{y}^{(y)}(1-\hat{y})^{(1-y)}\right)$ </p><p>其实就是将 $p(y \mid x)$ 代入，通过对数函数化简为:</p><script type="math/tex; mode=display">-L(\hat{y}, y)=y \log \hat{y}+(1-y) \log (1-\hat{y})</script>]]></content>
      
      
      <categories>
          
          <category> 吴恩达——深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 吴恩达 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>10_推荐系统(Recommender Systems)</title>
      <link href="2020/12/10/machinelearning/wu-en-da-ji-qi-xue-xi/10-recommender-systems/"/>
      <url>2020/12/10/machinelearning/wu-en-da-ji-qi-xue-xi/10-recommender-systems/</url>
      
        <content type="html"><![CDATA[<h2 id="1-问题形式化"><a href="#1-问题形式化" class="headerlink" title="1.问题形式化"></a>1.问题形式化</h2><p>我们从一个例子开始定义推荐系统的问题。</p><p>假使我们是一个电影供应商，我们有 5 部电影和 4 个用户，我们要求用户为电影打分。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201210134224.png" alt="电影打分"></p><p>前三部电影是爱情片，后两部则是动作片，我们可以看出 Alice 和 Bob 似乎更倾向与爱情片，而 Carol 和 Dave 似乎更倾向与动作片。并且没有一个用户给所有的电影都打过分。 我们希望构建一个算法来预测他们每个人可能会给他们没看过的电影打多少分，并以此作为推荐的依据。</p><p>下面引入一些标记:</p><ul><li>$n_{u}$ 代表用户的数量</li><li>$n_{m}$ 代表电影的数量</li><li>$r(i, j) \quad$ 如果用户 $j$ 给电影 $i$ 评过分则 $r(i, j)=1$</li><li>$y^{(i, j)} \quad$ 代表用户 $j$ 给电影 $i$ 的评分</li><li>$m_{j}$ 代表用户 $j$ 评过分的电影的总数</li></ul><h2 id="2-基于内容的推荐系统（Content-Based-Recommendations）"><a href="#2-基于内容的推荐系统（Content-Based-Recommendations）" class="headerlink" title="2.基于内容的推荐系统（Content Based Recommendations）"></a>2.基于内容的推荐系统（Content Based Recommendations）</h2><p>在一个基于内容的推荐系统算法中，我们假设对于我们希望推荐的东西有一些数据，这些数据是有关这些东西的特征。在我们的例子中，我们可以假设每部电影都有两个特征，如$𝑥_1$代表电影的浪漫程度，$𝑥_2$代表电影的动作程度。</p><script type="math/tex; mode=display">\begin{array}{c|cccc|cc}\text { Movie } & \text { Alice (1) } & \text { Bob (2) } & \text { Carol (3) } & \text { Dave (4) } & \begin{array}{c}x_{1} \\\text { (romance) }\end{array} & \begin{array}{c}x_{2} \\\text { (action) }\end{array} \\\hline \text { Love at last } & 5 & 5 & 0 & 0 & 0.9 & 0 \\\text { Romance forever } & 5 & ? & ? & 0 & 1.0 & 0.01 \\\text { Cute puppies of } & ? & 4 & 0 & ? & 0.99 & 0 \\\text { love } & & & & & & \\\begin{array}{c}\text { Nonstop car } \\\text { chases }\end{array} & 0 & 0 & 5 & 4 & 0.1 & 1.0 \\\text { Swords vs. karate } & 0 & 0 & 5 & ? & 0 & 0.9\end{array}</script><p>则每部电影都有一个特征向量，如 $x^{(1)}$ 是第一部电影的特征向量为[0.9 0]。</p><p>下面我们要基于这些特征来构建一个推荐系统算法。 假设我们采用线性回归模型，我们可以针对每一个用户都训练一个线性回归模型，如 $\theta^{(1)}$ 是第一个用户的模型的参数。 于是，我们有:</p><ul><li>$\theta^{(j)}$ 用户 $j$ 的参数向量</li><li>$x^{(i)}$ 电影 $i$ 的特征向量</li><li>对于用户 $j$ 和电影 $i,$ 我们预测评分为: $\left(\theta^{(j)}\right)^{T} x^{(i)}$</li><li>代价函数针对用户 $j$, 该线性回归模型的代价为预测误差的平方和，加上正则化项:</li></ul><script type="math/tex; mode=display">\min _{\theta(j)} \frac{1}{2} \sum_{i: r(i, j)=1}\left(\left(\theta^{(j)}\right)^{T} x^{(i)}-y^{(i, j)}\right)^{2}+\frac{\lambda}{2}\left(\theta_{k}^{(j)}\right)^{2}</script><p>其中 $i: r(i, j)$ 表示我们只计算那些用户 $j$ 评过分的电影。在一般的线性回归模型中, 误差项和正则项应该都是乘以1/2m，在这里我们将m去掉。并且我们不对方差项 $\theta_{0}$ 进行正则化处理。<br>上面的代价函数只是针对一个用户的, 为了学习所有用户, 我们将所有用户的代价函数求和:</p><script type="math/tex; mode=display">\min _{\theta^{(1)}, \ldots, \theta^{\left(n_{u}\right)}} \frac{1}{2} \sum_{j=1}^{n_{u}} \sum_{i: r(i, j)=1}\left(\left(\theta^{(j)}\right)^{T} x^{(i)}-y^{(i, j)}\right)^{2}+\frac{\lambda}{2} \sum_{j=1}^{n_{u}} \sum_{k=1}^{n}\left(\theta_{k}^{(j)}\right)^{2}</script><p>如果我们要用梯度下降法来求解最优解，我们计算代价函数的偏导数后得到梯度下降的 更新公式为:</p><script type="math/tex; mode=display">\begin{array}{c} \theta_{k}^{(j)}:=\theta_{k}^{(j)}-\alpha \sum_{i: r(i, j)=1}\left(\left(\theta^{(j)}\right)^{T} x^{(i)}-y^{(i, j)}\right) x_{k}^{(i)} \quad \text { (for } \left.k=0\right) \\\theta_{k}^{(j)}:=\theta_{k}^{(j)}-\alpha\left(\sum_{i: r(i, j)=1}\left(\left(\theta^{(j)}\right)^{T} x^{(i)}-y^{(i, j)}\right) x_{k}^{(i)}+\lambda \theta_{k}^{(j)}\right) \quad(\text { for } k \neq 0)\end{array}</script><h2 id="3-协同过滤-Collaborative-Filtering"><a href="#3-协同过滤-Collaborative-Filtering" class="headerlink" title="3.协同过滤 Collaborative Filtering"></a>3.协同过滤 Collaborative Filtering</h2><p>在之前的基于内容的推荐系统中，对于每一部电影，我们都掌握了可用的特征，使用这 些特征训练出了每一个用户的参数。相反地，如果我们拥有用户的参数，我们可以学习得出电影的特征。</p><script type="math/tex; mode=display">\min _{x^{(1)}, \ldots, x^{\left(n_{m}\right)}} \frac{1}{2} \sum_{i=1}^{n_{m}} \sum_{j r(i, j)=1}\left(\left(\theta^{(j)}\right)^{T} x^{(i)}-y^{(i, j)}\right)^{2}+\frac{\lambda}{2} \sum_{i=1}^{n_{m}} \sum_{k=1}^{n}\left(x_{k}^{(i)}\right)^{2}</script><p>但是如果我们既没有用户的参数，也没有电影的特征，这两种方法都不可行了。协同过滤算法可以同时学习这两者。</p><p>我们的优化目标便改为同时针对𝑥和𝜃进行。</p><script type="math/tex; mode=display">\begin{array}{l}J\left(x^{(1)}, \ldots x^{\left(n_{m}\right)}, \theta^{(1)}, \ldots, \theta^{\left(n_{u}\right)}\right) \\\qquad \begin{array}{l}=\frac{1}{2} \sum_{(i: j): r(i, j)=1}\left(\left(\theta^{(j)}\right)^{T} x^{(i)}-y^{(i, j)}\right)^{2}+\frac{\lambda}{2} \sum_{i=1}^{n_{m}} \sum_{k=1}^{n}\left(x_{k}^{(j)}\right)^{2} \\+\frac{\lambda}{2} \sum_{j=1}^{n_{u}} \sum_{k=1}^{n}\left(\theta_{k}^{(j)}\right)^{2}\end{array}\end{array}</script><p>对代价函数求偏导数的结果如下：</p><script type="math/tex; mode=display">\begin{array}{c}x_{k}^{(i)}:=x_{k}^{(i)}-\alpha\left(\sum_{j: r(i, j)=1}\left(\left(\theta^{(j)}\right)^{T} x^{(i)}-y^{(i, j)} \theta_{k}^{j}+\lambda x_{k}^{(i)}\right)\right. \\\theta_{k}^{(i)}:=\theta_{k}^{(i)}-\alpha\left(\sum_{i: r(i, j)=1}\left(\left(\theta^{(j)}\right)^{T} x^{(i)}-y^{(i, j)} x_{k}^{(i)}+\lambda \theta_{k}^{(j)}\right)\right. \\\min _{x^{(1)}, \ldots, x^{\left(n_{m}\right)}} \frac{1}{2} \sum_{i=1}^{n_{m}} \sum_{j r(i, j)=1}\left(\left(\theta^{(j)}\right)^{T} x^{(i)}-y^{(i, j)}\right)^{2}+\frac{\lambda}{2} \sum_{i=1}^{n_{m}} \sum_{k=1}^{n}\left(x_{k}^{(i)}\right)^{2}\end{array}</script><p>协同过滤从算法中，我们通常不使用方差项，如果需要的话，算法会自动学得。 协同过滤算法使用步骤如下:</p><ol><li>初始 $x^{(1)}, x^{(1)}, \ldots x^{(n m)}, \theta^{(1)}, \theta^{(2)}, \ldots, \theta^{\left(n_{u}\right)}$ 为一些随机小值</li><li>使用梯度下降算法最小化代价函数</li><li>在训练完算法后，我们预测 $\left(\theta^{(j)}\right)^{T} x^{(i)}$ 为用户 $j$ 给电影 $i$ 的评分</li></ol><p>通过这个学习过程获得的特征矩阵包含了有关电影的重要数据，这些数据不总是人能读懂的，但是我们可以用这些数据作为给用户推荐电影的依据。</p><p>例如，如果一位用户正在观看电影 $x^{(i)},$ 我们可以寻找另一部电影 $x^{(j)},$ 依据两部电影的特征向量之间的距离 $\left|x^{(i)}-x^{(j)}\right|$ 的大小。</p><h2 id="4-协同过滤算法"><a href="#4-协同过滤算法" class="headerlink" title="4.协同过滤算法"></a>4.协同过滤算法</h2><p>协同过滤优化目标:<br>给定 $x^{(1)}, \ldots, x^{\left(n_{m}\right)},$ 估计 $\theta^{(1)}, \ldots, \theta^{\left(n_{u}\right)}:$</p><script type="math/tex; mode=display">\min _{\theta^{(1)}, \ldots, \theta^{\left(n_{u}\right)}} \frac{1}{2} \sum_{j=1}^{n_{u}} \sum_{i: r(i, j)=1}\left(\left(\theta^{(j)}\right)^{T} x^{(i)}-y^{(i, j)}\right)^{2}+\frac{\lambda}{2} \sum_{j=1}^{n_{u}} \sum_{k=1}^{n}\left(\theta_{k}^{(j)}\right)^{2}</script><p>给定 $\theta^{(1)}, \ldots, \theta^{\left(n_{u}\right)},$ 估计 $x^{(1)}, \ldots, x^{\left(n_{m}\right)}:$<br>同时最小化 $x^{(1)}, \ldots, x^{\left(n_{m}\right)}$ 和 $\theta^{(1)}, \ldots, \theta^{\left(n_{u}\right)}:$</p><script type="math/tex; mode=display">\begin{array}{c}J\left(x^{(1)}, \ldots, x^{\left(n_{m}\right)}, \theta^{(1)}, \ldots, \theta^{\left(n_{u}\right)}\right) \\=\frac{1}{2} \sum_{(i, j): r(i, j)=1}\left(\left(\theta^{(j)}\right)^{T} x^{(i)}-y^{(i, j)}\right)^{2}+\frac{\lambda}{2} \sum_{i=1}^{n_{m}} \sum_{k=1}^{n}\left(x_{k}^{(i)}\right)^{2}+\frac{\lambda}{2} \sum_{j=1}^{n_{u}} \sum_{k=1}^{n}\left(\theta_{k}^{(j)}\right)^{2} \\\min _{x^{(1)}, \ldots, x^{\left(n_{m}\right)}, \theta^{(1)}, \ldots, \theta^{\left(n_{u}\right)}} J\left(x^{(1)}, \ldots, x^{\left(n_{m}\right)}, \theta^{(1)}, \ldots, \theta^{\left(n_{u}\right)}\right)\end{array}</script><h2 id="5-向量化-低秩矩阵分解"><a href="#5-向量化-低秩矩阵分解" class="headerlink" title="5.向量化:低秩矩阵分解"></a>5.向量化:低秩矩阵分解</h2><p>举例子:</p><ol><li><p>当给出一件产品时，你能否找到与之相关的其它产品。 </p></li><li><p>一位用户最近看上一件产品，有没有其它相关的产品，你可以推荐给他。 </p></li></ol><p>我将要做的是:实现一种选择的方法，写出协同过滤算法的预测情况。 </p><p>我们有关于五部电影的数据集，我将要做的是，将这些用户的电影评分，进行分组并存到一个矩阵中。</p><p>我们有五部电影，以及四位用户，那么这个矩阵 𝑌 就是一个 5 行 4 列的矩阵，它将这些电影的用户评分数据都存在矩阵里:</p><script type="math/tex; mode=display">\begin{aligned}&\begin{array}{|l|l|l|l|l|}\hline \text { Movie } & \text { Alice (1) } & \text { Bob (2) } & \text { Carol (3) } & \text { Dave (4) } \\\hline \text { Love at last } & 5 & 5 & 0 & 0 \\\hline \text { Romance forever } & 5 & ? & ? & 0 \\\hline \text { Cute puppies of love } & ? & 4 & 0 & ? \\\hline \text { Nonstop car chases } & 0 & 0 & 5 & 4 \\\hline \text { Swords vs. karate } & 0 & 0 & 5 & ? \\\hline\end{array}\\&Y=\left[\begin{array}{llll}5 & 5 & 0 & 0 \\5 & ? & ? & 0 \\? & 4 & 0 & ? \\0 & 0 & 5 & 4 \\0 & 0 & 5 & 0\end{array}\right]\end{aligned}</script><p>推出评分:</p><script type="math/tex; mode=display">\left[\begin{array}{cccc}\left(\theta^{(1)}\right)^{T}\left(x^{(1)}\right) & \left(\theta^{(2)}\right)^{T}\left(x^{(1)}\right) & \ldots & \left(\theta^{\left(n_{u}\right)}\right)^{T}\left(x^{(1)}\right) \\\left(\theta^{(1)}\right)^{T}\left(x^{(2)}\right) & \left(\theta^{(2)}\right)^{T}\left(x^{(2)}\right) & \ldots & \left(\theta^{\left(n_{u}\right)}\right)^{T}\left(x^{(2)}\right) \\\vdots & \vdots & \vdots & \vdots \\\left(\theta^{(1)}\right)^{T}\left(x^{\left(n_{m}\right)}\right) & \left(\theta^{(2)}\right)^{T}\left(x^{\left(n_{m}\right)}\right) & \ldots & \left(\theta^{\left(n_{u}\right)}\right)^{T}\left(x^{\left(n_{m}\right)}\right)\end{array}\right]</script><p>找到相关影片:</p><p>现在既然你已经对特征参数向量进行了学习,那么我们就会有一个很方便的方法来度量两部电影之间的相似性。</p><p>例如说: 电影 $i$ 有一个特征向量 $x^{(i)},$ 你是否能找到一部不同的电影 $j,$ 保证两部电影的特征向量之间的距离 $x^{(i)}$ 和 $x^{(j)}$ 很小，那就能很有力地表明电影 $i$ 和电影 $j$ 在某种程度上有相似，至少在某种意义上, 某些人喜欢电影 $i,$ 或许更有可能也对电影$j$ 感兴趣。</p><p>总结一下，当用户在看某部电影 $i$ 的时候，如果你想找 5 部与电影非常相似的电影，为了能给用户推荐 5 部新电影，你需要做的是找出电影 $j,$ 在这些不同的电影中与我们要找的电影 $i$ 的距离最小，这样你就能给你的用户推荐几部不同的电影了。</p><p>通过这个方法，希望你能知道，如何进行一个向量化的计算来对所有的用户和所有的电影进行评分计算。同时希望你也能掌握,通过学习特征参数, 来找到相关电影和产品的方法。</p><h2 id="6-推行工作上的细节-均值归一化"><a href="#6-推行工作上的细节-均值归一化" class="headerlink" title="6.推行工作上的细节:均值归一化"></a>6.推行工作上的细节:均值归一化</h2><p>让我们来看下面的用户评分数据:</p><script type="math/tex; mode=display">\begin{aligned}&\begin{array}{|l|l|l|l|l|}\hline \text { Movie } & \text { Alice (1) } & \text { Bob (2) } & \text { Carol (3) } & \text { Dave (4) } \\\hline \text { Love at last } & 5 & 5 & 0 & 0 \\\hline \text { Romance forever } & 5 & ? & ? & 0 \\\hline \text { Cute puppies of love } & ? & 4 & 0 & ? \\\hline \text { Nonstop car chases } & 0 & 0 & 5 & 4 \\\hline \text { Swords vs. karate } & 0 & 0 & 5 & ? \\\hline\end{array}\\&Y=\left[\begin{array}{llll}5 & 5 & 0 & 0 \\5 & ? & ? & 0 \\? & 4 & 0 & ? \\0 & 0 & 5 & 4 \\0 & 0 & 5 & 0\end{array}\right]\end{aligned}</script><p>如果我们新增一个用户 Eve，并且 Eve 没有为任何电影评分，那么我们以什么为依据为 Eve 推荐电影呢?</p><p>我们首先需要对结果$ 𝑌$矩阵进行均值归一化处理，将每一个用户对某一部电影的评分减去所有用户对该电影评分的平均值:</p><script type="math/tex; mode=display">Y=\left[\begin{array}{ccccc}5 & 5 & 0 & 0 & ? \\5 & ? & ? & 0 & ? \\? & 4 & 0 & ? & ? \\0 & 0 & 5 & 4 & ? \\0 & 0 & 5 & 0 & ?\end{array}\right] \quad \mu=\left[\begin{array}{c}2.5 \\2.5 \\2 \\2.25 \\1.25\end{array}\right] \rightarrow Y=\left[\begin{array}{ccccc}2.5 & 2.5 & -2.5 & -2.5 & ? \\2.5 & ? & ? & -2.5 & ? \\? & 2 & -2 & ? & ? \\-2.25 & -2.25 & 2.75 & 1.75 & ? \\-1.25 & -1.25 & 3.75 & -1.25 & ?\end{array}\right]</script><p>然后我们利用这个新的 $Y$ 矩阵来训练算法。 如果我们要用新训练出的算法来预测评分，则需要将平均值重新加回去，预测 $\left(\theta^{(j)}\right)^{T} x^{(i)}+\mu_{i},$ 对于 Eve，我们的新模型会认为她给每部电影的评分都是该电影的平均分。</p>]]></content>
      
      
      <categories>
          
          <category> 吴恩达——机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 推荐系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>09_异常检测(Anomaly Detection)</title>
      <link href="2020/12/10/machinelearning/wu-en-da-ji-qi-xue-xi/09-anomaly-detection/"/>
      <url>2020/12/10/machinelearning/wu-en-da-ji-qi-xue-xi/09-anomaly-detection/</url>
      
        <content type="html"><![CDATA[<h2 id="1-问题的动机"><a href="#1-问题的动机" class="headerlink" title="1.问题的动机"></a>1.问题的动机</h2><p>异常检测(Anomaly detection)问题是机器学习算法的一个常见应用。这种算法的一个有趣之处在于:它虽然主要用于非监督学习问 题，但从某些角度看，它又类似于一些监督学习问题。</p><p> 例如你是一个飞机引擎制造商，当你生产的飞机引擎从生产线上流出时，你需要进行质量控制测试，而作为这个测试的一部分，你测量了飞机引擎的一些特征变量，比如引擎运转时产生的热量，或者引擎的振动等等。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201209163209.png" alt="image-20201209163209508"></p><p>这样一来，你就有了一个数据集，从$𝑥^{(1)}$到$𝑥^{(𝑚)}$，如果你生产了$𝑚$个引擎的话，你将这些数据绘制成图表，看起来就是这个样子:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201209163305.png" alt="image-20201209163305193"></p><p>这里的每个点、每个叉，都是你的无标签数据。这样，异常检测问题可以定义如下:我们假设后来有一天，你有一个新的飞机引擎从生产线上流出，而你的新飞机引擎有特征变量$𝑥_{𝑡𝑒𝑠𝑡}$ 。所谓的异常检测问题就是:我们希望知道这个新的飞机引擎是否有某种异常，或者说，我们希望判断这个引擎是否需要进一步测试。因为，如果它看起来像一个正常的引擎，那么 我们可以直接将它运送到客户那里，而不需要进一步的测试。</p><p>给定数据集 $x^{(1)}, x^{(2)}, \ldots, x^{(m)},$ 我们假使数据集是正常的, 我们希望知道新的数据 $x_{\text {test}}$是不是异常的, 即这个测试数据不属于该组数据的几率如何。我们所构建的模型应该能根据该测试数据的位置告诉我们其属于一组数据的可能性 $p(x)$ 。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201209163429.png" alt="image-20201209163429830"></p><p>上图中，在蓝色圈内的数据属于该组数据的可能性较高，而越是偏远的数据，其属于该组数据的可能性就越低。<br>这种方法称为密度估计，表达如下:</p><script type="math/tex; mode=display">\text { if } \quad p(x)\left\{\begin{array}{ll}<\varepsilon & \text { anomal } y \\>=\varepsilon & \text { normal }\end{array}\right.</script><p>欺计检测: $x^{(i)}=$ 用户的第 $i$ 个活动特征模型 $p(x)$ 为我们其属于一组数据的可能性，通过 $p(x)&lt;\varepsilon$ 检测非正常用户。</p><p>异常检测主要用来识别欺骗。例如在线采集而来的有关用户的数据，一个特征向量中可能会包含如: 用户多久登录一次，访问过的页面，在论坛发布的帖子数量，甚至是打字速度等。</p><p>尝试根据这些特征构建一个模型，可以用这个模型来识别那些不符合该模式的用户。再一个例子是检测一个数据中心，特征可能包含：内存使用情况，被访问的磁盘数量,CPU 的负载，网络的通信量等。根据这些特征可以构建一个模型，用来判断某些计算机是不是有可能出错了。</p><h2 id="2-高斯分布"><a href="#2-高斯分布" class="headerlink" title="2.高斯分布"></a>2.高斯分布</h2><p>高斯分布，也称为正态分布。</p><p>通常如果我们认为变量 $x$ 符合高斯分布 $x \sim N\left(\mu, \sigma^{2}\right)$ 则其概率密度函数为$p\left(x, \mu, \sigma^{2}\right)=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right)$ 。</p><p>我们可以利用已有的数据来预测总体中的 $\mu$ 和 $\sigma^{2}$ 的计算方法如下 : </p><script type="math/tex; mode=display">\quad \mu=\frac{1}{m} \sum_{i=1}^{m} x^{(i)}</script><script type="math/tex; mode=display">\sigma^{2}=\frac{1}{m} \sum_{i=1}^{m}\left(x^{(i)}-\mu\right)^{2}</script><p>高斯分布样例:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201210103351.png" alt="高斯分布样例"></p><h2 id="3-算法"><a href="#3-算法" class="headerlink" title="3.算法"></a>3.算法</h2><p>应用高斯分布开发异常检测算法。</p><p>对于给定的数据集 $x^{(1)}, x^{(2)}, \ldots, x^{(m)},$ 我们要针对每一个特征计算 $\mu$ 和 $\sigma^{2}$ 的估计值。</p><script type="math/tex; mode=display">\begin{array}{c}\mu_{j}=\frac{1}{m} \sum_{i=1}^{m} x_{j}^{(i)} \\\sigma_{j}^{2}=\frac{1}{m} \sum_{i=1}^{m}\left(x_{j}^{(i)}-\mu_{j}\right)^{2}\end{array}</script><p>一旦我们获得了平均值和方差的估计值,给定新的一个训练实例,根据模型计算 $p(x)$ :</p><script type="math/tex; mode=display">p(x)=\prod_{j=1}^{n} p\left(x_{j} ; \mu_{j}, \sigma_{j}^{2}\right)=\prod_{j=1}^{1} \frac{1}{\sqrt{2 \pi} \sigma_{j}} \exp \left(-\frac{\left(x_{j}-\mu_{j}\right)^{2}}{2 \sigma_{j}^{2}}\right)</script><p>当 $p(x)&lt;\varepsilon$ 时，为异常。</p><p>下图是一个由两个特征的训练集，以及特征的分布情况:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201210103633.png" alt="两个特征的训练集"></p><p>下面的三维图表表示的是密度估计函数，𝑧轴为根据两个特征的值所估计$𝑝(𝑥)$值:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201210103824.png" alt="密度估计函数"></p><p>我们选择一个 $\varepsilon$， 将 $p(x)=\varepsilon$ 作为我们的判定边界，当 $p(x)&gt;\varepsilon$ 时预测数据为正常数据，否则为异常。<br>在这段视频中，我们介绍了如何拟合 $p(x)$, 也就是 $x$ 的概率值，以开发出一种异常检测算法。同时，在这节课中，我们也给出了通过给出的数据集拟合参数，进行参数估计，得到参数 $\mu$ 和 $\sigma,$ 然后检测新的样本，确定新样本是否是异常。</p><h2 id="4-开发和评价一个异常检测系统"><a href="#4-开发和评价一个异常检测系统" class="headerlink" title="4.开发和评价一个异常检测系统"></a>4.开发和评价一个异常检测系统</h2><p>异常检测算法是一个非监督学习算法，意味着我们无法根据结果变量 𝑦 的值来告诉我 们数据是否真的是异常的。我们需要另一种方法来帮助检验算法是否有效。当我们开发一个异常检测系统时，我们从带标记（异常或正常）的数据着手，我们从其中选择一部分正常数据用于构建训练集，然后用剩下的正常数据和异常数据混合的数据构成交叉检验集和测试集。</p><p>例如:我们有 10000 台正常引擎的数据，有 20 台异常引擎的数据。 我们这样分配数据:</p><ul><li><p>6000 台正常引擎的数据作为训练集</p></li><li><p>2000 台正常引擎和 10 台异常引擎的数据作为交叉检验集</p></li><li><p>2000 台正常引擎和 10 台异常引擎的数据作为测试集</p></li></ul><p>具体的评价方法如下:</p><ol><li>根据测试集数据，我们估计特征的平均值和方差并构建$𝑝(𝑥)$函数</li><li>对交叉检验集，我们尝试使用不同的$\varepsilon,$值作为阀值，并预测数据是否异常，根据F1值或者查准率与查全率的比例来选择 $\varepsilon,$</li><li>选出$\varepsilon,$ 后，针对测试集进行预测，计算异常检验系统的$𝐹1$值，或者查准率与查全率</li></ol><p>之比。</p><h2 id="5-异常检测与监督学习对比"><a href="#5-异常检测与监督学习对比" class="headerlink" title="5.异常检测与监督学习对比"></a>5.异常检测与监督学习对比</h2><p>之前我们构建的异常检测系统也使用了带标记的数据，与监督学习有些相似，下面的对 比有助于选择采用监督学习还是异常检测:</p><p>两者比较:</p><div class="table-container"><table><thead><tr><th>异常检测</th><th style="text-align:left">监督学习</th></tr></thead><tbody><tr><td>非常少量的正向类（异常数据 𝑦 = 1）, 大量的负向类(𝑦 = 0)</td><td style="text-align:left">同时有大量的正向类和负向类</td></tr><tr><td>许多不同种类的异常，非常难。根据非常少 量的正向类数据来训练算法。</td><td style="text-align:left">有足够多的正向类实例，足够用于训练算法，未来遇到的正向类实例可能与训练集中的非常近似。</td></tr><tr><td>未来遇到的异常可能与已掌握的异常、非常的不同。</td><td style="text-align:left"></td></tr><tr><td>例如:欺诈行为检测生产（例如飞机引擎） 检测数据中心的计算机运行状况</td><td style="text-align:left">例如:邮件过滤器 天气预报 肿瘤分类</td></tr></tbody></table></div><h2 id="6-选择特征"><a href="#6-选择特征" class="headerlink" title="6.选择特征"></a>6.选择特征</h2><p>对于异常检测算法，我们使用的特征是至关重要的，下面谈谈如何选择特征:</p><p>异常检测假设特征符合高斯分布，如果数据的分布不是高斯分布，异常检测算法也能够 工作，但是最好还是将数据转换成高斯分布，例如使用对数函数:$𝑥 = 𝑙𝑜𝑔(𝑥 + 𝑐)$，其中 $𝑐$ 为非负常数; 或者$ 𝑥 = 𝑥^𝑐$，𝑐为 0-1 之间的一个分数，等方法。</p><p>（注:在 python 中，通常用 np.log1p()函数，$𝑙𝑜𝑔1𝑝$就是 $𝑙𝑜𝑔(𝑥 + 1)$，可以避免 出现负数结果，反向函数就是 np.expm1（））</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201210112512.png" alt="image-20201210112512865"></p><p>误差分析:</p><p>一个常见的问题是一些异常的数据可能也会有较高的𝑝(𝑥)值，因而被算法认为是正常的。 这种情况下误差分析能够帮助我们，我们可以分析那些被算法错误预测为正常的数据，观察能否找出一些问题。我们可能能从问题中发现我们需要增加一些新的特征，增加这些新特征后获得的新算法能够帮助我们更好地进行异常检测。</p><p>我们通常可以通过将一些相关的特征进行组合，来获得一些新的更好的特征(异常数据 的该特征值异常地大或小)，例如，在检测数据中心的计算机状况的例子中，我们可以用 CPU 负载与网络通信量的比例作为一个新的特征，如果该值异常地大，便有可能意味着该服务器 是陷入了一些问题中。</p><p>在这段视频中，我们介绍了如何选择特征，以及对特征进行一些小小的转换，让数据更 像正态分布，然后再把数据输入异常检测算法。同时也介绍了建立特征时，进行的误差分析 方法，来捕捉各种异常的可能。希望你通过这些方法，能够了解如何选择好的特征变量，从 而帮助你的异常检测算法，捕捉到各种不同的异常情况。</p><h2 id="7-多元高斯分布"><a href="#7-多元高斯分布" class="headerlink" title="7.多元高斯分布"></a>7.多元高斯分布</h2><p>假使我们有两个相关的特征，而且这两个特征的值域范围比较宽，这种情况下，一般的 高斯分布模型可能不能很好地识别异常数据。其原因在于，一般的高斯分布模型尝试的是去 同时抓住两个特征的偏差，因此创造出一个比较大的判定边界。</p><p>下图中是两个相关特征，洋红色的线（根据$\varepsilon$的不同其范围可大可小）是一般的高斯分布模型获得的判定边界，很明显绿色的$X$所代表的数据点很可能是异常值，但是其$𝑝(𝑥)$值却 仍然在正常范围内。多元高斯分布将创建像图中蓝色曲线所示的判定边界。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201210132534.png" alt="判定边界"></p><p>在一般的高斯分布模型中，我们计算$ 𝑝(𝑥)$ 的方法是: 通过分别计算每个特征对应的几率然后将其累乘起来，在多元高斯分布模型中，我们将构建特征的协方差矩阵，用所有的特征一起来计算 $𝑝(𝑥)$。</p><p>我们首先计算所有特征的平均值，然后再计算协方差矩阵:</p><script type="math/tex; mode=display">\begin{array}{c}p(x)=\prod_{j=1}^{n} p\left(x_{j} ; \mu, \sigma_{j}^{2}\right)=\prod_{j=1}^{n} \frac{1}{\sqrt{2 \pi} \sigma_{j}} \exp \left(-\frac{\left(x_{j}-\mu_{j}\right)^{2}}{2 \sigma_{j}^{2}}\right) \\\mu=\frac{1}{m} \sum_{i=1}^{m} x^{(i)} \\\Sigma=\frac{1}{m} \sum_{i=1}^{m}\left(x^{(i)}-\mu\right)\left(x^{(i)}-\mu\right)^{T}=\frac{1}{m}(X-\mu)^{T}(X-\mu)\end{array}</script><p>注:其中 $\mu$ 是一个向量，其每一个单元都是原特征矩阵中一行数据的均值。最后我们计算多元高斯分布的p(x): </p><script type="math/tex; mode=display">p(x)=\frac{1}{(2 \pi)^{\frac{n}{2}|\Sigma|^{\frac{1}{2}}}} \exp \left(-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)\right)</script><p>其中:</p><p>$|\Sigma|$是定矩阵，在 Octave 中用 det(sigma)计算</p><p>$\Sigma^{-1}$ 是逆矩阵，下面我们来看看协方差矩阵是如何影响模型的:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201210132934.png" alt="协方差矩阵对模型的影响"></p><p>上图是 5 个不同的模型，从左往右依次分析:</p><ol><li><p>是一个一般的高斯分布模型</p></li><li><p>通过协方差矩阵，令特征 1 拥有较小的偏差，同时保持特征 2 的偏差</p></li><li><p>通过协方差矩阵，令特征 2 拥有较大的偏差，同时保持特征 1 的偏差</p></li><li><p>通过协方差矩阵，在不改变两个特征的原有偏差的基础上，增加两者之间的正相关性</p></li><li><p>通过协方差矩阵，在不改变两个特征的原有偏差的基础上，增加两者之间的负相关性</p></li></ol><p>多元高斯分布模型与原高斯分布模型的关系: </p><p>可以证明的是，原本的高斯分布模型是多元高斯分布模型的一个子集，即像上图中的第1、2、3，3 个例子所示，如果协方差矩阵只在对角线的单位上有非零的值时，即为原本的高斯分布模型了。</p><p>原高斯分布模型和多元高斯分布模型的比较:</p><div class="table-container"><table><thead><tr><th>原高斯分布模型</th><th>多元高斯分布模型</th></tr></thead><tbody><tr><td>不能捕捉特征之间的相关性但可以通过将特征进行组合的方法来解决</td><td>自动捕捉特征之间的相关性</td></tr><tr><td>计算代价低，能适应大规模的特征</td><td>计算代价较高 训练集较小时也同样适用</td></tr><tr><td></td><td>必须要有 𝑚 &gt; 𝑛，不然的话协方差矩阵不可逆的，通常需要 𝑚 &gt; 10𝑛 另外特征冗余 也会导致协方差矩阵不可逆</td></tr></tbody></table></div><h2 id="8-使用多元高斯分布进行异常检测"><a href="#8-使用多元高斯分布进行异常检测" class="headerlink" title="8.使用多元高斯分布进行异常检测"></a>8.使用多元高斯分布进行异常检测</h2><p>回顾一下多元高斯分布和多元正态分布:</p><script type="math/tex; mode=display">p(x ; \mu, \Sigma)=\frac{1}{(2 \pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}} \exp \left(-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)\right)</script><p>分布有两个参数, $\mu$ 和 $\Sigma$。其中$\mu$这一个$n$维向量和 $\Sigma$ 的协方差矩阵，是一种 $n \times n$ 的矩阵。而这里的公式$x$的概率，如按 $\mu$ 和参数化 $\Sigma,$ 和你的变量 $\mu$ 和 $\Sigma,$ 你可以得到一个范围的不同分布一样。<br>我有一组样本 $x^{(1)}, x^{(2)}, \ldots, x^{(m)}$ 是一个 $n$ 维向量，我想我的样本来自一个多元高斯分布。我如何尝试估计我的参数 $\mu$ 和 $\Sigma$ 以及标准公式?</p><p>估计他们是你设置 $\mu$ 是你的训练样本的平均值。</p><script type="math/tex; mode=display">\mu=\frac{1}{m} \sum_{i=1}^{m} x^{(i)}</script><p>并设置 $\Sigma$ :</p><script type="math/tex; mode=display">\Sigma=\frac{1}{m} \sum_{i=1}^{m}\left(x^{(i)}-\mu\right)\left(x^{(i)}-\mu\right)^{T}</script><p>这其实只是当我们使用 PCA 算法时候, 有 $\Sigma$ 时写出来。所以你只需插入上述两个公式,这会给你你估计的参数 $\mu$ 和你估计的参数 $\Sigma$ 。所以, 这里给出的数据集是你如何估计 $\mu$ 和 $\Sigma$ 。让我们以这种方法而只需将其插入到异常检测算法。</p><p>那么，我们如何把所有这一切共同开发一个异常检测算法?</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201210133723.png" alt="异常检测算法"></p>]]></content>
      
      
      <categories>
          
          <category> 吴恩达——机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 异常检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>08_降维_Dimensionality_Reduction_</title>
      <link href="2020/12/09/machinelearning/wu-en-da-ji-qi-xue-xi/08-dimensionality-reduction-2/"/>
      <url>2020/12/09/machinelearning/wu-en-da-ji-qi-xue-xi/08-dimensionality-reduction-2/</url>
      
        <content type="html"><![CDATA[<h1 id="降维（DimensionalityReduction）"><a href="#降维（DimensionalityReduction）" class="headerlink" title="降维（DimensionalityReduction）"></a>降维（DimensionalityReduction）</h1><h2 id="1-动机一-数据压缩"><a href="#1-动机一-数据压缩" class="headerlink" title="1.动机一:数据压缩"></a>1.动机一:数据压缩</h2><p>第二种类型的无监督学习问题，称为降维。有几个不同的的原 因使你可能想要做降维。一是数据压缩，后面我们会看了一些视频后，数据压缩不仅允许我 们压缩数据，因而使用较少的计算机内存或磁盘空间，但它也让我们加快我们的学习算法。</p><p>但首先，让我们谈论降维是什么。作为一种生动的例子，我们收集的数据集，有许多特征，我绘制两个在这里。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201209151927.png" alt="image-20201209151927763"></p><p>假设我们未知两个的特征:$𝑥_1$ :长度:用厘米表示;$𝑥_2$ :是用英寸表示同一物体的长度。</p><p>所以，这给了我们高度冗余表示，也许不是两个分开的特征$𝑥_1$ 和$𝑥_2$ ，这两个基本的长度 度量，也许我们想要做的是减少数据到一维，只有一个数测量这个长度。这个例子似乎有点做作，这里厘米英寸的例子实际上不是那么不切实际的，两者并没有什么不同。</p><p>将数据从二维降至一维: 假使我们要采用两种不同的仪器来测量一些东西的尺寸，其中一个仪器测量结果的单位是英寸，另一个仪器测量的结果是厘米，我们希望将测量的结果作为我们机器学习的特征。现在的问题的是，两种仪器对同一个东西测量的结果不完全相等 (由于误差、精度等)，而将两者都作为特征有些重复，因而，我们希望将这个二维的数据降至一维。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201209155836.png" alt="image-20201209152203718"></p><p>做一个调查或做这些不同飞行员的测试——你可能有一个特征:$𝑥_1$ ，这也许是他们的技能（直升机飞行员），也许$𝑥_2$ 可能是飞行员的爱好。这是表示他们是否喜欢飞行，也许这两个特征将高度相关。你真正关心的可能是这条红线的方向，不同的特征，决定飞行员的能力。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201209152343.png" alt="image-20201209152343815"></p><p>将数据从三维降至二维: 这个例子中我们要将一个三维的特征向量降至一个二维的特 征向量。过程是与上面类似的，我们将三维向量投射到一个二维的平面上，强迫使得所有的 数据都在同一个平面上，降至二维的特征向量。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201209155851.png" alt="image-20201209152436504"></p><h2 id="2-动机二-数据可视化"><a href="#2-动机二-数据可视化" class="headerlink" title="2.动机二:数据可视化"></a>2.动机二:数据可视化</h2><p>在许多及其学习问题中，如果我们能将数据可视化，我们便能寻找到一个更好的解决方案，降维可以帮助我们。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201209152521.png" alt="image-20201209152521627"></p><p>假使我们有有关于许多不同国家的数据，每一个特征向量都有 50 个特征（如 GDP，人 均GDP，平均寿命等）。如果要将这个 50 维的数据可视化是不可能的。使用降维的方法将其降至 2 维，我们便可以将其可视化了。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201209153239.png" alt="image-20201209153239202"></p><p>这样做的问题在于，降维的算法只负责减少维数，新产生的特征的意义就必须由我们自己去发现了。</p><h2 id="3-主成分分析问题"><a href="#3-主成分分析问题" class="headerlink" title="3.主成分分析问题"></a>3.主成分分析问题</h2><p>主成分分析（PCA）是最常见的降维算法。</p><p>在 PCA 中，我们要做的是找到一个方向向量（Vector direction），当我们把所有的数据都投射到该向量上时，我们希望投射平均均方误差能尽可能地小。方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201209153450.png" alt="image-20201209153449985"></p><p>问题是要将n维数据降至 $k$ 维，目标是找到向量 $u^{(1)}, u^{(2)}, \ldots, u^{(k)}$ 使得总的投射误差最小。主成分分析与线性回顾的比较:</p><p>主成分分析与线性回归是两种不同的算法。主成分分析最小化的是投射误差（Projected Error），而线性回归尝试的是最小化预测误差。线性回归的目的是预测结果，而主成分分析不作任何预测。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201209153610.png" alt="image-20201209153610882"></p><p>上图中，左边的是线性回归的误差（垂直于横轴投影），右边则是主要成分分析的误差 （垂直于红线投影）。</p><p>PCA 将𝑛个特征降维到𝑘个，可以用来进行数据压缩，如果 100 维的向量最后可以用 10 维来表示，那么压缩率为 90%。同样图像处理领域的 KL 变换使用 PCA 做图像压缩。但 PCA 要保证降维后，还要保证数据的特性损失最小。</p><p>PCA 技术的一大好处是对数据进行降维的处理。我们可以对新求出的“主元”向量的重要 性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。</p><p>PCA 技术的一个很大的优点是，它是完全无参数限制的。在 PCA 的计算过程中完全不 需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。</p><p>但是，这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识，掌握 数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果， 效率也不高。</p><h2 id="4-主成分分析算法"><a href="#4-主成分分析算法" class="headerlink" title="4.主成分分析算法"></a>4.主成分分析算法</h2><p>PCA 减少 $n$ 维到 $k$ 维 :</p><ul><li><p>第一步是均值归一化。我们需要计算出所有特征的均值，然后令 $x_{j}=x_{j}-\mu_{j}$。 如果特征是在不同的数量级上，我们还需要将其除以标准差 $\sigma^{2}$ 。</p></li><li><p>第二步是计算协方差矩阵（covariance matrix ）</p><script type="math/tex; mode=display">\Sigma=\frac{1}{m} \sum_{i=1}^{n}\left(x^{(i)}\right)\left(x^{(i)}\right)^{T}</script><p>补充知识：</p><p>协方差为：</p><script type="math/tex; mode=display">\sigma\left(x_{m}, x_{k}\right)=\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{m i}-\bar{x}_{m}\right)\left(x_{k i}-\bar{x}_{k}\right)</script><p>因此，协方差矩阵为：</p><script type="math/tex; mode=display">\Sigma=\left[\begin{array}{ccc}\sigma\left(x_{1}, x_{1}\right) & \cdots & \sigma\left(x_{1}, x_{d}\right) \\ \vdots & \ddots & \vdots \\ \sigma\left(x_{d}, x_{1}\right) & \cdots & \sigma\left(x_{d}, x_{d}\right)\end{array}\right] \in \mathbb{R}^{d \times d}</script></li><li><p>第三步是计算协方差矩阵 $\Sigma$ 的特征向量（eigenvectors）:</p></li><li><p>利用奇异值分解（singularvaluedecomposition）来求解$[U,S, V]= svd(sigma)$:</p></li></ul><p>对于一个 $n \times n$ 维度的矩阵，上式中的 $U$ 是一个具有与数据之间最小投射误差的方向向量构成的矩阵。如果我们希望将数据从n维降至k维，我们只需要从U中选取前k个向量，获得一个 $n \times k$ 维度的矩阵, 我们用 $U_{r e d u c e}$ 表示，然后通过如下计算获得要求的新特征向量 $z^{(i)}:z^{(i)}=U_{r e d u c e}^{T} * x^{(i)}$</p><p>其中$𝑥$是$𝑛 × 1$维的，因此结果为$𝑘 × 1$维度。注，我们不对方差特征进行处理。</p><p>PCA 算法，我们可能有一个这样的样本。如图中样本 $x^{(1)}, x^{(2)}$ 。我们做的是，我们把这些样本投射到图中这个一维平面。然后现在我们需要只使用一个实数，比如z $^{(1)}$ ，指定这些点的位置后他们被投射到这一个三维曲面。给定一个点 $z^{(1)},$ 我们怎么能回去这个原始的二维空间呢？x为 2 维，z 为 1 维， $z=U_{r e d u c e}^{T} x,$ 相反的方程为:</p><script type="math/tex; mode=display">x_{\text {appox}}=U_{\text {reduce}} \cdot z, x_{\text {appox}} \approx x_{\circ}</script><h2 id="5-选择主成分的数量"><a href="#5-选择主成分的数量" class="headerlink" title="5.选择主成分的数量"></a>5.选择主成分的数量</h2><p>主要成分分析是减少投射的平均均方误差:</p><p>训练集的方差为: $\frac{1}{m} \sum_{i=1}^{m}\left|x^{(i)}\right|^{2}$</p><p>我们希望在平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的k值。如果我们希望这个比例小于$1\%$，就意味着原本数据的偏差有 $99\%$都保留下来了，如果我们选择保留$ 95\%$的偏差，便能非常显著地降低模型中特征的维度了。</p><p>我们可以先令 $k=1$, 然后进行主要成分分析，获得 $U_{r e d u c e}$ 和 $z$, 然后计算比例是否小于$1\%$。如果不是的话再令$k = 2$，如此类推，直到找到可以使得比例小于 $1\%$的最小 $k$ 值 （原因是各个特征之间通常情况存在某种相关性）。</p><p>对于之前的SVD函数，其中返回的$𝑆$是一个$𝑛 × 𝑛$的矩阵，只有对角线上有值，而其它单元都是 0，我们可以使用这个矩阵来计算平均均方误差与训练集方差的比例:</p><script type="math/tex; mode=display">\frac{\frac{1}{m} \sum_{i=1}^{m}\left\|x^{(i)}-x_{a p p r o x}^{(i)}\right\|^{2}}{\frac{1}{m} \sum_{i=1}^{m}\left\|x^{(i)}\right\|^{2}}=1-\frac{\sum_{i=1}^{k} s_{i i}}{\sum_{i=1}^{n} s_{i i}} \leq 1 \%</script><p>也就是:</p><script type="math/tex; mode=display">\frac{\sum_{i=1}^{k} s_{i i}}{\sum_{i=1}^{n} s_{i i}} \geq 0.99</script><p>在压缩过数据后，我们可以采用如下方法来近似地获得原有的特征:</p><script type="math/tex; mode=display">x_{a p p r o x}^{(i)}=U_{r e d u c e} z^{(i)}</script><h2 id="6-主成分分析法的应用建议"><a href="#6-主成分分析法的应用建议" class="headerlink" title="6.主成分分析法的应用建议"></a>6.主成分分析法的应用建议</h2><p>假使我们正在针对一张 100×100 像素的图片进行某个计算机视觉的机器学习，即总共 有 10000 个特征。</p><ol><li>第一步是运用主要成分分析将数据压缩至 1000 个特征</li><li>然后对训练集运行学习算法。</li><li>在预测时，采用之前学习而来的$𝑈_{𝑟𝑒𝑑𝑢𝑐𝑒}$将输入的特征𝑥转换成特征向量𝑧，然后再进行预测</li></ol><p>注:如果我们有交叉验证集合测试集，也采用对训练集学习而来的$𝑈_{𝑟𝑒𝑑𝑢𝑐𝑒}$ 。 错误的主要成分分析情况:一个常见错误使用主要成分分析的情况是，将其用于减少过</p><p>拟合（减少了特征的数量）。这样做非常不好，不如尝试正则化处理。原因在于主要成分分析只是近似地丢弃掉一些特征，它并不考虑任何与结果变量有关的信息，因此可能会丢失非 常重要的特征。然而当我们进行正则化处理时，会考虑到结果变量，不会丢掉重要的数据。</p><p>另一个常见的错误是，默认地将主要成分分析作为学习过程中的一部分，这虽然很多时候有效果，最好还是从所有原始特征开始，只在有必要的时候（算法运行太慢或者占用太多内存）才考虑采用主要成分分析。</p>]]></content>
      
      
      <categories>
          
          <category> 吴恩达——机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 降维 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>08_降维_Dimensionality_Reduction_</title>
      <link href="2020/12/09/machinelearning/wu-en-da-ji-qi-xue-xi/08-dimensionality-reduction/"/>
      <url>2020/12/09/machinelearning/wu-en-da-ji-qi-xue-xi/08-dimensionality-reduction/</url>
      
        <content type="html"><![CDATA[<h1 id="降维（DimensionalityReduction）"><a href="#降维（DimensionalityReduction）" class="headerlink" title="降维（DimensionalityReduction）"></a>降维（DimensionalityReduction）</h1><h2 id="1-动机一-数据压缩"><a href="#1-动机一-数据压缩" class="headerlink" title="1.动机一:数据压缩"></a>1.动机一:数据压缩</h2><p>第二种类型的无监督学习问题，称为降维。有几个不同的的原 因使你可能想要做降维。一是数据压缩，后面我们会看了一些视频后，数据压缩不仅允许我 们压缩数据，因而使用较少的计算机内存或磁盘空间，但它也让我们加快我们的学习算法。</p><p>但首先，让我们谈论降维是什么。作为一种生动的例子，我们收集的数据集，有许多特征，我绘制两个在这里。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201209151927.png" alt="image-20201209151927763"></p><p>假设我们未知两个的特征:$𝑥_1$ :长度:用厘米表示;$𝑥_2$ :是用英寸表示同一物体的长度。</p><p>所以，这给了我们高度冗余表示，也许不是两个分开的特征$𝑥_1$ 和$𝑥_2$ ，这两个基本的长度 度量，也许我们想要做的是减少数据到一维，只有一个数测量这个长度。这个例子似乎有点做作，这里厘米英寸的例子实际上不是那么不切实际的，两者并没有什么不同。</p><p>将数据从二维降至一维: 假使我们要采用两种不同的仪器来测量一些东西的尺寸，其中一个仪器测量结果的单位是英寸，另一个仪器测量的结果是厘米，我们希望将测量的结果作为我们机器学习的特征。现在的问题的是，两种仪器对同一个东西测量的结果不完全相等 (由于误差、精度等)，而将两者都作为特征有些重复，因而，我们希望将这个二维的数据降至一维。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201209155836.png" alt="image-20201209152203718"></p><p>做一个调查或做这些不同飞行员的测试——你可能有一个特征:$𝑥_1$ ，这也许是他们的技能（直升机飞行员），也许$𝑥_2$ 可能是飞行员的爱好。这是表示他们是否喜欢飞行，也许这两个特征将高度相关。你真正关心的可能是这条红线的方向，不同的特征，决定飞行员的能力。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201209152343.png" alt="image-20201209152343815"></p><p>将数据从三维降至二维: 这个例子中我们要将一个三维的特征向量降至一个二维的特 征向量。过程是与上面类似的，我们将三维向量投射到一个二维的平面上，强迫使得所有的 数据都在同一个平面上，降至二维的特征向量。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201209155851.png" alt="image-20201209152436504"></p><h2 id="2-动机二-数据可视化"><a href="#2-动机二-数据可视化" class="headerlink" title="2.动机二:数据可视化"></a>2.动机二:数据可视化</h2><p>在许多及其学习问题中，如果我们能将数据可视化，我们便能寻找到一个更好的解决方案，降维可以帮助我们。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201209152521.png" alt="image-20201209152521627"></p><p>假使我们有有关于许多不同国家的数据，每一个特征向量都有 50 个特征（如 GDP，人 均GDP，平均寿命等）。如果要将这个 50 维的数据可视化是不可能的。使用降维的方法将其降至 2 维，我们便可以将其可视化了。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201209153239.png" alt="image-20201209153239202"></p><p>这样做的问题在于，降维的算法只负责减少维数，新产生的特征的意义就必须由我们自己去发现了。</p><h2 id="3-主成分分析问题"><a href="#3-主成分分析问题" class="headerlink" title="3.主成分分析问题"></a>3.主成分分析问题</h2><p>主成分分析（PCA）是最常见的降维算法。</p><p>在 PCA 中，我们要做的是找到一个方向向量（Vector direction），当我们把所有的数据都投射到该向量上时，我们希望投射平均均方误差能尽可能地小。方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201209153450.png" alt="image-20201209153449985"></p><p>问题是要将n维数据降至 $k$ 维，目标是找到向量 $u^{(1)}, u^{(2)}, \ldots, u^{(k)}$ 使得总的投射误差最小。主成分分析与线性回顾的比较:</p><p>主成分分析与线性回归是两种不同的算法。主成分分析最小化的是投射误差（Projected Error），而线性回归尝试的是最小化预测误差。线性回归的目的是预测结果，而主成分分析不作任何预测。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201209153610.png" alt="image-20201209153610882"></p><p>上图中，左边的是线性回归的误差（垂直于横轴投影），右边则是主要成分分析的误差 （垂直于红线投影）。</p><p>PCA 将𝑛个特征降维到𝑘个，可以用来进行数据压缩，如果 100 维的向量最后可以用 10 维来表示，那么压缩率为 90%。同样图像处理领域的 KL 变换使用 PCA 做图像压缩。但 PCA 要保证降维后，还要保证数据的特性损失最小。</p><p>PCA 技术的一大好处是对数据进行降维的处理。我们可以对新求出的“主元”向量的重要 性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。</p><p>PCA 技术的一个很大的优点是，它是完全无参数限制的。在 PCA 的计算过程中完全不 需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。</p><p>但是，这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识，掌握 数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果， 效率也不高。</p><h2 id="4-主成分分析算法"><a href="#4-主成分分析算法" class="headerlink" title="4.主成分分析算法"></a>4.主成分分析算法</h2><p>PCA 减少 $n$ 维到 $k$ 维 :</p><ul><li><p>第一步是均值归一化。我们需要计算出所有特征的均值，然后令 $x_{j}=x_{j}-\mu_{j}$。 如果特征是在不同的数量级上，我们还需要将其除以标准差 $\sigma^{2}$ 。</p></li><li><p>第二步是计算协方差矩阵（covariance matrix ）</p><script type="math/tex; mode=display">\Sigma=\frac{1}{m} \sum_{i=1}^{n}\left(x^{(i)}\right)\left(x^{(i)}\right)^{T}</script><p>补充知识：</p><p>协方差为：</p><script type="math/tex; mode=display">\sigma\left(x_{m}, x_{k}\right)=\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{m i}-\bar{x}_{m}\right)\left(x_{k i}-\bar{x}_{k}\right)</script><p>因此，协方差矩阵为：</p><script type="math/tex; mode=display">\Sigma=\left[\begin{array}{ccc}\sigma\left(x_{1}, x_{1}\right) & \cdots & \sigma\left(x_{1}, x_{d}\right) \\ \vdots & \ddots & \vdots \\ \sigma\left(x_{d}, x_{1}\right) & \cdots & \sigma\left(x_{d}, x_{d}\right)\end{array}\right] \in \mathbb{R}^{d \times d}</script></li><li><p>第三步是计算协方差矩阵 $\Sigma$ 的特征向量（eigenvectors）:</p></li><li><p>利用奇异值分解（singularvaluedecomposition）来求解$[U,S, V]= svd(sigma)$:</p></li></ul><p>对于一个 $n \times n$ 维度的矩阵，上式中的 $U$ 是一个具有与数据之间最小投射误差的方向向量构成的矩阵。如果我们希望将数据从n维降至k维，我们只需要从U中选取前k个向量，获得一个 $n \times k$ 维度的矩阵, 我们用 $U_{r e d u c e}$ 表示，然后通过如下计算获得要求的新特征向量 $z^{(i)}:z^{(i)}=U_{r e d u c e}^{T} * x^{(i)}$</p><p>其中$𝑥$是$𝑛 × 1$维的，因此结果为$𝑘 × 1$维度。注，我们不对方差特征进行处理。</p><p>PCA 算法，我们可能有一个这样的样本。如图中样本 $x^{(1)}, x^{(2)}$ 。我们做的是，我们把这些样本投射到图中这个一维平面。然后现在我们需要只使用一个实数，比如z $^{(1)}$ ，指定这些点的位置后他们被投射到这一个三维曲面。给定一个点 $z^{(1)},$ 我们怎么能回去这个原始的二维空间呢？x为 2 维，z 为 1 维， $z=U_{r e d u c e}^{T} x,$ 相反的方程为:</p><script type="math/tex; mode=display">x_{\text {appox}}=U_{\text {reduce}} \cdot z, x_{\text {appox}} \approx x_{\circ}</script><h2 id="5-选择主成分的数量"><a href="#5-选择主成分的数量" class="headerlink" title="5.选择主成分的数量"></a>5.选择主成分的数量</h2><p>主要成分分析是减少投射的平均均方误差:</p><p>训练集的方差为: $\frac{1}{m} \sum_{i=1}^{m}\left|x^{(i)}\right|^{2}$</p><p>我们希望在平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的k值。如果我们希望这个比例小于$1\%$，就意味着原本数据的偏差有 $99\%$都保留下来了，如果我们选择保留$ 95\%$的偏差，便能非常显著地降低模型中特征的维度了。</p><p>我们可以先令 $k=1$, 然后进行主要成分分析，获得 $U_{r e d u c e}$ 和 $z$, 然后计算比例是否小于$1\%$。如果不是的话再令$k = 2$，如此类推，直到找到可以使得比例小于 $1\%$的最小 $k$ 值 （原因是各个特征之间通常情况存在某种相关性）。</p><p>对于之前的SVD函数，其中返回的$𝑆$是一个$𝑛 × 𝑛$的矩阵，只有对角线上有值，而其它单元都是 0，我们可以使用这个矩阵来计算平均均方误差与训练集方差的比例:</p><script type="math/tex; mode=display">\frac{\frac{1}{m} \sum_{i=1}^{m}\left\|x^{(i)}-x_{a p p r o x}^{(i)}\right\|^{2}}{\frac{1}{m} \sum_{i=1}^{m}\left\|x^{(i)}\right\|^{2}}=1-\frac{\sum_{i=1}^{k} s_{i i}}{\sum_{i=1}^{n} s_{i i}} \leq 1 \%</script><p>也就是:</p><script type="math/tex; mode=display">\frac{\sum_{i=1}^{k} s_{i i}}{\sum_{i=1}^{n} s_{i i}} \geq 0.99</script><p>在压缩过数据后，我们可以采用如下方法来近似地获得原有的特征:</p><script type="math/tex; mode=display">x_{a p p r o x}^{(i)}=U_{r e d u c e} z^{(i)}</script><h2 id="6-主成分分析法的应用建议"><a href="#6-主成分分析法的应用建议" class="headerlink" title="6.主成分分析法的应用建议"></a>6.主成分分析法的应用建议</h2><p>假使我们正在针对一张 100×100 像素的图片进行某个计算机视觉的机器学习，即总共 有 10000 个特征。</p><ol><li>第一步是运用主要成分分析将数据压缩至 1000 个特征</li><li>然后对训练集运行学习算法。</li><li>在预测时，采用之前学习而来的$𝑈_{𝑟𝑒𝑑𝑢𝑐𝑒}$将输入的特征𝑥转换成特征向量𝑧，然后再进行预测</li></ol><p>注:如果我们有交叉验证集合测试集，也采用对训练集学习而来的$𝑈_{𝑟𝑒𝑑𝑢𝑐𝑒}$ 。 错误的主要成分分析情况:一个常见错误使用主要成分分析的情况是，将其用于减少过</p><p>拟合（减少了特征的数量）。这样做非常不好，不如尝试正则化处理。原因在于主要成分分析只是近似地丢弃掉一些特征，它并不考虑任何与结果变量有关的信息，因此可能会丢失非 常重要的特征。然而当我们进行正则化处理时，会考虑到结果变量，不会丢掉重要的数据。</p><p>另一个常见的错误是，默认地将主要成分分析作为学习过程中的一部分，这虽然很多时候有效果，最好还是从所有原始特征开始，只在有必要的时候（算法运行太慢或者占用太多内存）才考虑采用主要成分分析。</p>]]></content>
      
      
      <categories>
          
          <category> 吴恩达——机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 降维 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>07_聚类(Clustering)</title>
      <link href="2020/12/09/machinelearning/wu-en-da-ji-qi-xue-xi/07-clustering/"/>
      <url>2020/12/09/machinelearning/wu-en-da-ji-qi-xue-xi/07-clustering/</url>
      
        <content type="html"><![CDATA[<h1 id="聚类-Clustering"><a href="#聚类-Clustering" class="headerlink" title="聚类_Clustering"></a>聚类_Clustering</h1><h2 id="1-K-均值算法（K-Means）"><a href="#1-K-均值算法（K-Means）" class="headerlink" title="1.K-均值算法（K-Means）"></a>1.K-均值算法（K-Means）</h2><p>K-均值是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据聚类成不同的 组。</p><p>K-均值是一个迭代算法，假设我们想要将数据聚类成 n 个组，其方法为:</p><ol><li>首先选择𝐾个随机的点，称为聚类中心（cluster centroids）</li><li>对于数据集中的每一个数据，按照距离𝐾个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。</li><li>计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置。</li><li>重复上述步骤2-3直至中心点不再变化</li></ol><p>用 $\mu^{1}, \mu^{2}, \ldots, \mu^{k}$ 来表示聚类中心, 用 $c^{(1)}, c^{(2)}, \ldots, c^{(m)}$ 来存储与第i个实例数据最近的聚类中心的索引，K-均值算法的代码如下:</p><h2 id="2-优化目标"><a href="#2-优化目标" class="headerlink" title="2.优化目标"></a>2.优化目标</h2><p>K-均值最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和， 因此 K-均值的代价函数（又称畸变函数 Distortion function）为:</p><script type="math/tex; mode=display">J\left(c^{(1)}, \ldots, c^{(m)}, \mu_{1}, \ldots, \mu_{K}\right)=\frac{1}{m} \sum_{i=1}^{m}\left\|X^{(i)}-\mu_{c^{(i)}}\right\|^{2}</script><p>其中 $\mu_{c^{(i)}}$ 代表与 $x^{(i)}$ 最近的聚类中心点。 我们的的优化目标便是找出使得代价函数最小的 $c^{(1)}, c^{(2)}, \ldots, c^{(m)}$ 和 $\mu^{1}, \mu^{2}, \ldots, \mu^{k}$ :</p><script type="math/tex; mode=display">\min _{c^{(1)}, \ldots, c^{(m)}, \atop \mu_{1}, \ldots, \mu_{K}} J\left(c^{(1)}, \ldots, c^{(m)}, \mu_{1}, \ldots, \mu_{K}\right)</script><p>回顾刚才给出的: K-均值迭代算法，我们知道，第一个循环是用于减小$𝑐^{(𝑖)}$引起的代价， 而第二个循环则是用于减小$\mu^{i}$引起的代价。迭代的过程一定会是每一次迭代都在减小代价函数，不然便是出现了错误。</p><h2 id="3-随机初始化"><a href="#3-随机初始化" class="headerlink" title="3.随机初始化"></a>3.随机初始化</h2><p>在运行 K-均值算法的之前，我们首先要随机初始化所有的聚类中心点，下面介绍怎样做:</p><ol><li>我们应该选择$𝐾 &lt; 𝑚$，即聚类中心点的个数要小于所有训练集实例的数量。</li><li>随机选择$𝐾$个训练实例，然后令$𝐾$个聚类中心分别与这$𝐾$个训练实例相等。</li></ol><p>K-均值的一个问题在于，它有可能会停留在一个局部最小值处，而这取决于初始化的情况。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201209110404.png" alt="image-20201209110404386"></p><p>为了解决这个问题，我们通常需要多次运行 K-均值算法，每一次都重新进行随机初始化，最后再比较多次运行K-均值的结果，选择代价函数最小的结果。这种方法在𝐾较小的时候还是可行的，但是如果𝐾较大，这么做也可能不会有明显地改善。</p><h2 id="4-选择聚类数"><a href="#4-选择聚类数" class="headerlink" title="4.选择聚类数"></a>4.选择聚类数</h2><p>没有所谓最好的选择聚类数的方法，通常是需要根据不同的问题，人工进行选择的。选择的时候思考我们运用 K-均值算法聚类的动机是什么，然后选择能最好服务于该目的标聚类数。</p><p>当人们在讨论，选择聚类数目的方法时，有一个可能会谈及的方法叫作“肘部法则”。关 于“肘部法则”，我们所需要做的是改变𝐾值，也就是聚类类别数目的总数。我们用一个聚类 来运行 K 均值聚类方法。这就意味着，所有的数据都会分到一个聚类里，然后计算成本函数或者计算畸变函数𝐽。𝐾代表聚类数字。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201209110610.png" alt="image-20201209110610896"></p><p>我们可能会得到一条类似于这样的曲线。像一个人的肘部。这就是“肘部法则”所做的， 让我们来看这样一个图，看起来就好像有一个很清楚的肘在那儿。好像人的手臂，如果你伸 出你的胳膊，那么这就是你的肩关节、肘关节、手。这就是“肘部法则”。你会发现这种模式， 它的畸变值会迅速下降，从 1 到 2，从 2 到 3 之后，你会在 3 的时候达到一个肘点。在此之 后，畸变值就下降的非常慢，看起来就像使用 3 个聚类来进行聚类是正确的，这是因为那个 点是曲线的肘点，畸变值下降得很快，𝐾 = 3之后就下降得很慢，那么我们就选𝐾 = 3。当你 应用“肘部法则”的时候，如果你得到了一个像上面这样的图，那么这将是一种用来选择聚类 个数的合理方法。</p><p>聚类参考资料:</p><ol><li><p>相似度/距离计算方法总结</p><ul><li>闵可夫斯基距离 Minkowski/（其中欧式距离： $p=2$ )<script type="math/tex; mode=display">\operatorname{dist}(X, Y)=\left(\sum_{i=1}^{n}\left|x_{i}-y_{i}\right|\right)^{\frac{1}{p}}</script></li><li>杰卡德相似系数(Jaccard):<script type="math/tex; mode=display">       J(A, B)=\frac{|A \cap B|}{|A \cup B|}</script></li><li><p>余弦相似度(cosine similarity):$n$ 维向量 $x$ 和 $y$ 的夹角记做 $\theta$, 根据余弦定理，其余弦值为:</p><script type="math/tex; mode=display">\cos (\theta)=\frac{x^{T} y}{|x| \cdot|y|}=\frac{\sum_{i=1}^{n} x_{i} y_{i}}{\sqrt{\sum_{i=1}^{n} x_{i}^{2}} \sqrt{\sum_{i=1}^{n} y_{i}^{2}}}</script></li><li><p>Pearson 皮尔逊相关系数:Pearson 相关系数即将 $x 、 y$ 坐标向量各自平移到原点后的夹角余弦。</p><script type="math/tex; mode=display">\rho_{X Y}=\frac{\operatorname{cov}(X, Y)}{\sigma_{X} \sigma_{Y}}=\frac{E\left[\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right]}{\sigma_{X} \sigma_{Y}}=\frac{\sum_{i=1}^{n}\left(x-\mu_{X}\right)\left(y-\mu_{Y}\right)}{\sqrt{\sum_{i=1}^{n}\left(x-\mu_{X}\right)^{2}} \sqrt{\sum_{i=1}^{n}\left(y-\mu_{Y}\right)^{2}}}</script></li></ul></li><li><p>聚类的衡量指标</p><ul><li><p>均一性:𝑝</p><p>类似于精确率，一个簇中只包含一个类别的样本，则满足均一性。其实也可以认为就是正确率（每个聚簇中正确分类的样本数占该聚簇总样本数的比例和）</p></li><li><p>完整性:𝑟</p><p>类似于召回率，同类别样本被归类到相同簇中，则满足完整性;(每个聚簇中正确分类的 样本数占该类型的总样本数比例的和)</p></li><li><p>V-measure:</p><p>均一性和完整性的加权平均：</p><script type="math/tex; mode=display">V=\frac{\left(1+\beta^{2}\right) * p r}{\beta^{2} * p+r}</script></li><li><p>轮廓系数</p><p>样本i的轮廓系数: $s(i)$<br>族内不相似度:计算样本i到同族其它样本的平均距离为 $a(i),$ 应尽可能小。<br>族间不相似度:计算样本 $i$ 到其它族 $C_{j}$ 的所有样本的平均距离 $b_{i j},$ 应尽可能大。<br>轮廓系数 $s(i)$ 值越接近 1 表示样本 $i$ 聚类越合理，越接近-1，表示样本 $i$ 应该分类到另外的簇中，近似为 0，表示样本$𝑖$应该在边界上，所有样本的$𝑠(𝑖)$的均值被成为聚类结果的轮廓系数。</p><script type="math/tex; mode=display">s(i)=\frac{b(i)-a(i)}{\max \{a(i), b(i)\}}</script></li></ul></li></ol><ul><li><p>ARI</p><p>数据集S共有N个元素， 两个聚类结果分别是:</p><script type="math/tex; mode=display">X=\left\{X_{1}, X_{2}, \ldots, X_{r}\right\}, Y=\left\{Y_{1}, Y_{2}, \ldots, Y_{S}\right\}</script><p>  $X$ 和 $Y$ 的元素个数为:</p><script type="math/tex; mode=display">  a=\left\{a_{1}, a_{2}, \ldots, a_{r}\right\}, b=\left\{b_{1}, b_{2}, \ldots, b_{s}\right\}</script><p>  记: $n_{i j}=\left|X_{i} \cap Y_{i}\right|$</p><script type="math/tex; mode=display">  A R I=\frac{\sum_{i, j} C_{n_{i j}}^{2}-\left[\left(\sum_{i} C_{a_{i}}^{2}\right) \cdot\left(\sum_{i} C_{b_{i}}^{2}\right)\right] / C_{n}^{2}}{\frac{1}{2}\left[\left(\sum_{i} C_{a_{i}}^{2}\right)+\left(\Sigma_{i} C_{b_{i}}^{2}\right)\right]-\left[\left(\sum_{i} C_{a_{i}}^{2}\right) \cdot\left(\sum_{i} C_{b_{i}}^{2}\right)\right] / C_{n}^{2}}</script></li></ul>]]></content>
      
      
      <categories>
          
          <category> 吴恩达——机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 聚类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>06_支持向量机(SupportVectorMachines)</title>
      <link href="2020/12/08/machinelearning/wu-en-da-ji-qi-xue-xi/06-support-vector-machines/"/>
      <url>2020/12/08/machinelearning/wu-en-da-ji-qi-xue-xi/06-support-vector-machines/</url>
      
        <content type="html"><![CDATA[<h1 id="支持向量机-Support-Vector-Machines"><a href="#支持向量机-Support-Vector-Machines" class="headerlink" title="支持向量机_Support_Vector_Machines"></a>支持向量机_Support_Vector_Machines</h1><h2 id="1-优化目标（optimization-objective）"><a href="#1-优化目标（optimization-objective）" class="headerlink" title="1.优化目标（optimization objective）"></a>1.优化目标（optimization objective）</h2><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201208155835.png" alt="image-20201208155835819"></p><p>那么，在逻辑回归中我们已经熟悉了这里的假设函数形式，和右边的 S 型激励函数。然而，为了解释一些数学知识。将用z 表示 $\theta^{T} x_{\circ}$<br>现在考虑下我们想要逻辑回归做什么：如果有一个 $y=1$ 的样本，我的意思是不管是在训练集中或是在测试集中，又或者在交义验证集中，总之是 $y=1,$ 现在我们希望 $h_{\theta}(x)$ 趋近1。因为我们想要正确地将此样本分类，这就意味着当 $h_{\theta}(x)$ 趋近于 1 时, $\theta^{T} x$ 应当远大于 0，这里的&gt;&gt;意思是远远大于 0 。这是因为由于 $z$ 表示 $\theta^{T} x,$ 当 Z远大于 0 时，即到了该图的右边, 你不难发现此时逻辑回归的输出将趋近于 1 。相反地, 如果我们有另一个样本,即 $y=0$ 。我们希望假设函数的输出值将趋近于 0，这对应于 $\theta^{T} x,$ 或者就是 $z$ 会远小于 $0,$<br>因为对应的假设函数的输出值趋近 0 。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201208172623.png" alt="image-20201208155942584"></p><p>如果你进一步观察逻辑回归的代价函数，你会发现每个样本 (𝑥,𝑦)都会为总代价函数， 增加这里的一项，</p><script type="math/tex; mode=display">-\left(y \log h_{\theta}(x)+(1-y) \log \left(1-h_{\theta}(x)\right)\right)</script><p>因此，对于总代价函数通常会有对所有的训练样本求和，并且这里还有一 个1/𝑚项，但是，在逻辑回归中，这里的这一项就是表示一个训练样本所对应的表达式。现 在，如果我将完整定义的假设函数代入这里。那么，我们就会得到每一个训练样本都影响这一项。</p><p>现在，一起来考虑两种情况:</p><p>一种是𝑦等于 1 的情况;另一种是 𝑦 等于 0 的情况。</p><p>在第一种情况中，假设 $𝑦 = 1$ ，此时在目标函数中只需有第一项起作用，因为$𝑦 = 1$时，$(1−𝑦)$项将等于 0。因此，当在$ 𝑦=1 $的样本中时，即在 $(𝑥,𝑦)$中 ，我们得到$y=1-\log \left(1-\frac{1}{1+e^{-z}}\right)$这样一项</p><p>我们用 $z$ 表示 $\theta^{T} x,$ 即 $: \quad z=\theta^{T} x_{\circ}$ 当然，在代价函数中， $y$ 前面有负号。我们只是这样表示，如果 $y=1$ 代价函数中，这一项也等于 1 。这样做是为了简化此处的表达式。如果画出关于 $z$ 的函数，你会看到左下角的这条曲线，我们同样可以看到，当z 增大时，也就是相<br>当于 $\theta^{T} x$ 增大时， $z$ 对应的值会变的非常小。对整个代价函数而言，影响也非常小。这也就解释了，为什么逻辑回归在观察到正样本 $y=1$ 时，试图将 $\theta^{T} x$ 设置得非常大。因为，在代价函数中的这一项会变的非常小。现在开始建立支持向量机，我们从这里开始:我们会从这个代价函数开始, 也就是$- \log \left(1-\frac{1}{1+e^{-z}}\right)$ 一点一点修改，让我取这里的$z=1$点，我先画出将要用的代价函数。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201208172713.png" alt="image-20201208162008412"></p><p>新的代价函数将会水平的从这里到右边(图外)，然后我再画一条同逻辑回归非常相似的 直线，但是，在这里是一条直线，也就是我用紫红色画的曲线，就是这条紫红色的曲线。那么，到了这里已经非常接近逻辑回归中使用的代价函数了。只是这里是由两条线段组成，即位于右边的水平部分和位于左边的直线部分，先别过多的考虑左边直线部分的斜率，这并不是很重要。但是，这里我们将使用的新的代价函数，是在$𝑦 = 1$的前提下的。你也许能想到， 这应该能做同逻辑回归中类似的事情，但事实上，在之后的优化问题中，这会变得更坚定， 并且为支持向量机，带来计算上的优势。例如，更容易计算股票交易的问题等等。</p><p>目前，我们只是讨论了𝑦 = 1的情况，另外一种情况是当𝑦 = 0时，此时如果你仔细观察 代价函数只留下了第二项，因为第一项被消除了。如果当𝑦 = 0时，那么这一项也就是 0 了。 所以上述表达式只留下了第二项。因此，这个样本的代价或是代价函数的贡献。将会由这一 项表示。并且，如果你将这一项作为𝑧的函数，那么，这里就会得到横轴𝑧。现在，你完成了 支持向量机中的部分内容，同样地，我们要替代这一条蓝色的线，用相似的方法。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201208172716.png" alt="image-20201208162104035"></p><p>如果我们用一个新的代价函数来代替，即这条从 0 点开始的水平直线，然后是一条斜线, 像上图。那么，现在让我给这两个方程命名，左边的函数，我称之为 $cost_{1}(z),$ 同时,右边函数我称它为 $cost_{0}(z)$。 这里的下标是指在代价函数中，对应的 $y=1$ 和 $y=0$ 的情<br>况，拥有了这些定义后，现在，我们就开始构建支持向量机。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201208172748.png" alt="image-20201208162206375"></p><p>这是我们在逻辑回归中使用代价函数 $J(\theta)$ 。也许这个方程看起来不是非常熟悉。这是因为之前有个负号在方程外面，但是，这里我所做的是，将负号移到了表达式的里面，这样做使得方程看起来有些不同。对于支持向量机而言，实质上我们要将这替换为 $cost_{1}(z),$ 也就<br>是 $cost_{1}\left(\theta^{T} x\right),$ 同样地，我也将这一项替换为 $cost_{0}(z),$ 也就是代价 $\cos t_{0}\left(\theta^{T} x\right)$ 。这里的代价函数 $cost_{1}$, 就是之前所提到的那条线。此外，代价函数 $cost_{0}$, 也是上面所介绍过的那条线。因此，对于支持向量机，我们得到了这里的最小化问题，即:</p><script type="math/tex; mode=display">\min _{\theta} C \sum_{i=1}^{m}\left[y^{(i)} \cos t_{1}\left(\theta^{T} x^{(i)}\right)+\left(1-y^{(i)}\right) \operatorname{cost}_{0}\left(\theta^{T} x^{(i)}\right)\right]+\frac{1}{2} \sum_{i=1}^{n} \theta_{j}^{2}</script><p>然后，再加上正则化参数。现在，按照支持向量机的惯例，事实上，我们的书写会稍微 有些不同，代价函数的参数表示也会稍微有些不同。</p><p>最后有别于逻辑回归输出的概率。在这里，我们的代价函数，当最小化代价函数，获得参数 $\theta$ 时，支持向量机所做的是它来直接预测 $y$ 的值等于 1，还是等于 0 。因此，这个假设函数会预测 1。当 $\theta^{T} x$ 大于或者等于 0 时，或者等于 0 时，所以学习参数 $\theta$ 就是支持向量机假设函数的形式。那么，这就是支持向量机数学上的定义。</p><h2 id="2-大边界的直观理解"><a href="#2-大边界的直观理解" class="headerlink" title="2.大边界的直观理解"></a>2.大边界的直观理解</h2><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201208165502.png" alt="image-20201208165502304"></p><p>这是我的支持向量机模型的代价函数，在左边这里我画出了关于z的代价函数 $cost_{1}(z)$,此函数用于正样本，而在右边这里我画出了关于z的代价函数 $cost_{0}(z),$ 横轴表示 $z$ ，现在让我们考虑一下，最小化这些代价函数的必要条件是什么。如果你有一个正样本， $y=1$, 则只有在 $z&gt;=1$ 时，代价函数 $\cos t_{1}(z)$ 才等于 0 。换句话说，如果你有一个正样本，我们会希望 $\theta^{T} x&gt;=1,$ 反之，如果 $y=0,$ 我们观察一下，函数cost $_{0}(z),$ 它只有在 $z&lt;=-1$ 的区间里函数值为 0 。</p><p>这是支持向量机的一个有趣性质。事实上，如果你有一个正样本 $y=1$ ，则其实我们仅仅要求 $\theta^{T} x$ 大于等于 0，就能将该样本恰当分出，这是因为如果 $\theta^{T} x&gt;0$ 大的话，我们的模型代价函数值为 0，类似地，如果你有一个负样本，则仅需要 $\theta^{T} x&lt;=0$ 就会将负例正确分离，但是，支持向量机的要求更高，不仅仅要能正确分开输入的样本，即不仅仅要求 $\theta^{T} x&gt;0,$ 我们需要的是比 0 值大很多，比如大于<br>等于 1，我也想这个比 0 小很多，比如我希望它小于等于-1，这就相当于在支持向量机中嵌入了一个额外的安全因子，或者说安全的间距因子。</p><p>当然，逻辑回归做了类似的事情。但是让我们看一下，在支持向量机中，这个因子会导 致什么结果。具体而言，我接下来会考虑一个特例。我们将这个常数𝐶设置成一个非常大的 值。比如我们假设𝐶的值为 100000 或者其它非常大的数，然后来观察支持向量机会给出什 么结果?</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201208165637.png" alt="image-20201208165636975"></p><p>如果 $C$ 非常大，则最小化代价函数的时候，我们将会很希望找到一个使第一项为 0 的最优解。因此，让我们尝试在代价项的第一项为 0 的情形下理解该优化问题。比如我们可以把C设置成了非常大的常数，这将给我们一些关于支持向量机模型的直观感受。</p><script type="math/tex; mode=display">\min _{\theta} C \sum_{i=1}^{m}\left[y^{(i)} \cos t_{1}\left(\theta^{T} x^{(i)}\right)+\left(1-y^{(i)}\right) \cos t\left(\theta^{T} x^{(i)}\right)\right]+\frac{1}{2} \sum_{i=1}^{n} \theta_{j}^{2}</script><p>我们已经看到输入一个训练样本标签为 $y=1,$ 你想令第一项为 $0,$ 你需要做的是找到一个 $\theta,$ 使得 $\theta^{T} x&gt;=1,$ 类似地，对于一个训练样本，标签为 $y=0,$ 为了使 $cost_{0}(z)$ 函数的值为 0，我们需要 $\theta^{T} x&lt;=-1$ 。因此，现在考虑我们的优化问题。选择参数，使得第一项等<br>于 0，就会导致下面的优化问题，因为我们将选择参数使第一项为 0，因此这个函数的第一项为 0，因此是C乘以 0 加上二分之一乘以第二项。这里第一项是C乘以 0，因此可以将其删去，因为我知道它是 0。<br>这将遵从以下的约束: $\theta^{T} x^{(i)}&gt;=1,$ 如果 $y^{(i)}$ 是等于 1 的, $\theta^{T} x^{(i)}&lt;=-1,$ 如果样本 $i$是一个负样本，这样当你求解这个优化问题的时候，当你最小化这个关于变量 $\theta$ 的函数的时候，你会得到一个非常有趣的决策边界。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201208171625.png" alt="image-20201208171625321"></p><p>具体而言，如果你考察这样一个数据集，其中有正样本，也有负样本，可以看到这个数据集是线性可分的。存在一条直线把正负样本分开。当然有多条不同的直线， 可以把正样本和负样本完全分开。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201208172849.png" alt="image-20201208171752369"></p><p>比如，这就是一个决策边界可以把正样本和负样本分开。但是多多少少这个看起来并不 是非常自然是么?</p><p>或者我们可以画一条更差的决策界，这是另一条决策边界，可以将正样本和负样本分开， 但仅仅是勉强分开，这些决策边界看起来都不是特别好的选择，支持向量机将会选择这个黑色的决策边界，相较于之前我用粉色或者绿色画的决策界。这条黑色的看起来好得多，黑线 看起来是更稳健的决策界。在分离正样本和负样本上它显得的更好。数学上来讲，这是什么意思呢?这条黑线有更大的距离，这个距离叫做间距（margin）。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201208171859.png" alt="image-20201208171859274"></p><p>当画出这两条额外的蓝线，我们看到黑色的决策界和训练样本之间有更大的最短距离。 然而粉线和蓝线离训练样本就非常近，在分离样本的时候就会比黑线表现差。因此，这个距离叫做支持向量机的间距，而这是支持向量机具有鲁棒性的原因，因为它努力用一个最大间 距来分离样本。因此支持向量机有时被称为大间距分类器。</p><p>我们将这个大间距分类器中的正则化 因子常数𝐶设置的非常大，我记得我将其设置为了 100000，因此对这样的一个数据集，也许 我们将选择这样的决策界，从而最大间距地分离开正样本和负样本。那么在让代价函数最小 化的过程中，我们希望找出在𝑦 = 1和𝑦 = 0两种情况下都使得代价函数中左边的这一项尽量 为零的参数。如果我们找到了这样的参数，则我们的最小化问题便转变成:</p><script type="math/tex; mode=display">\min \frac{1}{2} \sum_{j=1}^{n} \theta_{j}^{2} \text { s.t }\left\{\begin{array}{c}\theta^{T} x^{(i)} \geq 1 \text { if } y^{(i)}=1 \\\theta^{T} x^{(i)} \leq-1 \text { if } y^{(i)}=0\end{array}\right.</script><p>事实上，支持向量机现在要比这个大间距分类器所体现得更成熟，尤其是当你使用大间 距分类器的时候，你的学习算法会受异常点（outlier） 的影响。比如我们加入一个额外的正样本。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201208172038.png" alt="image-20201208172018848"></p><p>在这里，如果你加了这个样本，为了将样本用最大间距分开，也许我最终会得到一条类似这样的决策界，就是这条粉色的线，仅仅基于一个异常值，仅仅基于一个样本，就将我的决策界从这条黑线变到这条粉线，这实在是不明智的。而如果正则化参数𝐶，设置的非常大，这事实上正是支持向量机将会做的。它将决策界，从黑线变到了粉线，但是如果𝐶设置的小一点，如果你将 C 设置的不要太大，则你最终会得到这条黑线，当然数据如果不是 线性可分的，如果你在这里有一些正样本或者你在这里有一些负样本，则支持向量机也会将 它们恰当分开。因此，大间距分类器的描述，仅仅是从直观上给出了正则化参数𝐶非常大的 情形，同时，要提醒你𝐶的作用类似于$1/𝜆$，$𝜆$是我们之前使用过的正则化参数。这只是𝐶非 常大的情形，或者等价地 $𝜆$ 非常小的情形。你最终会得到类似粉线这样的决策界，但是实 际上应用支持向量机的时候，当𝐶不是非常非常大的时候，它可以忽略掉一些异常点的影响， 得到更好的决策界。甚至当你的数据不是线性可分的时候，支持向量机也可以给出好的结果。</p><h2 id="3-核函数"><a href="#3-核函数" class="headerlink" title="3.核函数"></a>3.核函数</h2><p>回顾我们之前讨论过可以使用高级数的多项式模型来解决无法用直线进行分隔的分类问题:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201209090403.png" alt="image-20201209090403405"></p><p>为了获得上图所示的判定边界，我们的模型可能是 $\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}+\theta_{3} x_{1} x_{2}+\theta_{4} x_{1}^{2}+\theta_{5} x_{2}^{2}+\cdots$ 的形式。我们可以用一系列的新的特征 $\mathrm{f}$ 来替换模型中的每一项。例如令: $f_{1}=x_{1}, f_{2}=x_{2}, f_{3}=x_{1} x_{2}, f_{4}=x_{1}^{2}, f_{5}=x_{2}^{2}\ldots$得到 $h_{\theta}(x)=\theta_{1} f_{1}+\theta_{2} f_{2}+\ldots+\theta_{n} f_{n}$ 。然而，除了对原有的特征进行组合以外，有没有<br>更好的方法来构造 $f_{1}, f_{2}, f_{3}$ ？</p><p>我们可以利用核函数来计算出新的特征。</p><p>给定一个训练 实例 $x$, 我们利 用 $x$ 的各个特征与我们预先选定的地标（landmarks） $l^{(1)}, l^{(2)}, l^{(3)}$ 的近似程度来选取新的特征 $f_{1}, f_{2}, f_{3} $。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201209090721.png" alt="image-20201209090721107"></p><p>例如：</p><script type="math/tex; mode=display">f_{1}=\operatorname{similarity}\left(x, l^{(1)}\right)=exp\left(-\frac{\left\|x-l^{(1)}\right\|^{2}}{2 \sigma^{2}}\right)</script><p>其中: </p><script type="math/tex; mode=display">\left\|x-l^{(1)}\right\|^{2}=\sum_{j=1}^{n}\left(x_{j}-l_{j}^{(1)}\right)^{2}</script><p>为实例 $x$ 中所有特征与地标 $l^{(1)}$ 之间的距离的和。上例中的 $similarity\left(x, l^{(1)}\right)$ 就是核函数, 具体而言, 这里是一个高斯核函数（Gaussian Kernel）。</p><p>这些地标的作用是什么?如果一个训练实例$𝑥$与地标$𝐿$之间的距离近似于 0，则新特征 $𝑓$近似于$𝑒^{−0} = 1$，如果训练实例$𝑥$与地标$𝐿$之间距离较远，则$𝑓$近似于$𝑒^{−(一个较大的数)} = 0$</p><p>假设我们的训练实例含有两个特征 $\left[x_{1} x_{2}\right],$ 给定地标 $l^{(1)}$ 与不同的 $\sigma$ 值，见下图:</p><script type="math/tex; mode=display">l^{(1)}=\left[\begin{array}{l}3 \\5\end{array}\right], \quad f_{1}=\exp \left(-\frac{\left\|x-l^{(1)}\right\|^{2}}{2 \sigma^{2}}\right)</script><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201209091305.png" alt="image-20201209091305215"></p><p>图中水平面的坐标为 $x_{1}, x_{2}$ 而垂直坐标轴代表 $f 。$ 可以看出，只有当 $x$ 与 $l^{(1)}$ 重合时 $f$ 才具有最大值。随着x的改变 $f$ 值改变的速率受到 $\sigma^{2}$ 的控制。</p><p>在下图中，当实例处于洋红色的点位置处，因为其离 $l^{(1)}$ 更近，但是离 $l^{(2)}$ 和 $l^{(3)}$ 较远，因此 $f_{1}$ 接近 1，而 $f_{2}, f_{3}$ 接近 0 。因此 $h_{\theta}(x)=\theta_{0}+\theta_{1} f_{1}+\theta_{2} f_{2}+\theta_{1} f_{3}&gt;0,$ 因此预测 $y=1$ 。同理可以求出，对于离 $l^{(2)}$ 较近的绿色点，也预测 $y=1$, 但是对于蓝绿色的点，因为其离三个地标都较远，预测 $y=0$ 。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201209092707.png" alt="image-20201209092707947"></p><p>这样，图中红色的封闭曲线所表示的范围，便是我们依据一个单一的训练实例和我们选取的地标所得出的判定边界，在预测时，我们采用的特征不是训练实例本身的特征，而是通过核函数计算出的新特征$f_1,f_2,f_3$。</p><h2 id="4-核函数-2"><a href="#4-核函数-2" class="headerlink" title="4.核函数 2"></a>4.核函数 2</h2><p>如何选择地标?</p><p>我们通常是根据训练集的数量选择地标的数量，即如果训练集中有m个实例，则我们选取 $m$ 个地标，并且令: $l^{(1)}=x^{(1)}, l^{(2)}=x^{(2)}, \ldots ., l^{(m)}=x^{(m)} 。$ 这样做的好处在于：现在我们得到的新特征是建立在原有特征与训练集中所有其他特征之间距离的基础之上的，即:</p><script type="math/tex; mode=display">f^{(i)}=\left[\begin{array}{c}f_{0}^{(i)}=1 \\f_{1}^{(i)}=\operatorname{sim}\left(x^{(i)}, l^{(1)}\right) \\f_{2}^{(i)}=\operatorname{sim}\left(x^{(i)}, l^{(2)}\right) \\f_{i}^{(i)}=\operatorname{sim}\left(x^{(i)}, l^{(i)}\right)=e^{0}=1 \\\vdots \\f_{m}^{(i)}=\operatorname{sim}\left(x^{(i)}, l^{(m)}\right)\end{array}\right]</script><p>下面我们将核函数运用到支持向量机中，修改我们的支持向量机假设为:</p><ul><li>给定 $x,$ 计算新特征 $f,$ 当 $\theta^{T} f&gt;=0$ 时，预测 $y=1,$ 否则反之。相应地修改代价函数为: </li></ul><script type="math/tex; mode=display">J(\theta)=\min C \sum_{i=1}^{m}\left[y^{(i)} \operatorname{cost}_{1}\left(\theta^{T} f^{(i)}\right)+\left(1-y^{(i)}\right) \operatorname{cost}_{0}\left(\theta^{T} f^{(i)}\right)\right]+\frac{1}{2} \sum_{j=1}^{n=m} \theta_{j}^{2}</script><p>在具体实施过程中，我们还需要对最后的正则化项进行些微调整，在计算 $\sum_{j=1}^{n=m} \theta_{j}^{2}=\theta^{T} \theta$ 时，我们用 $\theta^{T} M \theta$ 代替$\theta^{T} \theta,$ 其中 $M$ 是根据我们选择的核函数而不同的一个矩阵。这样做的原因是为了简化计算。</p><p>支持向量机也可以不使用核函数，不使用核函数又称为线性核函数（linear kernel）， 当我们不采用非常复杂的函数，或者我们的训练集特征非常多而实例非常少的时候，可以采用这种不带核函数的支持向量机。</p><ul><li>下面是支持向量机的两个参数 $C$ 和 $\sigma$ 的影响:</li><li>$C=1 / \lambda$</li><li>$C$ 较大时，相当于入较小，可能会导致过拟合，高方差;</li><li>$C$ 较小时，相当于入较大，可能会导致低拟合，高偏差;</li><li>$\sigma$ 较大时，可能会导致低方差，高偏差;</li><li>$\sigma$ 较小时，可能会导致低偏差，高方差。</li></ul><h2 id="5-使用支持向量机"><a href="#5-使用支持向量机" class="headerlink" title="5.使用支持向量机"></a>5.使用支持向量机</h2><p>在高斯核函数之外我们还有其他一些选择，如:</p><ul><li>多项式核函数（Polynomial Kernel）</li><li>字符串核函数（String kernel）</li><li>卡方核函数（chi-square kernel）</li><li>直方图交集核函数（histogram intersection kernel）</li><li>等等</li></ul><p>多类分类问题：</p><p>假设我们利用之前介绍的一对多方法来解决一个多类分类问题。如果一共有𝑘个类，则我们需要𝑘个模型，以及𝑘个参数向量𝜃。我们同样也可以训练𝑘个支持向量机来解决多类分类问题。但是大多数支持向量机软件包都有内置的多类分类功能，我们只要直接使用即可。</p><p>尽管你不去写你自己的 SVM 的优化软件，但是你也需要做几件事:</p><ol><li>是提出参数𝐶的选择。我们在之前的视频中讨论过误差/方差在这方面的性质。</li><li>你也需要选择内核参数或你想要使用的相似函数，其中一个选择是:我们选择不需要任何内核参数，没有内核参数的理念，也叫线性核函数。因此，如果有人说他使用了线性核的 SVM(支持向量机)，这就意味这他使用了不带有核函数的 SVM(支持向量机)</li></ol><p>从逻辑回归模型，我们得到了支持向量机模型，在两者之间，我们应该如何选择呢?</p><p>𝑛为特征数，𝑚为训练样本数。</p><ol><li>如果相较于𝑚而言，𝑛要大许多，即训练集数据量不够支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。 </li><li>如果𝑛较小，而且𝑚大小中等，例如𝑛在 1-1000 之间，而𝑚在 10-10000 之间，使用高斯核函数的支持向量机。</li><li>如果𝑛较小，而𝑚较大，例如𝑛在 1-1000 之间，而𝑚大于 50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。 值得一提的是，神经网络在以上三种情况下都可能会有较好的表现，但是训练神经网络可能非常慢，选择支持向量机的原因主要在于它的代价函数是凸函数，不存在局部最小值。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 吴恩达——机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> SVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>05_神经网络的学习_Neural_Networks——Learning</title>
      <link href="2020/12/07/machinelearning/wu-en-da-ji-qi-xue-xi/05-neural-networks-learning/"/>
      <url>2020/12/07/machinelearning/wu-en-da-ji-qi-xue-xi/05-neural-networks-learning/</url>
      
        <content type="html"><![CDATA[<h1 id="神经网络的学习（Neural-Networks-Learning）"><a href="#神经网络的学习（Neural-Networks-Learning）" class="headerlink" title="神经网络的学习（Neural Networks: Learning）"></a>神经网络的学习（Neural Networks: Learning）</h1><h2 id="1-代价函数-Cost-Function"><a href="#1-代价函数-Cost-Function" class="headerlink" title="1.代价函数-Cost Function"></a>1.代价函数-Cost Function</h2><p>假设神经网络的训练样本有m个，每个包含一组输入x和一组输出信号y， $L$ 表示神经网络层数, $S_{I}$ 表示每层的 neuron 个数 （$S_{l}$ 表示输出层神经元个数）， $S_{L}$ 代表最后一层中处理单元的个数。</p><p>将神经网络的分类定义为两种情况：</p><ul><li>二类分类: $S_{L}=0, y=0$ or 1表示哪一类;</li><li>$K$ 类分类: $S_{L}=k, y_{i}=1$ 表示分到第 $\mathrm{i}$ 类; $(k&gt;2)$</li></ul><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201207103355.png" alt="分类神经网络"></p><p>我们回顾逻辑回归问题中我们的代价函数为:</p><script type="math/tex; mode=display">J(\theta)=-\frac{1}{m}\left[\sum_{j=1}^{n} y^{(i)} \log h_{\theta}\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]+\frac{\lambda}{2 m} \sum_{j=1}^{n} \theta_{j}^{2}</script><p>在逻辑回归中，我们只有一个输出变量，又称标量（scalar），也只有一个因变量y，但是在神经网络中，我们可以有很多输出变量, 我们的 $h_{\theta}(x)$ 是一个维度为 $K$ 的向量，并且我们训练集中的因变量也是同样维度的一个向量, 因此我们的代价函数会比逻辑回归更加复杂一些，为: $\quad h_{\theta}(x) \in \mathbb{R}^{K},\left(h_{\theta}(x)\right)_{i}=i^{t h}$ output</p><script type="math/tex; mode=display">\begin{array}{l}J(\Theta)=-\frac{1}{m}\left[\sum_{i=1}^{m} \sum_{k=1}^{k} y_{k}^{(i)} \log \left(h_{\Theta}\left(x^{(i)}\right)\right)_{k}+\left(1-y_{k}^{(i)}\right) \log \left(1-\left(h_{\Theta}\left(x^{(i)}\right)\right)_{k}\right)\right] \\+\frac{\lambda}{2 m} \sum_{l=1}^{L-1} \sum_{i=1}^{s_{l}} \sum_{j=1}^{s_{l}+1}\left(\Theta_{j i}^{(l)}\right)^{2}\end{array}</script><p>这个看起来复杂很多的代价函数背后的思想还是一样的，我们希望通过代价函数来观察 算法预测的结果与真实情况的误差有多大，唯一不同的是，对于每一行特征，我们都会给出 𝐾个预测，基本上我们可以利用循环，对每一行特征都预测𝐾个不同结果，然后在利用循环 在𝐾个预测中选择可能性最高的一个，将其与𝑦中的实际数据进行比较。</p><p>正则化的那一项只是排除了每一层 $\theta_{0}$ 后，每一层的 $\theta$ 矩阵的和。最里层的循环j循环所有的行（由 $s_{l}+1$ 层的激活单元数决定），循环i则循环所有的列，由该层（ $s_{l}$ 层）的激活单元数所决定。即 $: h_{\theta}(x)$ 与真实值之间的距离为每个样本-每个类输出的加和，对参数进行regularization 的 bias 项处理所有参数的平方和。</p><h2 id="2-反向传播算法（Backpropagation-Algorithm）"><a href="#2-反向传播算法（Backpropagation-Algorithm）" class="headerlink" title="2.反向传播算法（Backpropagation Algorithm）"></a>2.反向传播算法（Backpropagation Algorithm）</h2><p>现在，为了计算代价函数的偏导数 $\frac{\partial}{\partial \theta_{i j}^{(l)}} J(\Theta),$ 我们需要采用一种反向传播算法，也就是首先计算最后一层的误差，然后再一层一层反向求出各层的误差，直到倒数第二层。 以一个例子来说明反向传播算法。假设我们的训练集只有一个实例 $\left(x^{(1)}, y^{(1)}\right),$ 我们的神经网络是一个四层的神经网络,其中 $K=4, S_{L}=4, L=4$</p><p>前向传播算法:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201207162444.png" alt="Forward propagation"></p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201207161326.png" alt="Forward propagation"></p><p>我们从最后一层的误差开始计算，误差是激活单元的预测（ $a_{k}^{(4)}$） 与实际值（ $y^{k}$ ）之间的误差，($𝑘 = 1: 𝑘$)。</p><p>我们用 $\delta$ 来表示误差，则 $: \delta^{(4)}=a^{(4)}-y$<br>我们利用这个误差值来计算前一层的误差: </p><script type="math/tex; mode=display">\delta^{(3)}=\left(\Theta^{(3)}\right)^{T} \delta^{(4)} * g^{\prime}\left(z^{(3)}\right)</script><p>其中 </p><script type="math/tex; mode=display">g^{\prime}\left(z^{(3)}\right)</script><p>是 $S$ 形函数的导数， </p><script type="math/tex; mode=display">g^{\prime}\left(z^{(3)}\right)=a^{(3)} *\left(1-a^{(3)}\right)</script><p>而 </p><script type="math/tex; mode=display">\left(\theta^{(3)}\right)^{T} \delta^{(4)}</script><p>则是权重导致的误差的和。下一步是继续计算第二层的误差: </p><script type="math/tex; mode=display">\quad \delta^{(2)}=\left(\Theta^{(2)}\right)^{T} \delta^{(3)} * g^{\prime}\left(z^{(2)}\right)</script><p>因为第一层是输入变量，不存在误差。我们有了所有的误差的表达式后，便可以计算代价函数的偏导数了, 假设 $\lambda=0$, 即我们不做任何正则化处理时有: $\frac{\partial}{\partial \theta_{i j}^{(l)}} J(\Theta)=a_{j}^{(l)} \delta_{i}^{l+1}$重要的是清楚地知道上面式子中上下标的含义:</p><ul><li><p>$𝑙$ 代表目前所计算的是第几层。</p></li><li><p>$𝑗$代表目前计算层中的激活单元的下标，也将是下一层的第$𝑗$个输入变量的下标。</p></li><li><p>$𝑖$代表下一层中误差单元的下标，是受到权重矩阵中第$𝑖$行影响的下一层中的误差单元的下标。</p></li></ul><p>如果我们考虑正则化处理，并且我们的训练集是一个特征矩阵而非向量。在上面的特殊情况中，我们需要计算每一层的误差单元来计算代价函数的偏导数。在更为一般的情况中， 我们同样需要计算每一层的误差单元，但是我们需要为整个训练集计算误差单元，此时的误差单元也是一个矩阵，我们用 $\Delta_{i j}^{(l)}$ 来表示这个误差矩阵。第 $l$ 层的第 $i$ 个激活单元受到第 $j$个参数影响而导致的误差。</p><p>我们的算法表示为:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201207161945.png" alt="算法"></p><p>即首先用正向传播方法计算出每一层的激活单元,利用训练集的结果与神经网络预测的结果求出最后一层的误差，然后利用该误差运用反向传播法计算出直至第二层的所有误差。在求出了 $\Delta_{i j}^{(l)}$ 之后，我们便可以计算代价函数的偏导数了，计算方法如下:</p><script type="math/tex; mode=display">\begin{array}{l}D_{i j}^{(l)}:=\frac{1}{m} \Delta_{i j}^{(l)}+\lambda \theta_{i j}^{(l)} \text { if } j \neq 0 \\D_{i j}^{(l)}:=\frac{1}{m} \Delta_{i j}^{(l)} \text { if } j=0\end{array}</script><h2 id="3-反向传播算法的直观理解"><a href="#3-反向传播算法的直观理解" class="headerlink" title="3.反向传播算法的直观理解"></a>3.反向传播算法的直观理解</h2><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201207164455.png" alt="Forward propagation"></p><p>反向传播算法做的是:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201207164521.png" alt="Back propagation"></p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201207164616.png" alt="Back propagation"></p><h2 id="4-梯度检测"><a href="#4-梯度检测" class="headerlink" title="4.梯度检测"></a>4.梯度检测</h2><p>当我们对一个较为复杂的模型（例如神经网络）使用梯度下降算法时，可能会存在一些 不容易察觉的错误，意味着，虽然代价看上去在不断减小，但最终的结果可能并不是最优解。</p><p>为了避免这样的问题，我们采取一种叫做梯度的数值检验（Numerical Gradient Checking）方法。这种方法的思想是通过估计梯度值来检验我们计算的导数值是否真的是我们要求的。</p><p>对梯度的估计采用的方法是在代价函数上沿着切线的方向选择离两个非常近的点然后计算两个点的平均值用以估计梯度。即对于某个特定的 $\theta,$ 我们计算出在 $\theta-\varepsilon$ 处和 $\theta+\varepsilon$ 的代价值 $(\varepsilon$ 是一个非常小的值，通常选取 0.001$)$ ，然后求两个代价的平均，用以估计在 $\theta$处的代价值。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201208153537.png" alt="image-20201208153537013"></p><p>当 $\theta$ 是一个向量时，我们则需要对偏导数进行检验。因为代价函数的偏导数检验只针对一个参数的改变进行检验，下面是一个只针对 $\theta_{1}$ 进行检验的示例:</p><script type="math/tex; mode=display">\frac{\partial}{\partial \theta_{1}}=\frac{J\left(\theta_{1}+\varepsilon_{1}, \theta_{2}, \theta_{3} \ldots \theta_{n}\right)-J\left(\theta_{1}-\varepsilon_{1}, \theta_{2}, \theta_{3} \ldots \theta_{n}\right)}{2 \varepsilon}</script><p>根据上面的算法，计算出的偏导数存储在矩阵 $D_{i j}^{(l)}$ 中。检验时，我们要将该矩阵展开成为向量，同时我们也将 $\theta$ 矩阵展开为向量，我们针对每一个 $\theta$ 都计算一个近似的梯度值，将这些值存储于一个近似梯度矩阵中，最终将得出的这个矩阵同 $D_{i j}^{(l)}$ 进行比较。</p><h2 id="5-随机初始化"><a href="#5-随机初始化" class="headerlink" title="5.随机初始化"></a>5.随机初始化</h2><p>任何优化算法都需要一些初始的参数。到目前为止我们都是初始所有参数为 0，这样的 初始方法对于逻辑回归来说是可行的，但是对于神经网络来说是不可行的。如果我们令所有 的初始参数都为 0，这将意味着我们第二层的所有激活单元都会有相同的值。同理，如果我 们初始所有的参数都为一个非 0 的数，结果也是一样的。我们通常初始参数为正负𝜀之间的随机值。</p><h2 id="6-总结"><a href="#6-总结" class="headerlink" title="6.总结"></a>6.总结</h2><p> 小结一下使用神经网络时的步骤:</p><p>网络结构:第一件要做的事是选择网络结构，即决定选择多少层以及决定每层分别有多少个单元。第一层的单元数即我们训练集的特征数量。 最后一层的单元数是我们训练集的结果的类的数量。如果隐藏层数大于 1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好。我们真正要决定的是隐藏层的层数和每个中间层的单元数。</p><p>训练神经网络:</p><ul><li>参数的随机初始化</li><li>利用正向传播方法计算所有的h𝜃 (𝑥)</li><li>编写计算代价函数 𝐽 的代码</li><li>利用反向传播方法计算所有偏导数</li><li>利用数值检验方法检验这些偏导数</li><li>使用优化算法来最小化代价函数</li></ul>]]></content>
      
      
      <categories>
          
          <category> 吴恩达——机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>04_神经网络_Neura_ Network</title>
      <link href="2020/12/06/machinelearning/wu-en-da-ji-qi-xue-xi/04-neural-networks/"/>
      <url>2020/12/06/machinelearning/wu-en-da-ji-qi-xue-xi/04-neural-networks/</url>
      
        <content type="html"><![CDATA[<h1 id="神经网络Neural-Network"><a href="#神经网络Neural-Network" class="headerlink" title="神经网络Neural Network"></a>神经网络Neural Network</h1><h2 id="1-非线性假设"><a href="#1-非线性假设" class="headerlink" title="1.非线性假设"></a>1.非线性假设</h2><p>线性回归还是逻辑回归都有这样一个缺点，即:当特征太多时， 计算的负荷会非常大。例如下面的例子：</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206183433.png" alt="示例图"></p><p>之前我们已经看到过，使用非线性的多项式项，能够帮助我们建立更好的分类模型。假设我们有非常多的特征, 例如大于 100 个变量, 我们希望用这 100 个特征来构建一个非线性的多项式模型, 结果将是数量非常惊人的特征组合, 即便我们只采用两两特征的组合 $\left(x_{1} x_{2}+\right.\left.x_{1} x_{3}+x_{1} x_{4}+\ldots+x_{2} x_{3}+x_{2} x_{4}+\ldots+x_{99} x_{100}\right),$ 我们也会有接近 5000 个组合而成的特征。这对于一般的逻辑回归来说需要计算的特征太多了。</p><p>假设我们希望训练一个模型来识别视觉对象（例如识别一张图片上是否是一辆汽车）， 我们怎样才能这么做呢?一种方法是我们利用很多汽车的图片和很多非汽车的图片，然后利用这些图片上一个个像素的值（饱和度或亮度）来作为特征。</p><p>假如我们只选用灰度图片，每个像素则只有一个值（而非 RGB 值），我们可以选取图片上的两个不同位置上的两个像素，然后训练一个逻辑回归算法利用这两个像素的值来判断图片上是否是汽车:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206183716.png" alt="汽车识别"></p><p>假使我们采用的都是 50x50 像素的小图片，并且我们将所有的像素视为特征，则会有 2500 个特征，如果我们要进一步将两两特征组合构成一个多项式模型，则会有约25002/2个 （接近 3 百万个）特征。普通的逻辑回归模型，不能有效地处理这么多的特征，这时候我们需要神经网络。</p><h2 id="2-模型表示-1"><a href="#2-模型表示-1" class="headerlink" title="2.模型表示 1"></a>2.模型表示 1</h2><p>为了构建神经网络模型，我们需要首先思考大脑中的神经网络是怎样的?每一个神经元 都可以被认为是一个处理单元/神经核（processing unit/Nucleus），它含有许多输入/树突 （input/Dendrite），并且有一个输出/轴突（output/Axon）。神经网络是大量神经元相互链 接并通过电脉冲来交流的一个网络。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206223440.png" alt="人体神经网络图"></p><p>神经网络模型建立在很多神经元之上，每一个神经元又是一个个学习模型。这些神经元 （也叫激活单元，activation unit）采纳一些特征作为输出，并且根据本身的模型提供一个输出。下图是一个以逻辑回归模型作为自身学习模型的神经元示例，在神经网络中，参数又可 被成为权重（weight）。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206184747.png" alt="神经元示例"></p><p>我们设计出了类似于神经元的神经网络，效果如下:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206223511.png" alt="神经网络模型"></p><p>其中 $x_{1}, x_{2}, x_{3}$ 是输入单元（input units），我们将原始数据输入给它们。$a_{1}, a_{2}, a_{3}$ 是中间单元，它们负责将数据进行处理，然后呈递到下一层。最后是输出单元，它负责计算 $h_{\theta}(x)$ 。</p><p>神经网络模型是许多逻辑单元按照不同层级组织起来的网络，每一层的输出变量都是下 一层的输入变量。下图为一个 3 层的神经网络，第一层成为输入层（Input Layer），最后一 层称为输出层（Output Layer），中间一层成为隐藏层（Hidden Layers）。我们为每一层都增 加一个偏差单位（bias unit），下面引入一些标记法来帮助描述模型:</p><p>$a_{i}^{(j)}$ 代表第j 层的第 $i$ 个激活单元。 $\theta^{(j)}$ 代表从第 $j$ 层映射到第 $j+1$ 层时的权重的矩阵，例如 $\theta^{(1)}$ 代表从第一层映射到第二层的权重的矩阵。其尺寸为: 以第 $j+1$ 层的激活单元数量为行数，以第 $j$ 层的激活单元数加一为列数的矩阵。例如：上图所示的神经网络中$\theta^{(1)}$ 的尺寸为 $3^{*} 4$ 。</p><p>对于上图所示的模型，激活单元和输出分别表达为:</p><script type="math/tex; mode=display">\begin{array}{l}a_{1}^{(2)}=g\left(\Theta_{10}^{(1)} x_{0}+\Theta_{11}^{(1)} x_{1}+\Theta_{12}^{(1)} x_{2}+\Theta_{13}^{(1)} x_{3}\right) \\a_{2}^{(2)}=g\left(\theta_{20}^{(1)} x_{0}+\Theta_{21}^{(1)} x_{1}+\Theta_{22}^{(1)} x_{2}+\theta_{23}^{(1)} x_{3}\right) \\a_{3}^{(2)}=g\left(\theta_{30}^{(1)} x_{0}+\Theta_{31}^{(1)} x_{1}+\theta_{32}^{(1)} x_{2}+\theta_{33}^{(1)} x_{3}\right) \\h_{\theta}(x)=g\left(\Theta_{10}^{(2)} a_{0}^{(2)}+\Theta_{11}^{(2)} a_{1}^{(2)}+\Theta_{12}^{(2)} a_{2}^{(2)}+\Theta_{13}^{(2)} a_{3}^{(2)}\right)\end{array}</script><p>上面进行的讨论中只是将特征矩阵中的一行（一个训练实例）喂给了神经网络，我们需要将整个训练集都喂给我们的神经网络算法来学习模型。</p><p>我们可以知道:每一个𝑎都是由上一层所有的𝑥和每一个𝑥所对应的决定的。 （我们把这样从左到右的算法称为前向传播算法（FORWARD PROPAGATION））</p><p>把𝑥, 𝜃, 𝑎 分别用矩阵表示，我们可以得到𝜃 ⋅ 𝑋 = 𝑎 :</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206223520.png" alt="image-20201206185810317"></p><h2 id="3-模型表示-2"><a href="#3-模型表示-2" class="headerlink" title="3.模型表示 2"></a>3.模型表示 2</h2><p>（FORWARD PROPAGATION）相对于使用循环来编码，利用向量化的方法会使得计算更 为简便。以上面的神经网络为例，试着计算第二层的值:</p><script type="math/tex; mode=display">x=\left[\begin{array}{l}x_{0} \\x_{1} \\x_{2} \\x_{3}\end{array}\right] \quad z^{(2)}=\left[\begin{array}{l}z_{1}^{(2)} \\z_{2}^{(2)} \\z_{3}^{(2)}\end{array}\right]</script><script type="math/tex; mode=display">\begin{array}{l}z^{(2)}=\Theta^{(1)} x \\a^{(2)}=g\left(z^{(2)}\right)\end{array}</script><script type="math/tex; mode=display">g\left(\left[\begin{array}{llll}\theta_{10}^{(1)} & \theta_{11}^{(1)} & \theta_{12}^{(1)} & \theta_{13}^{(1)} \\\theta_{20}^{(1)} & \theta_{21}^{(1)} & \theta_{22}^{(1)} & \theta_{23}^{(1)} \\\theta_{30}^{(1)} & \theta_{31}^{(1)} & \theta_{32}^{(1)} & \theta_{33}^{(1)}\end{array}\right] \times\left[\begin{array}{l}x_{0} \\x_{1} \\x_{2} \\x_{3}\end{array}\right]\right)=g\left(\left[\begin{array}{l}\theta_{10}^{(1)} x_{0}+\theta_{11}^{(1)} x_{1}+\theta_{12}^{(1)} x_{2}+\theta_{13}^{(1)} x_{3} \\\theta_{20}^{(1)} x_{0}+\theta_{21}^{(1)} x_{1}+\theta_{22}^{(1)} x_{2}+\theta_{23}^{(1)} x_{3} \\\theta_{30}^{(1)} x_{0}+\theta_{31}^{(1)} x_{1}+\theta_{32}^{(1)} x_{2}+\theta_{33}^{(1)} x_{3}\end{array}\right]\right)=\left[\begin{array}{l}a_{1}^{(2)} \\a_{2}^{(2)} \\a_{3}^{(2)}\end{array}\right]</script><p>我们令 $z^{(2)}=\theta^{(1)} x,$ 则 $a^{(2)}=g\left(z^{(2)}\right),$ 计算后添加 $a_{0}^{(2)}=1$。计算输出的值为 :</p><script type="math/tex; mode=display">\left.g\left(\left[\begin{array}{llll}\theta_{10}^{(2)} & \theta_{11}^{(2)} & \theta_{12}^{(2)} & \theta_{13}^{(2)}\end{array}\right] \times \mid \begin{array}{l}u_{0} \\a_{1}^{(2)} \\a_{2}^{(2)} \\a_{3}^{(2)}\end{array}\right]\right)=g\left(\theta_{10}^{(2)} a_{0}^{(2)}+\theta_{11}^{(2)} a_{1}^{(2)}+\theta_{12}^{(2)} a_{2}^{(2)}+\theta_{13}^{(2)} a_{3}^{(2)}\right)=h_{\theta}(x)</script><p>我们令 $z^{(3)}=\theta^{(2)} a^{(2)},$ 则 $h_{\theta}(x)=a^{(3)}=g\left(z^{(3)}\right)$ 。</p><p>这只是针对训练集中一个训练实例所进行的计算。如果我们要对整个训练集进行计算， 我们需要将训练集特征矩阵进行转置，使得同一个实例的特征都在同一列里。即:</p><script type="math/tex; mode=display">\begin{array}{l}z^{(2)}=\Theta^{(1)} \times X^{T} \\a^{(2)}=g\left(z^{(2)}\right)\end{array}</script><p>为了更好了了解 Neuron Networks 的工作原理，我们先把左半部分遮住:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206192646.png" alt="遮住"></p><p>其实神经网络就像是 logistic regression， 只不过我们把 logistic regression 中的输入向量<br>$\left[x_{1} \sim x_{3}\right]$ 变成了中间层的 $\left[a_{1}^{(2)} \sim a_{3}^{(2)}\right],$ 即:</p><p>$h_{\theta}(x)=g\left(\theta_{0}^{(2)} a_{0}^{(2)}+\theta_{1}^{(2)} a_{1}^{(2)}+\theta_{2}^{(2)} a_{2}^{(2)}+\theta_{3}^{(2)} a_{3}^{(2)}\right)$</p><p>我们可以把 $a_{0}, a_{1}, a_{2}, a_{3}$ 看成更为高级的特征值, 也就是 $x_{0}, x_{1}, x_{2}, x_{3}$ 的进化体, 并且它们是由 $x$ 与决定的, 因为是梯度下降的，所以 $a$ 是变化的，并且变得越来越厉害，所以这些更高级的特征值远比仅仅将 $x$ 次方厉害，也能更好的预测新数据。这就是神经网络相比于逻辑回归和线性回归的优势。</p><h2 id="4-特征和直观理解-1"><a href="#4-特征和直观理解-1" class="headerlink" title="4.特征和直观理解 1"></a>4.特征和直观理解 1</h2><p>神经网络中，单层神经元（无中间层）的计算可用来表示逻辑运算，比如逻辑与（AND）、 逻辑或（OR）。</p><p>举例说明:逻辑与（AND）;下图中左半部分是神经网络的设计与 output 层表达式，右边 上部分是 sigmod 函数，下半部分是真值表。</p><p>我们可以用这样的一个神经网络表示 AND 函数:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206194031.png" alt="神经网络表示 AND 函数"></p><p>其中 $\theta_{0}=-30, \theta_{1}=20, \theta_{2}=20$ 我们的输出函数 $h_{\theta}(x)$ 即为: $h_{\theta}(x)=g\left(-30+20 x_{1}+20 x_{2}\right)$</p><p>我们知道 $g(x)$ 的图像是:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206194157.png" alt="Sigmoid Function"></p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206223530.png" alt="image-20201206194239927"></p><p>所以我们有: $h_{\theta}(x) \approx \mathrm{x}_{1} \mathrm{AND} \mathrm{x}_{2}$<br>接下来再介绍一个 OR 函数:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206194319.png" alt="OR 函数"></p><h2 id="5-样本和直观理解-II"><a href="#5-样本和直观理解-II" class="headerlink" title="5.样本和直观理解 II"></a>5.样本和直观理解 II</h2><p>二元逻辑运算符（BINARY LOGICAL OPERATORS）当输入特征为布尔值（0 或 1）时，我 们可以用一个单一的激活层可以作为二元逻辑运算符，为了表示不同的运算符，我们只需要 选择不同的权重即可。</p><p>下图的神经元（三个权重分别为-30，20，20）可以被视为作用同于逻辑与（AND）:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206222420.png" alt="AND"></p><p>下图的神经元（三个权重分别为-10，20，20）可以被视为作用等同于逻辑或（OR）:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206223540.png" alt="OR"></p><p>下图的神经元（两个权重分别为 10，-20）可以被视为作用等同于逻辑非（NOT）:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206223543.png" alt="NOT"></p><p>我们可以利用神经元来组合成更为复杂的神经网络以实现更复杂的运算。例如我们要实现 XNOR 功能（输入的两个值必须一样，均为 1 或均为 0），即:</p><script type="math/tex; mode=display">\mathrm{XNOR}=\left(\mathrm{x}_{1} \mathrm{AND} \mathrm{x}_{2}\right) \mathrm{OR}\left(\left(\mathrm{NOT} \mathrm{x}_{1}\right) \mathrm{AND}\left(\mathrm{NOT} \mathrm{x}_{2}\right)\right)</script><p>首先构造一个能表达$(NOTx_{1}) AND(NOT x_{2})$ 部分的神经元:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206223611.png" alt="AND"></p><p>然后将表示 AND 的神经元和表示 $\left(\mathrm{NOT} \mathrm{x}_{1}\right) \mathrm{AND}\left(\mathrm{NOT} \mathrm{x}_{2}\right)$ 的神经元以及表示 $\mathrm{OR}$ 的神经元进行组合:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206222706.png" alt="XNOR"></p><p>按这种方法我们可以逐渐构造出越来越复杂的函数，也能得到更加厉害的特征值。 这就是神经网络的厉害之处。</p><h2 id="6-多类分类"><a href="#6-多类分类" class="headerlink" title="6.多类分类"></a>6.多类分类</h2><p>当我们有不止两种分类时 （也就是 $y=1,2,3 \ldots $.）比如以下这种情况，该怎么办？如果我们要训练一个神经网络算法来识别路人、汽车、摩托车和卡车，在输出层我们应该有 4 个值。例如，第一个值为 1 或 0 用于预测是否是行人，第二个值用于判断是否为汽车。输入向量x有三个维度，两个中间层，输出层 4 个神经元分别用来表示 4 类，也就是每一个数据在输出层都会出现$[a,b,c,d]^{T},$ 且 $a, b, c, d$ 中仅有一个为 $1,$ 表示当前类。下面是该神经网络的可能结构示例:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206223146.png" alt="多类分类网络"></p>]]></content>
      
      
      <categories>
          
          <category> 吴恩达——机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>03_正则化_Regularization</title>
      <link href="2020/12/05/machinelearning/wu-en-da-ji-qi-xue-xi/03-regularization/"/>
      <url>2020/12/05/machinelearning/wu-en-da-ji-qi-xue-xi/03-regularization/</url>
      
        <content type="html"><![CDATA[<h1 id="正则化（Regularization）"><a href="#正则化（Regularization）" class="headerlink" title="正则化（Regularization）"></a>正则化（Regularization）</h1><h2 id="1-过拟合的问题"><a href="#1-过拟合的问题" class="headerlink" title="1.过拟合的问题"></a>1.过拟合的问题</h2><p>如果我们有非常多的特征，我们通过学习得到的假设可能能够非常好地适应训练集(代价函数可能几乎为 0)，但是可能会不能推广到新的数据。下图是一个回归问题的例子:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201207104636.png" alt="回归问题过拟合例子"></p><p>第一个模型是一个线性模型，欠拟合，不能很好地适应我们的训练集;第三个模型是一个四次方的模型，过于强调拟合原始数据，而丢失了算法的本质:预测新数据。我们可以看出，若给出一个新的值使之预测，它将表现的很差，是过拟合，虽然能非常好地适应我们的训练集但在新输入变量进行预测时可能会效果不好，而中间的模型似乎最合适。分类问题中也存在这样的问题：</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201207104717.png" alt="分类问题过拟合例子"></p><p>就以多项式理解，𝑥 的次数越高，拟合的越好，但相应的预测的能力就可能变差。问题是，如果我们发现了过拟合问题，应该如何处理?</p><ul><li>丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一 些模型选择的算法来帮忙（例如 PCA）</li><li>正则化。 保留所有的特征，但是减少参数的大小（magnitude）</li></ul><h2 id="2-代价函数"><a href="#2-代价函数" class="headerlink" title="2.代价函数"></a>2.代价函数</h2><p>回归问题中如果我们的模型是:</p><script type="math/tex; mode=display">h_{\theta}(x)=\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}^{2}+\theta_{3} x_{3}^{3}+\theta_{4} x_{4}^{4}</script><p>我们可以从之前的事例中看出，正是那些高次项导致了过拟合的产生，所以如果我们能让这些高次项的系数接近于 0 的话，我们就能很好的拟合了。所以我们要做的就是在一定程度上减小这些参数 $\theta$ 的值, 这就是正则化的基本方法。我们决定要减少 $\theta_{3}$ 和 $\theta_{4}$ 的大小, 我们要做的便是修改代价函数,在其中 $\theta_{3}$ 和 $\theta_{4}$ 设置一点惩罚。这样做的话，我们在尝试最小化代价时也需要将这个惩罚纳入考虑中，并最终导致选择较小一些的 $\theta_{3}$ 和 $\theta_{4}$ 。修改后的代价函数如下: </p><script type="math/tex; mode=display">\min _{\theta} \frac{1}{2 m}\left[\sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+1000 \theta_{3}^{2}+10000 \theta_{4}^{2}\right]</script><p>通过这样的代价函数选择出的𝜃3和𝜃4 对预测结果的影响就比之前要小许多。假如我们有非常多的特征，我们并不知道其中哪些特征我们要惩罚，我们将对所有的特征进行惩罚， 并且让代价函数最优化的软件来选择这些惩罚的程度。这样的结果是得到了一个较为简单的能防止过拟合问题的假设：</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{2 m}\left[\sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+\lambda \sum_{j=1}^{n} \theta_{j}^{2}\right]</script><p>其中$\lambda$又称为正则化参数（Regularization Parameter $）$ 。 注: 根据惯例，我们不对 $\theta_{0}$ 进行惩罚。经过正则化处理的模型与原模型的可能对比如下图所示:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201207113348.png" alt="对比图"></p><p>如果选择的正则化参数 $\lambda$ 过大，则会把所有的参数都最小化了，导致模型变成$h_𝜃(𝑥) = 𝜃_0$，也就是上图中红色直线所示的情况，造成欠拟合。</p><p>那为什么增加的一项 $\lambda=\sum_{j=1}^{n} \theta_{j}^{2}$ 可以使 $\theta$ 的值减小呢?因为如果我们令 $\lambda$ 的值很大的话，为了使 cost Function 尽可能的小，所有的 $\theta$ 的值（不包括 $\theta_{0}$ ）都会在一定程度上减小。</p><p>但若 $\lambda$ 的值太大了，那么 $\theta$ （不包括 $\theta_{0}$ ） 都会趋近于 0，这样我们所得到的只能是一条平行于x轴的直线。所以对于正则化，我们要取一个合理的 $\lambda$ 的值，这样才能更好的应用正则化。回顾一下代价函数，为了使用正则化，让我们把这些概念应用到到线性回归和逻辑回归中去，那么我们就可以让他们避免过度拟合了。</p><h2 id="3-正则化线性回归"><a href="#3-正则化线性回归" class="headerlink" title="3.正则化线性回归"></a>3.正则化线性回归</h2><p>正则化线性回归的代价函数为:</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{2 m} \sum_{i=1}^{m}\left[\left(\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+\lambda \sum_{j=1}^{n} \theta_{j}^{2}\right)\right]</script><p>此时梯度下降的公式为：</p><script type="math/tex; mode=display">\theta_{j}:=\theta_{j}-a\left[\frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}+\frac{\lambda}{m} \theta_{j}\right]</script><p>调整后可得：</p><script type="math/tex; mode=display">\theta_{j}:=\theta_{j}\left(1-a \frac{\lambda}{m}\right)-a \frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}</script><p>可以看出，正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新规则的 基础上令𝜃值减少了一个额外的值。</p><p>我们同样也可以利用正规方程来求解正则化线性回归模型，方法如下所示:</p><script type="math/tex; mode=display">\theta=\left(X^{T} X+\lambda\left[\begin{array}{ccc}0 \\& 1 \\& & 1 \\& & & \ddots \\& & & & 1 \\\end{array}\right]\right)^{-1} X^{T} y</script><h2 id="4-正则化逻辑回归"><a href="#4-正则化逻辑回归" class="headerlink" title="4.正则化逻辑回归"></a>4.正则化逻辑回归</h2><p>针对逻辑回归问题，我们在之前的课程已经学习过两种优化算法: 我们首先学习了使用梯度下降法来优化代价函数 $J(\theta)$, 接下来学习了更高级的优化算法，这些高级优化算法需要你自己设计代价函数 $J(\theta)$ 。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201207154734.png" alt="逻辑回归"></p><p>自己计算导数同样对于逻辑回归，我们也给代价函数增加一个正则化的表达式，得到代价函数:</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{m} \sum_{i=1}^{m}\left[-y^{(i)} \log \left(h_{\theta}\left(x^{(i)}\right)\right)-\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]+\frac{\lambda}{2 m} \sum_{j=1}^{n} \theta_{j}^{2}</script><p>Python代码：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">def</span> <span class="token function">costReg</span><span class="token punctuation">(</span>theta<span class="token punctuation">,</span> X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> learningRate<span class="token punctuation">)</span><span class="token punctuation">:</span>  theta <span class="token operator">=</span> np<span class="token punctuation">.</span>matrix<span class="token punctuation">(</span>theta<span class="token punctuation">)</span>  X <span class="token operator">=</span> np<span class="token punctuation">.</span>matrix<span class="token punctuation">(</span>X<span class="token punctuation">)</span>  y <span class="token operator">=</span> np<span class="token punctuation">.</span>matrix<span class="token punctuation">(</span>y<span class="token punctuation">)</span>  first <span class="token operator">=</span> np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span><span class="token operator">-</span>y<span class="token punctuation">,</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>sigmoid<span class="token punctuation">(</span>X<span class="token operator">*</span>theta<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  second <span class="token operator">=</span> np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> y<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> sigmoid<span class="token punctuation">(</span>X<span class="token operator">*</span>theta<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  reg <span class="token operator">=</span> <span class="token punctuation">(</span>learningRate <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">*</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>power<span class="token punctuation">(</span>theta<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">:</span>the  ta<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token keyword">return</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>first <span class="token operator">-</span> second<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> reg<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>要最小化该代价函数，通过求导，得出梯度下降算法为:</p><script type="math/tex; mode=display">\begin{array}{l}\theta_{0}:=\theta_{0}-a \frac{1}{m} \sum_{i=1}^{m}\left(\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{0}^{(i)}\right) \\\theta_{j}:=\theta_{j}-a\left[\frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}+\frac{\lambda}{m} \theta_{j}\right]\end{array}</script>]]></content>
      
      
      <categories>
          
          <category> 吴恩达——机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 正则化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>02_逻辑回归_Logistic_Regression</title>
      <link href="2020/12/04/machinelearning/wu-en-da-ji-qi-xue-xi/02-logistic-regression/"/>
      <url>2020/12/04/machinelearning/wu-en-da-ji-qi-xue-xi/02-logistic-regression/</url>
      
        <content type="html"><![CDATA[<h1 id="逻辑回归（Logistic-Regression）"><a href="#逻辑回归（Logistic-Regression）" class="headerlink" title="逻辑回归（Logistic Regression）"></a>逻辑回归（Logistic Regression）</h1><h2 id="1-分类问题（Classification）"><a href="#1-分类问题（Classification）" class="headerlink" title="1.分类问题（Classification）"></a>1.分类问题（Classification）</h2><p>分类问题中，需要预测的变量y是离散的值</p><h2 id="2-假说表示"><a href="#2-假说表示" class="headerlink" title="2.假说表示"></a>2.假说表示</h2><p>逻辑回归模型的假设是: $\quad h_{\theta}(x)=g\left(\theta^{T} X\right)$ 其中: $\quad X$ 代表特征向量 $g$ 代表逻辑函数（logistic function)是一个常用的逻辑函数为 S 形函数(Sigmoid function)，公式为:</p><script type="math/tex; mode=display">g(z)=\frac{1}{1+e^{-z}}</script><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np <span class="token keyword">def</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>z<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">return</span> <span class="token number">1</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>z<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>$h_{\theta}(x)$ 的作用是，对于给定的输入变量，根据选择的参数计算输出变量=1 的可能性(estimated probablity) 即 $h_{\theta}(x)=P(y=1 \mid x ; \theta)$<br>例如，如果对于给定的 $x,$ 通过已经确定的参数计算得出 $h_{\theta}(x)=0.7,$ 则表示有 $70 \%$ 的几率 $y$ 为正向类，相应地 $y$ 为负向类的几率为 1-0.7=0.3。</p><h2 id="3-判定边界Decision-Boundary"><a href="#3-判定边界Decision-Boundary" class="headerlink" title="3.判定边界Decision Boundary"></a>3.判定边界Decision Boundary</h2><p>0现在假设我们有一个模型:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201205224807.png" alt="image-20201205224807126"></p><p>可以选直线：</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201205224836.png" alt="image-20201205224836663"></p><p>但其他的比如边界的圆形：</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201205224903.png" alt="image-20201205224903704"></p><p>我们可以用非常复杂的模型来适应非常复杂形状的判定边界</p><h2 id="4-代价函数Cost-Function"><a href="#4-代价函数Cost-Function" class="headerlink" title="4.代价函数Cost Function"></a>4.代价函数Cost Function</h2><p>如何拟合逻辑回归模型的参数𝜃。具体来说，拟合参数的优化目标或者叫代价函数，这便是监督学习问题中的逻辑回归模型的拟合问题。</p><p>对于线性回归模型，我们定义的代价函数是所有模型误差的平方和。理论上来说，我们也可以对逻辑回归模型沿用这个定义，但是问题在于，当我们将 $h_{\theta}(x)=\frac{1}{1+e^{-\theta} T_{X}}$ 带入到这样定义了的代价函数中时，我们得到的代价函数将是一个非凸函数（non-convexfunction），这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201205225126.png" alt="image-20201205225126378"></p><p>线性回归的代价函数为:</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{m} \sum_{i=1}^{m} \frac{1}{2}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}</script><p>我们重新定义逻辑回归的代价函数为:</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{m} \sum_{i=1}^{m} \operatorname{cost}\left(h_{\theta}\left(x^{(i)}\right), y^{(i)}\right)</script><p>其中，</p><script type="math/tex; mode=display">\operatorname{cost}\left(h_{\theta}(x), y\right)=\left\{\begin{aligned}-\log \left(h_{\theta}(x)\right) & \text { if } y=1 \\-\log \left(1-h_{\theta}(x)\right) & \text { if } y=0\end{aligned}\right.</script><p>$h_{\theta}(x)$ 与 $\operatorname{cost}\left(h_{\theta}(x), y\right)$ 之间的关系如下图所示:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201205225317.png" alt="image-20201205225317462"></p><p>这样构建的 $\operatorname{cost}\left(h_{\theta}(x), y\right)$ 函数的特点是：当实际的 $y=1$ 且 $h_{\theta}(x)$ 也为 1 时误差为 0 ,当 $y=1$ 但 $h_{\theta}(x)$ 不为 1 时误差随着 $h_{\theta}(x)$ 变小而变大; 当实际的 $y=0$ 且 $h_{\theta}(x)$ 也为 0 时代价为 $0,$ 当 $y=0$ 但 $h_{\theta}(x)$ 不为 0 时误差随着 $h_{\theta}(x)$ 的变大而变大。将构建的 $\operatorname{cost}\left(h_{\theta}(x), y\right)$ 简化如下:</p><script type="math/tex; mode=display">\operatorname{cost}\left(h_{\theta}(x), y\right)=-y \times \log \left(h_{\theta}(x)\right)-(1-y) \times \log \left(1-h_{\theta}(x)\right)</script><p>带入代价函数得到:</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{m} \sum_{i=1}^{m}\left[-y^{(i)} \log \left(h_{\theta}\left(x^{(i)}\right)\right)-\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]</script><p>即: </p><script type="math/tex; mode=display">J(\theta)=-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)} \log \left(h_{\theta}\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]</script><p>Python代码实现为：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np <span class="token keyword">def</span> <span class="token function">cost</span><span class="token punctuation">(</span>theta<span class="token punctuation">,</span> X<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>  theta <span class="token operator">=</span> np<span class="token punctuation">.</span>matrix<span class="token punctuation">(</span>theta<span class="token punctuation">)</span>  X <span class="token operator">=</span> np<span class="token punctuation">.</span>matrix<span class="token punctuation">(</span>X<span class="token punctuation">)</span>  y <span class="token operator">=</span> np<span class="token punctuation">.</span>matrix<span class="token punctuation">(</span>y<span class="token punctuation">)</span>  first <span class="token operator">=</span> np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span><span class="token operator">-</span>y<span class="token punctuation">,</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>sigmoid<span class="token punctuation">(</span>X<span class="token operator">*</span> theta<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  second <span class="token operator">=</span> np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> y<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> sigmoid<span class="token punctuation">(</span>X<span class="token operator">*</span> theta<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>   <span class="token keyword">return</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>first <span class="token operator">-</span> second<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在得到这样一个代价函数以后，我们便可以用梯度下降算法来求得能使代价函数最小的参数了。算法为:</p><script type="math/tex; mode=display">\theta_{j}:=\theta_{j}-\alpha \frac{\partial}{\partial \theta_{j}} J(\theta)</script><p>求导后：</p><script type="math/tex; mode=display">\theta_{j}:=\theta_{j}-\alpha \frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(\mathrm{x}^{(i)}\right)-\mathrm{y}^{(i)}\right) \mathrm{x}_{j}^{(i)}</script><p>代价函数𝐽(𝜃)会是一 个凸函数，并且没有局部最优值。推导过程:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201205230522.png" alt="image-20201205230522918"></p><p>所以：</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201205230500.png" alt="image-20201205230500427"></p><p>虽然得到的梯度下降算法表面上看上去与线性回归的梯度下降算法一样，但是这里的$h_𝜃(𝑥) = 𝑔(𝜃^𝑇𝑋)$与线性回归中不同，所以实际上是不一样的。另外，在运行梯度下降算法 之前，进行特征缩放依旧是非常必要的。一些梯度下降算法之外的选择: 除了梯度下降算法以外，还有一些常被用来令代价函数最小的算法，这些算法更加复杂和优越，而且通常不需要人工选择学习率，通常比梯度下降算法要更加快速。这些算法有:共轭梯度（Conjugate Gradient），局部优化法（Broyden fletcher goldfarb shann,BFGS）和有限内存局部优化法（LBFGS）</p><h2 id="5-梯度下降"><a href="#5-梯度下降" class="headerlink" title="5.梯度下降"></a>5.梯度下降</h2><p>和之前的线性回归类似:</p><script type="math/tex; mode=display">\theta_{j}:=\theta_{j}-\alpha \frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}</script><p>但是现在逻辑函数假设函数: $h_{\theta}(x)=\frac{1}{1+e^{-\theta} T_{X}}$，即使更新参数的规则看起来基本相同，但由于假设的定义发生了变化，所以逻辑函数的梯度下降，跟线性回归的梯度下降实际上是两个完全不同的东西。</p><h2 id="6-高级优化"><a href="#6-高级优化" class="headerlink" title="6.高级优化"></a>6.高级优化</h2><p>然而梯度下降并不是我们可以使用的唯一算法，还有其他一些算法，更高级、更复杂。如果我们能用这些方法来计算代价函数 $J(\theta)$ 和偏导数项 $\frac{\partial}{\partial \theta_{j}} J(\theta)$ 两个项的话，那么这些算法就是为我们优化代价函数的不同方法，共轩梯度法 BFGS （变尺度法）和 L-BFGS （限制变尺度法）就是其中一些更高级的优化算法，它们需要有一种方法来计算 𝐽(𝜃)，以及需要一种方法 计算导数项，然后使用比梯度下降更复杂的算法来最小化代价函数。</p><h2 id="7-多类别分类-一对多"><a href="#7-多类别分类-一对多" class="headerlink" title="7.多类别分类:一对多"></a>7.多类别分类:一对多</h2><p>然而对于之前的一个，二元分类问题，我们的数据看起来可能是像这样:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206000542.png" alt="image-20201206000542702"></p><p>对于一个多类分类问题，我们的数据集或许看起来像这样:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206000556.png" alt="image-20201206000556885"></p><p>我们现在已经知道如何进行二元分类，可以使用逻辑回归，对于直线或许你也知道，可以将数据集一分为二为正类和负类。用一对多的分类思想，我们可以将其用在多类分类问题 上。下面将介绍如何进行一对多的分类工作，有时这个方法也被称为”一对余”方法。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206000949.png" alt="image-20201206000949232"></p><p>方法就是将多分类问题转化成多个二分类问题，例如上图先把除了三角形的全视为一类，转化成一个二分类问题。为了能实现这样的转变，我们将多个类中的一个类标记为正向类（ $y=1$） ，然后将其他所有类都标记为负向类，这个模型记作 $h_{\theta}^{(1)}(x)_{\circ}$ 接着，类似地第我们选择另一个类标记为正向类 $(y=2)$ ，再将其它类都标记为负向类，将这个模型记作 $h_{\theta}^{(2)}(x)$,依此类推。最后我们得到一系列的模型简记为: $\quad h_{\theta}^{(i)}(x)=p(y=i \mid x ; \theta)$ 其中: $i=(1,2,3 \ldots k)$</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206001229.png" alt="image-20201206001229747"></p><p>最后，将所有的分类机都运行一遍，然后对每一个输入变量， 都选择最高可能性的输出变量。</p>]]></content>
      
      
      <categories>
          
          <category> 吴恩达——机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 逻辑回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>01_线性回归_Linear_Regression</title>
      <link href="2020/12/03/machinelearning/wu-en-da-ji-qi-xue-xi/01-linear-regression/"/>
      <url>2020/12/03/machinelearning/wu-en-da-ji-qi-xue-xi/01-linear-regression/</url>
      
        <content type="html"><![CDATA[<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>回归本质上是利用离散的值来试着推测出一个连续值的结果，对其他值进行预测</p><h2 id="1-单变量线性回归（Linear-Regression-with-One-Variable）"><a href="#1-单变量线性回归（Linear-Regression-with-One-Variable）" class="headerlink" title="1 单变量线性回归（Linear Regression with One Variable）"></a>1 单变量线性回归（Linear Regression with One Variable）</h2><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201202232210.png" alt="image-20201202232210402"></p><h3 id="1-1-例子房价预测问题："><a href="#1-1-例子房价预测问题：" class="headerlink" title="1.1 例子房价预测问题："></a>1.1 例子房价预测问题：</h3><p>假使回归问题的训练集（Training Set）如下表所示</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201202232330.png" alt="image-20201202232330802"></p><p>我们将要用来描述这个回归问题的标记如下: </p><ul><li><p>$𝑚$代表训练集中实例的数量</p></li><li><p>$𝑥$ 代表特征/输入变量</p></li><li>$𝑦$ 代表目标变量/输出变量</li><li>$(𝑥,𝑦)$ 代表训练集中的实例</li><li>$(𝑥^{(𝑖)},𝑦^{(𝑖)})$代表第$𝑖$ 个观察实例</li><li>$h$ 代表学习算法的解决方案或函数也称为假设(hypothesis)</li></ul><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201202232830.png" alt="image-20201202232830677"></p><p>我们该如何表达 h?</p><p>一种可能的表达方式为:$h_𝜃(𝑥) = 𝜃_0 + 𝜃_1𝑥$，因为只含有一个特征/输入变量，因此这样 的问题叫作单变量线性回归问题。</p><h3 id="1-2-代价函数"><a href="#1-2-代价函数" class="headerlink" title="1.2 代价函数"></a>1.2 代价函数</h3><h4 id="误差定义："><a href="#误差定义：" class="headerlink" title="误差定义："></a>误差定义：</h4><p>预测值与实际值的的差距。$h_𝜃(x)-y$</p><h4 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h4><p>对于$h_𝜃(𝑥) = 𝜃_0 + 𝜃_1𝑥$，目的是找到两个变量$𝜃_0,𝜃_1$来使得整体误差的值最小，即误差平方和最小，代价公式为：</p><script type="math/tex; mode=display">MSE=J\left(\theta_{0}, \theta_{1}\right)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}</script><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201202233929.png" alt="image-20201202233929481"></p><h3 id="1-3-梯度下降"><a href="#1-3-梯度下降" class="headerlink" title="1.3 梯度下降"></a>1.3 梯度下降</h3><p>批量梯度下降（batch gradient descent）:</p><script type="math/tex; mode=display">\begin{aligned}&\text { repeat until convergence \{}\\&\theta_{j}:=\theta_{j}-\alpha \frac{\partial}{\partial \theta_{j}} J\left(\theta_{0}, \theta_{1}\right) \quad(\text { for } j=0 \text { and } j=1)\\&\text{\}}\end{aligned}</script><p>其中$𝑎$是学习率（learning rate），它决定了我们沿着能让代价函数下降程度最大的方向 向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数，参数更新的步骤如下</p><script type="math/tex; mode=display">\begin{array}{l}\text { temp } 0:=\theta_{0}-\alpha \frac{\partial}{\partial \theta_{0}} J\left(\theta_{0}, \theta_{1}\right) \\\text { templ }:=\theta_{1}-\alpha \frac{\partial}{\partial \theta_{1}} J\left(\theta_{0}, \theta_{1}\right) \\\theta_{0}:=\text { temp } 0 \\\theta_{1}:=\text { temp } 1\end{array}</script><ul><li>$a$太小收敛速度太慢</li><li>$a$太大可能不收敛</li></ul><h3 id="1-4-梯度下降的线性回归"><a href="#1-4-梯度下降的线性回归" class="headerlink" title="1.4 梯度下降的线性回归"></a>1.4 梯度下降的线性回归</h3><script type="math/tex; mode=display">\begin{array}{c}\frac{\partial}{\partial \theta_{j}} J\left(\theta_{0}, \theta_{1}\right)=\frac{\partial}{\partial \theta_{j}} \frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2} \\j=0 \text { 时: } \frac{\partial}{\partial \theta_{0}} J\left(\theta_{0}, \theta_{1}\right)=\frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) \\j=1 \text { 时: } \quad \frac{\partial}{\partial \theta_{1}} J\left(\theta_{0}, \theta_{1}\right)=\frac{1}{m} \sum_{i=1}^{m}\left(\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) \cdot x^{(i)}\right)\end{array}</script><h2 id="2-多变量线性回归（Linear-Regression-with-Multiple-Variables）"><a href="#2-多变量线性回归（Linear-Regression-with-Multiple-Variables）" class="headerlink" title="2 多变量线性回归（Linear Regression with Multiple Variables）"></a>2 多变量线性回归（Linear Regression with Multiple Variables）</h2><p>对上面的模型增加更多的特征，模型中的特征为$(x_1,x_2,….,x_n)$</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201203163630.png" alt="image-20201203150333545"></p><p>增添更多特征后，我们引入一系列新的注释:</p><ul><li><p>$𝑛$ 代表特征的数量</p></li><li><p>$𝑥^{(𝑖)}$代表第 $𝑖$ 个训练实例，是特征矩阵中的第$𝑖$行，是一个向量(vector)。</p></li><li><p>比方说，上图的</p><script type="math/tex; mode=display">x^{(2)}=\left[\begin{array}{c}1416 \\3 \\2 \\40\end{array}\right]</script></li><li><p>$𝑥^{(𝑖)}_j$代表特征矩阵中第$ 𝑖$ 行的第$𝑗$个特征，也就是第 𝑖 个训练实例的第 𝑗 个特征。</p></li><li><p>如上图的$\mid x_{2}^{(2)}=3, x_{3}^{(2)}=2$</p></li><li><p>支持多变量的假设 h 表示为:$h_{\theta}(x)=\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}+\ldots+\theta_{n} x_{n}$</p></li><li><p>这个公式中有𝑛 + 1个参数和𝑛个变量，为了使得公式能够简化一些，引入$𝑥_0 = 1$，则公 式转化为:$h_{\theta}(x)=\theta_{0} x_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}+\ldots+\theta_{n} x_{n}$</p></li><li><p>此时模型中的参数是一个$𝑛 + 1$维的向量，任何一个训练实例也都是𝑛 + 1维的向量，特 征矩阵𝑋的维度是 $𝑚 ∗ (𝑛 + 1)$。 因此公式可以简化为:$h_𝜃(𝑥) = 𝜃^𝑇𝑋$，其中上标𝑇代表矩阵转置。</p></li></ul><h3 id="2-1-多变量梯度下降Gradient-Descent-for-Multiple-Variables"><a href="#2-1-多变量梯度下降Gradient-Descent-for-Multiple-Variables" class="headerlink" title="2.1 多变量梯度下降Gradient Descent for Multiple Variables"></a>2.1 多变量梯度下降Gradient Descent for Multiple Variables</h3><p>与单变量线性回归类似，在多变量线性回归中，我们也构建一个代价函数，则这个代价 函数是所有建模误差的平方和，即:</p><script type="math/tex; mode=display">J\left(\theta_{0}, \theta_{1} \ldots \theta_{n}\right)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}</script><p>梯度下降算法求导后：</p><p>Repeat：</p><script type="math/tex; mode=display">\theta_{j}:=\theta_{j}-\alpha \frac{1}{m} \sum_{i=1}^{m}\left(\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) \cdot x_{j}^{(i)}\right)</script><p>( simultaneously update  $\theta_{j}$$ \text { for } \mathrm{j}=0,1, \ldots, \mathrm{n})$</p><p>计算Cost函数Python代码：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">computeCost</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> theta<span class="token punctuation">)</span><span class="token punctuation">:</span>  inner <span class="token operator">=</span> np<span class="token punctuation">.</span>power<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">(</span>X <span class="token operator">*</span> theta<span class="token punctuation">.</span>T<span class="token punctuation">)</span> <span class="token operator">-</span> y<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>   <span class="token keyword">return</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>inner<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="2-2-梯度下降法实践-1-特征缩放"><a href="#2-2-梯度下降法实践-1-特征缩放" class="headerlink" title="2.2 梯度下降法实践 1-特征缩放"></a>2.2 梯度下降法实践 1-特征缩放</h3><p>Gradient Descent in Practice I - Feature Scaling</p><p>在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛，解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201203153342.png" alt="image-20201203153342751"></p><p>最简单的方法是令:$x_{n}=\frac{x_{n}-\mu_{n}}{s_{n}},$ 其中 $\mu_{n}$ 是平均值, $s_{n}$ 是标准差。</p><h3 id="2-3-梯度下降法实践-2-学习率"><a href="#2-3-梯度下降法实践-2-学习率" class="headerlink" title="2.3 梯度下降法实践 2-学习率"></a>2.3 梯度下降法实践 2-学习率</h3><p>Gradient Descent in Practice II - Learning Rate</p><p>梯度下降算法的每次迭代受到学习率的影响，如果学习率𝑎过小，则达到收敛所需的迭 代次数会非常高;如果学习率𝑎过大，每次迭代可能不会减小代价函数，可能会越过局部最 小值导致无法收敛。</p><h3 id="2-4-特征和多项式回归"><a href="#2-4-特征和多项式回归" class="headerlink" title="2.4 特征和多项式回归"></a>2.4 特征和多项式回归</h3><p>线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，比如一个二次方模型: $h_{\theta}(x)=\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}^{2}$<br>或者三次方模型: $\quad h_{\theta}(x)=\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}^{2}+\theta_{3} x_{3}^{3}$</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201203155639.png" alt="image-20201203155639868"></p><p>通常我们需要先观察数据然后再决定准备尝试怎样的模型。 另外，我们可以令：$x_{2}=x_{2}^{2}, x_{3}=x_{3}^{3},$ 从而将模型转化为线性回归模型。</p><p>从而将模型转化为线性回归模型。</p><h3 id="2-5-正规方程"><a href="#2-5-正规方程" class="headerlink" title="2.5 正规方程"></a>2.5 正规方程</h3><p>正规方程是通过求解下面的方程来找出使得代价函数最小的参数的 $： \frac{\partial}{\partial \theta_{j}} J\left(\theta_{j}\right)=0$，假设我们的训练集特征矩阵为 X（包含了 $x_{0}=1$ ）并且我们的训练集结果为向量 y，则利用正规方程解出向量 $\theta=\left(X^{T} X\right)^{-1} X^{T} y$ 。上标 T 代表矩阵转置，上标-1 代表矩阵的逆。设矩阵 $A=X^{T} X,$ 则: $\left(X^{T} X\right)^{-1}=A^{-1}$</p><p> 梯度下降与正规方程的比较:</p><div class="table-container"><table><thead><tr><th style="text-align:left">梯度下降</th><th>正规方程</th></tr></thead><tbody><tr><td style="text-align:left">需要选择学习率𝛼</td><td>不需要</td></tr><tr><td style="text-align:left">需要多次迭代</td><td>一次运算得出</td></tr><tr><td style="text-align:left">当特征数量𝑛大时也能较好适用</td><td>需要计算 $\left(X^{T} X\right)^{-1}$ 如果特征数量 $n$ 较大则运算代价大, 因为矩阵逆的计算时间复杂度 为 $O\left(n^{3}\right),$ 通常来说当n小于10000 时还是 可以接受的</td></tr><tr><td style="text-align:left">适用于各种类型的模型</td><td>只适用于线性模型，不适合逻辑回归模型等 其他模型</td></tr></tbody></table></div><p>正规方程的python实现：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np <span class="token keyword">def</span> <span class="token function">normalEqn</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>  theta <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>inv<span class="token punctuation">(</span>X<span class="token punctuation">.</span>T@X<span class="token punctuation">)</span>@X<span class="token punctuation">.</span>T@y <span class="token comment">#X.T@X 等价于 X.T.dot(X) </span>  <span class="token keyword">return</span> theta<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 吴恩达——机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 线性回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Chapter_03_Dictionaries_and_Sets</title>
      <link href="2020/12/03/python/fluentpython/chapter-03-dictionaries-and-sets/"/>
      <url>2020/12/03/python/fluentpython/chapter-03-dictionaries-and-sets/</url>
      
        <content type="html"><![CDATA[<h2 id="3-1-dict"><a href="#3-1-dict" class="headerlink" title="3.1 dict"></a>3.1 dict</h2><h3 id="泛映射类型"><a href="#泛映射类型" class="headerlink" title="泛映射类型"></a>泛映射类型</h3><ul><li>collections.abc中有Mapping和MutableMapping这两个抽象类，作用dict的接口<br><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201201112313.png" alt="Mutable Mapping"></li><li>dict只有<strong>可散列的</strong>数据类型才能作用这些映射里的键</li><li>可散列对象需要实现__hash__,__eq__</li><li>str，bytes，frozenset是可散列的，如果tuple中所有元素是可散列的，那么tuple也是可散列的</li><li>一般用户自定义的类型对象都是可散列的，id()函数的返回值就是散列值</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">tt<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">hash</span><span class="token punctuation">(</span>tt<span class="token punctuation">)</span><span class="token punctuation">)</span>tt<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">hash</span><span class="token punctuation">(</span>tt<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>-2725224101759650258</p><hr><p>TypeError Traceback (most recent call last)</p><p><ipython-input-1-083a0773f165> in <module>       2 print(hash(tt))<br>      3 tt=(1,2,[3,4])<br>——&gt; 4 print(hash(tt))<br>TypeError: unhashable type: ‘list’</module></ipython-input-1-083a0773f165></p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># dict的创建方式</span>a <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>one<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> two<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> three<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>b <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'one'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'two'</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'three'</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">}</span>c <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'one'</span><span class="token punctuation">,</span> <span class="token string">'two'</span><span class="token punctuation">,</span> <span class="token string">'three'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>d <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'two'</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'one'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'three'</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>e <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token string">'three'</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'one'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'two'</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">}</span><span class="token punctuation">)</span>a<span class="token operator">==</span>b<span class="token operator">==</span>c<span class="token operator">==</span>d<span class="token operator">==</span>e<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>True</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># dict 推导</span>DIAL_CODES <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token punctuation">(</span><span class="token number">86</span><span class="token punctuation">,</span> <span class="token string">'China'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token number">91</span><span class="token punctuation">,</span> <span class="token string">'India'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'United States'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token number">62</span><span class="token punctuation">,</span> <span class="token string">'Indonesia'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token number">55</span><span class="token punctuation">,</span> <span class="token string">'Brazil'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token number">92</span><span class="token punctuation">,</span> <span class="token string">'Pakistan'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token number">880</span><span class="token punctuation">,</span> <span class="token string">'Bangladesh'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token number">234</span><span class="token punctuation">,</span> <span class="token string">'Nigeria'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token string">'Russia'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token number">81</span><span class="token punctuation">,</span> <span class="token string">'Japan'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">]</span>country_code <span class="token operator">=</span> <span class="token punctuation">{</span>country<span class="token punctuation">:</span> code <span class="token keyword">for</span> code<span class="token punctuation">,</span> country <span class="token keyword">in</span> DIAL_CODES<span class="token punctuation">}</span><span class="token keyword">print</span><span class="token punctuation">(</span>country_code<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>{‘China’: 86, ‘India’: 91, ‘United States’: 1, ‘Indonesia’: 62, ‘Brazil’: 55, ‘Pakistan’: 92, ‘Bangladesh’: 880, ‘Nigeria’: 234, ‘Russia’: 7, ‘Japan’: 81}</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#d.get(k,default)来代替d[k],如果没有查找到就返回默认值</span><span class="token keyword">import</span> sys<span class="token keyword">import</span> reWORD_RE <span class="token operator">=</span> re<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span><span class="token string">r'\w+'</span><span class="token punctuation">)</span>index <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'poetry.txt'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> fp<span class="token punctuation">:</span>    <span class="token keyword">for</span> line_no<span class="token punctuation">,</span> line <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>fp<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> match <span class="token keyword">in</span> WORD_RE<span class="token punctuation">.</span>finditer<span class="token punctuation">(</span>line<span class="token punctuation">)</span><span class="token punctuation">:</span>            word <span class="token operator">=</span> match<span class="token punctuation">.</span>group<span class="token punctuation">(</span><span class="token punctuation">)</span>            column_no <span class="token operator">=</span> match<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span>            location <span class="token operator">=</span> <span class="token punctuation">(</span>line_no<span class="token punctuation">,</span> column_no<span class="token punctuation">)</span>            occurrences <span class="token operator">=</span> index<span class="token punctuation">.</span>get<span class="token punctuation">(</span>word<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment">#</span>            occurrences<span class="token punctuation">.</span>append<span class="token punctuation">(</span>location<span class="token punctuation">)</span>            index<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token operator">=</span> occurrences<span class="token keyword">for</span> word <span class="token keyword">in</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span>index<span class="token punctuation">,</span> key<span class="token operator">=</span><span class="token builtin">str</span><span class="token punctuation">.</span>upper<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>word<span class="token punctuation">,</span> index<span class="token punctuation">[</span>word<span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>a [(1, 22), (2, 7), (3, 6), (8, 7), (8, 27), (10, 19), (12, 6), (17, 20), (21, 19)]<br>A [(17, 1)]<br>above [(34, 34)]<br>ago [(1, 29), (15, 36)]<br>all [(26, 30), (43, 9)]<br>and [(1, 13), (6, 17), (8, 15), (11, 6), (13, 16), (25, 21), (29, 13), (44, 37)]<br>And [(5, 1), (15, 1), (20, 4), (34, 1), (41, 1), (43, 1)]<br>angels [(24, 5), (34, 17)]<br>Annabel [(4, 19), (11, 13), (18, 17), (29, 28), (37, 21), (40, 21), (42, 21)]<br>as [(26, 27)]<br>away [(20, 17)]<br>be [(6, 21)]<br>beams [(39, 20)]<br>beautiful [(18, 7), (37, 11), (40, 11), (42, 11)]<br>blew [(17, 8)]<br>bore [(20, 8)]<br>bride [(44, 44)]<br>bright [(41, 42)]<br>bringing [(39, 35)]<br>But [(10, 1), (31, 1)]<br>but [(41, 27)]<br>by [(2, 17), (6, 30), (9, 20), (16, 20), (22, 20), (27, 20), (28, 37), (31, 30), (43, 40), (45, 27), (46, 16)]<br>By [(4, 4)]<br>came [(19, 30), (28, 15)]<br>Can [(36, 1)]<br>child [(8, 9), (8, 29)]<br>chilling [(17, 29)]<br>Chilling [(29, 4)]<br>cloud [(17, 22), (28, 31)]<br>Coveted [(13, 4)]<br>darling [(44, 10), (44, 21)]<br>demons [(35, 12)]<br>dissever [(36, 10)]<br>down [(35, 19), (43, 35)]<br>dreams [(39, 47)]<br>envying [(25, 9)]<br>ever [(36, 5)]<br>eyes [(41, 49)]<br>far [(31, 33), (33, 12)]<br>feel [(41, 33)]<br>For [(39, 1)]<br>from [(20, 22), (36, 27)]<br>half [(24, 17)]<br>happy [(24, 25)]<br>Heaven [(12, 40), (24, 34), (34, 27)]<br>her [(13, 12), (19, 9), (20, 13), (21, 9), (25, 17), (45, 7), (46, 7)]<br>highborn [(19, 13)]<br>I [(8, 1), (11, 4), (41, 31), (43, 29)]<br>In [(2, 4), (9, 4), (16, 4), (22, 4), (27, 4), (45, 4), (46, 4)]<br>in [(21, 16), (24, 31), (34, 24)]<br>It [(1, 1)]<br>it [(31, 14)]<br>killing [(29, 17)]<br>kingdom [(2, 9), (9, 12), (16, 12), (22, 12), (27, 12)]<br>kinsmen [(19, 22)]<br>know [(3, 40), (26, 38)]<br>Lee [(4, 27), (11, 21), (18, 25), (29, 36), (37, 29), (40, 29), (42, 29)]<br>lie [(43, 31)]<br>life [(44, 32)]<br>lived [(3, 21), (5, 21)]<br>long [(15, 31)]<br>love [(6, 12), (10, 21), (10, 45), (12, 8), (31, 9), (31, 46)]<br>loved [(6, 24), (10, 8)]<br>maiden [(3, 8), (5, 10)]<br>many [(1, 8), (1, 17), (33, 7)]<br>may [(3, 36)]<br>me [(6, 33), (13, 20), (20, 27), (25, 25), (39, 44)]<br>men [(26, 34)]<br>moon [(39, 9)]<br>more [(10, 35)]<br>my [(11, 10), (29, 25), (36, 19), (44, 7), (44, 18), (44, 29), (44, 41)]<br>My [(18, 4)]<br>name [(4, 11)]<br>neither [(34, 5)]<br>never [(39, 14), (41, 15)]<br>night [(28, 40), (43, 17)]<br>no [(5, 32)]<br>Nor [(35, 4)]<br>not [(24, 13)]<br>of [(4, 16), (12, 37), (17, 17), (28, 24)]<br>Of [(32, 4), (33, 4), (37, 4), (40, 4), (42, 4), (44, 4)]<br>older [(32, 22)]<br>other [(5, 35)]<br>our [(31, 5)]<br>out [(17, 13), (28, 20)]<br>reason [(15, 18), (26, 19)]<br>rise [(41, 21)]<br>sea [(2, 24), (9, 27), (16, 27), (22, 27), (27, 27), (35, 34), (45, 34), (46, 32)]<br>sepulchre [(21, 21), (45, 11)]<br>seraphs [(12, 29)]<br>she [(5, 17), (8, 19)]<br>shut [(21, 4)]<br>side [(43, 47)]<br>So [(19, 1)]<br>so [(24, 22), (43, 5)]<br>soul [(36, 22), (36, 36)]<br>sounding [(46, 23)]<br>stars [(41, 9)]<br>stronger [(31, 21)]<br>Than [(6, 4)]<br>than [(10, 40), (31, 37), (32, 28), (33, 22)]<br>That [(3, 1), (28, 1)]<br>that [(10, 26), (12, 13), (15, 25), (19, 4), (26, 6)]<br>the [(2, 20), (4, 7), (9, 23), (12, 18), (15, 14), (16, 23), (22, 23), (26, 15), (27, 23), (28, 6), (28, 27), (31, 42), (34, 13), (35, 8), (35, 30), (36, 32), (37, 7), (39, 5), (40, 7), (41, 5), (41, 38), (42, 7), (43, 13), (43, 43), (45, 30), (46, 19)]<br>The [(24, 1)]<br>there [(3, 15), (45, 21)]<br>this [(5, 5), (9, 7), (15, 5), (16, 7), (22, 7), (27, 7)]<br>those [(32, 7)]<br>thought [(5, 41)]<br>tide [(43, 23)]<br>to [(6, 9)]<br>To [(21, 1)]<br>tomb [(46, 11)]<br>under [(35, 24)]<br>up [(21, 13)]<br>was [(1, 4), (8, 3), (8, 23), (10, 31), (15, 10), (26, 11), (31, 17)]<br>we [(10, 5), (32, 33), (33, 27)]<br>Went [(25, 4)]<br>were [(32, 17)]<br>who [(32, 13)]<br>whom [(3, 27)]<br>wind [(17, 3), (28, 10)]<br>wingèd [(12, 22)]<br>wiser [(33, 16)]<br>with [(5, 27), (10, 14)]<br>With [(12, 1)]<br>without [(39, 27)]<br>year [(1, 24)]<br>Yes [(26, 1)]<br>you [(3, 32)]</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#用setdefault处理找不到的键</span><span class="token keyword">import</span> sys<span class="token keyword">import</span> reWORD_RE <span class="token operator">=</span> re<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span><span class="token string">r'\w+'</span><span class="token punctuation">)</span>index <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'poetry.txt'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> fp<span class="token punctuation">:</span>    <span class="token keyword">for</span> line_no<span class="token punctuation">,</span> line <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>fp<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> match <span class="token keyword">in</span> WORD_RE<span class="token punctuation">.</span>finditer<span class="token punctuation">(</span>line<span class="token punctuation">)</span><span class="token punctuation">:</span>            word <span class="token operator">=</span> match<span class="token punctuation">.</span>group<span class="token punctuation">(</span><span class="token punctuation">)</span>            column_no <span class="token operator">=</span> match<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span>            location <span class="token operator">=</span> <span class="token punctuation">(</span>line_no<span class="token punctuation">,</span> column_no<span class="token punctuation">)</span>            index<span class="token punctuation">.</span>setdefault<span class="token punctuation">(</span>word<span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>location<span class="token punctuation">)</span><span class="token keyword">for</span> word <span class="token keyword">in</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span>index<span class="token punctuation">,</span> key<span class="token operator">=</span><span class="token builtin">str</span><span class="token punctuation">.</span>upper<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>word<span class="token punctuation">,</span> index<span class="token punctuation">[</span>word<span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>a [(1, 22), (2, 7), (3, 6), (8, 7), (8, 27), (10, 19), (12, 6), (17, 20), (21, 19)]<br>A [(17, 1)]<br>above [(34, 34)]<br>ago [(1, 29), (15, 36)]<br>all [(26, 30), (43, 9)]<br>and [(1, 13), (6, 17), (8, 15), (11, 6), (13, 16), (25, 21), (29, 13), (44, 37)]<br>And [(5, 1), (15, 1), (20, 4), (34, 1), (41, 1), (43, 1)]<br>angels [(24, 5), (34, 17)]<br>Annabel [(4, 19), (11, 13), (18, 17), (29, 28), (37, 21), (40, 21), (42, 21)]<br>as [(26, 27)]<br>away [(20, 17)]<br>be [(6, 21)]<br>beams [(39, 20)]<br>beautiful [(18, 7), (37, 11), (40, 11), (42, 11)]<br>blew [(17, 8)]<br>bore [(20, 8)]<br>bride [(44, 44)]<br>bright [(41, 42)]<br>bringing [(39, 35)]<br>But [(10, 1), (31, 1)]<br>but [(41, 27)]<br>by [(2, 17), (6, 30), (9, 20), (16, 20), (22, 20), (27, 20), (28, 37), (31, 30), (43, 40), (45, 27), (46, 16)]<br>By [(4, 4)]<br>came [(19, 30), (28, 15)]<br>Can [(36, 1)]<br>child [(8, 9), (8, 29)]<br>chilling [(17, 29)]<br>Chilling [(29, 4)]<br>cloud [(17, 22), (28, 31)]<br>Coveted [(13, 4)]<br>darling [(44, 10), (44, 21)]<br>demons [(35, 12)]<br>dissever [(36, 10)]<br>down [(35, 19), (43, 35)]<br>dreams [(39, 47)]<br>envying [(25, 9)]<br>ever [(36, 5)]<br>eyes [(41, 49)]<br>far [(31, 33), (33, 12)]<br>feel [(41, 33)]<br>For [(39, 1)]<br>from [(20, 22), (36, 27)]<br>half [(24, 17)]<br>happy [(24, 25)]<br>Heaven [(12, 40), (24, 34), (34, 27)]<br>her [(13, 12), (19, 9), (20, 13), (21, 9), (25, 17), (45, 7), (46, 7)]<br>highborn [(19, 13)]<br>I [(8, 1), (11, 4), (41, 31), (43, 29)]<br>In [(2, 4), (9, 4), (16, 4), (22, 4), (27, 4), (45, 4), (46, 4)]<br>in [(21, 16), (24, 31), (34, 24)]<br>It [(1, 1)]<br>it [(31, 14)]<br>killing [(29, 17)]<br>kingdom [(2, 9), (9, 12), (16, 12), (22, 12), (27, 12)]<br>kinsmen [(19, 22)]<br>know [(3, 40), (26, 38)]<br>Lee [(4, 27), (11, 21), (18, 25), (29, 36), (37, 29), (40, 29), (42, 29)]<br>lie [(43, 31)]<br>life [(44, 32)]<br>lived [(3, 21), (5, 21)]<br>long [(15, 31)]<br>love [(6, 12), (10, 21), (10, 45), (12, 8), (31, 9), (31, 46)]<br>loved [(6, 24), (10, 8)]<br>maiden [(3, 8), (5, 10)]<br>many [(1, 8), (1, 17), (33, 7)]<br>may [(3, 36)]<br>me [(6, 33), (13, 20), (20, 27), (25, 25), (39, 44)]<br>men [(26, 34)]<br>moon [(39, 9)]<br>more [(10, 35)]<br>my [(11, 10), (29, 25), (36, 19), (44, 7), (44, 18), (44, 29), (44, 41)]<br>My [(18, 4)]<br>name [(4, 11)]<br>neither [(34, 5)]<br>never [(39, 14), (41, 15)]<br>night [(28, 40), (43, 17)]<br>no [(5, 32)]<br>Nor [(35, 4)]<br>not [(24, 13)]<br>of [(4, 16), (12, 37), (17, 17), (28, 24)]<br>Of [(32, 4), (33, 4), (37, 4), (40, 4), (42, 4), (44, 4)]<br>older [(32, 22)]<br>other [(5, 35)]<br>our [(31, 5)]<br>out [(17, 13), (28, 20)]<br>reason [(15, 18), (26, 19)]<br>rise [(41, 21)]<br>sea [(2, 24), (9, 27), (16, 27), (22, 27), (27, 27), (35, 34), (45, 34), (46, 32)]<br>sepulchre [(21, 21), (45, 11)]<br>seraphs [(12, 29)]<br>she [(5, 17), (8, 19)]<br>shut [(21, 4)]<br>side [(43, 47)]<br>So [(19, 1)]<br>so [(24, 22), (43, 5)]<br>soul [(36, 22), (36, 36)]<br>sounding [(46, 23)]<br>stars [(41, 9)]<br>stronger [(31, 21)]<br>Than [(6, 4)]<br>than [(10, 40), (31, 37), (32, 28), (33, 22)]<br>That [(3, 1), (28, 1)]<br>that [(10, 26), (12, 13), (15, 25), (19, 4), (26, 6)]<br>the [(2, 20), (4, 7), (9, 23), (12, 18), (15, 14), (16, 23), (22, 23), (26, 15), (27, 23), (28, 6), (28, 27), (31, 42), (34, 13), (35, 8), (35, 30), (36, 32), (37, 7), (39, 5), (40, 7), (41, 5), (41, 38), (42, 7), (43, 13), (43, 43), (45, 30), (46, 19)]<br>The [(24, 1)]<br>there [(3, 15), (45, 21)]<br>this [(5, 5), (9, 7), (15, 5), (16, 7), (22, 7), (27, 7)]<br>those [(32, 7)]<br>thought [(5, 41)]<br>tide [(43, 23)]<br>to [(6, 9)]<br>To [(21, 1)]<br>tomb [(46, 11)]<br>under [(35, 24)]<br>up [(21, 13)]<br>was [(1, 4), (8, 3), (8, 23), (10, 31), (15, 10), (26, 11), (31, 17)]<br>we [(10, 5), (32, 33), (33, 27)]<br>Went [(25, 4)]<br>were [(32, 17)]<br>who [(32, 13)]<br>whom [(3, 27)]<br>wind [(17, 3), (28, 10)]<br>wingèd [(12, 22)]<br>wiser [(33, 16)]<br>with [(5, 27), (10, 14)]<br>With [(12, 1)]<br>without [(39, 27)]<br>year [(1, 24)]<br>Yes [(26, 1)]<br>you [(3, 32)]</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 映射的弹性键选择、</span><span class="token comment"># 利用defaultdict</span><span class="token keyword">from</span> collections <span class="token keyword">import</span> defaultdict<span class="token keyword">import</span> reWORD_RE <span class="token operator">=</span> re<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span><span class="token string">r'\w+'</span><span class="token punctuation">)</span>index <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">)</span><span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'poetry.txt'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> fp<span class="token punctuation">:</span>    <span class="token keyword">for</span> line_no<span class="token punctuation">,</span> line <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>fp<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> match <span class="token keyword">in</span> WORD_RE<span class="token punctuation">.</span>finditer<span class="token punctuation">(</span>line<span class="token punctuation">)</span><span class="token punctuation">:</span>            word <span class="token operator">=</span> match<span class="token punctuation">.</span>group<span class="token punctuation">(</span><span class="token punctuation">)</span>            column_no <span class="token operator">=</span> match<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span>            location <span class="token operator">=</span> <span class="token punctuation">(</span>line_no<span class="token punctuation">,</span> column_no<span class="token punctuation">)</span>            index<span class="token punctuation">[</span>word<span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>location<span class="token punctuation">)</span><span class="token keyword">for</span> word <span class="token keyword">in</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span>index<span class="token punctuation">,</span> key<span class="token operator">=</span><span class="token builtin">str</span><span class="token punctuation">.</span>upper<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>word<span class="token punctuation">,</span> index<span class="token punctuation">[</span>word<span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>a [(1, 22), (2, 7), (3, 6), (8, 7), (8, 27), (10, 19), (12, 6), (17, 20), (21, 19)]<br>A [(17, 1)]<br>above [(34, 34)]<br>ago [(1, 29), (15, 36)]<br>all [(26, 30), (43, 9)]<br>and [(1, 13), (6, 17), (8, 15), (11, 6), (13, 16), (25, 21), (29, 13), (44, 37)]<br>And [(5, 1), (15, 1), (20, 4), (34, 1), (41, 1), (43, 1)]<br>angels [(24, 5), (34, 17)]<br>Annabel [(4, 19), (11, 13), (18, 17), (29, 28), (37, 21), (40, 21), (42, 21)]<br>as [(26, 27)]<br>away [(20, 17)]<br>be [(6, 21)]<br>beams [(39, 20)]<br>beautiful [(18, 7), (37, 11), (40, 11), (42, 11)]<br>blew [(17, 8)]<br>bore [(20, 8)]<br>bride [(44, 44)]<br>bright [(41, 42)]<br>bringing [(39, 35)]<br>But [(10, 1), (31, 1)]<br>but [(41, 27)]<br>by [(2, 17), (6, 30), (9, 20), (16, 20), (22, 20), (27, 20), (28, 37), (31, 30), (43, 40), (45, 27), (46, 16)]<br>By [(4, 4)]<br>came [(19, 30), (28, 15)]<br>Can [(36, 1)]<br>child [(8, 9), (8, 29)]<br>chilling [(17, 29)]<br>Chilling [(29, 4)]<br>cloud [(17, 22), (28, 31)]<br>Coveted [(13, 4)]<br>darling [(44, 10), (44, 21)]<br>demons [(35, 12)]<br>dissever [(36, 10)]<br>down [(35, 19), (43, 35)]<br>dreams [(39, 47)]<br>envying [(25, 9)]<br>ever [(36, 5)]<br>eyes [(41, 49)]<br>far [(31, 33), (33, 12)]<br>feel [(41, 33)]<br>For [(39, 1)]<br>from [(20, 22), (36, 27)]<br>half [(24, 17)]<br>happy [(24, 25)]<br>Heaven [(12, 40), (24, 34), (34, 27)]<br>her [(13, 12), (19, 9), (20, 13), (21, 9), (25, 17), (45, 7), (46, 7)]<br>highborn [(19, 13)]<br>I [(8, 1), (11, 4), (41, 31), (43, 29)]<br>In [(2, 4), (9, 4), (16, 4), (22, 4), (27, 4), (45, 4), (46, 4)]<br>in [(21, 16), (24, 31), (34, 24)]<br>It [(1, 1)]<br>it [(31, 14)]<br>killing [(29, 17)]<br>kingdom [(2, 9), (9, 12), (16, 12), (22, 12), (27, 12)]<br>kinsmen [(19, 22)]<br>know [(3, 40), (26, 38)]<br>Lee [(4, 27), (11, 21), (18, 25), (29, 36), (37, 29), (40, 29), (42, 29)]<br>lie [(43, 31)]<br>life [(44, 32)]<br>lived [(3, 21), (5, 21)]<br>long [(15, 31)]<br>love [(6, 12), (10, 21), (10, 45), (12, 8), (31, 9), (31, 46)]<br>loved [(6, 24), (10, 8)]<br>maiden [(3, 8), (5, 10)]<br>many [(1, 8), (1, 17), (33, 7)]<br>may [(3, 36)]<br>me [(6, 33), (13, 20), (20, 27), (25, 25), (39, 44)]<br>men [(26, 34)]<br>moon [(39, 9)]<br>more [(10, 35)]<br>my [(11, 10), (29, 25), (36, 19), (44, 7), (44, 18), (44, 29), (44, 41)]<br>My [(18, 4)]<br>name [(4, 11)]<br>neither [(34, 5)]<br>never [(39, 14), (41, 15)]<br>night [(28, 40), (43, 17)]<br>no [(5, 32)]<br>Nor [(35, 4)]<br>not [(24, 13)]<br>of [(4, 16), (12, 37), (17, 17), (28, 24)]<br>Of [(32, 4), (33, 4), (37, 4), (40, 4), (42, 4), (44, 4)]<br>older [(32, 22)]<br>other [(5, 35)]<br>our [(31, 5)]<br>out [(17, 13), (28, 20)]<br>reason [(15, 18), (26, 19)]<br>rise [(41, 21)]<br>sea [(2, 24), (9, 27), (16, 27), (22, 27), (27, 27), (35, 34), (45, 34), (46, 32)]<br>sepulchre [(21, 21), (45, 11)]<br>seraphs [(12, 29)]<br>she [(5, 17), (8, 19)]<br>shut [(21, 4)]<br>side [(43, 47)]<br>So [(19, 1)]<br>so [(24, 22), (43, 5)]<br>soul [(36, 22), (36, 36)]<br>sounding [(46, 23)]<br>stars [(41, 9)]<br>stronger [(31, 21)]<br>Than [(6, 4)]<br>than [(10, 40), (31, 37), (32, 28), (33, 22)]<br>That [(3, 1), (28, 1)]<br>that [(10, 26), (12, 13), (15, 25), (19, 4), (26, 6)]<br>the [(2, 20), (4, 7), (9, 23), (12, 18), (15, 14), (16, 23), (22, 23), (26, 15), (27, 23), (28, 6), (28, 27), (31, 42), (34, 13), (35, 8), (35, 30), (36, 32), (37, 7), (39, 5), (40, 7), (41, 5), (41, 38), (42, 7), (43, 13), (43, 43), (45, 30), (46, 19)]<br>The [(24, 1)]<br>there [(3, 15), (45, 21)]<br>this [(5, 5), (9, 7), (15, 5), (16, 7), (22, 7), (27, 7)]<br>those [(32, 7)]<br>thought [(5, 41)]<br>tide [(43, 23)]<br>to [(6, 9)]<br>To [(21, 1)]<br>tomb [(46, 11)]<br>under [(35, 24)]<br>up [(21, 13)]<br>was [(1, 4), (8, 3), (8, 23), (10, 31), (15, 10), (26, 11), (31, 17)]<br>we [(10, 5), (32, 33), (33, 27)]<br>Went [(25, 4)]<br>were [(32, 17)]<br>who [(32, 13)]<br>whom [(3, 27)]<br>wind [(17, 3), (28, 10)]<br>wingèd [(12, 22)]<br>wiser [(33, 16)]<br>with [(5, 27), (10, 14)]<br>With [(12, 1)]<br>without [(39, 27)]<br>year [(1, 24)]<br>Yes [(26, 1)]<br>you [(3, 32)]</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 用__missing__处理</span><span class="token keyword">import</span> collections<span class="token keyword">class</span> <span class="token class-name">StrKeyDict</span><span class="token punctuation">(</span>collections<span class="token punctuation">.</span>UserDict<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__missing__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> key<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>key<span class="token punctuation">,</span> <span class="token builtin">str</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">raise</span> KeyError<span class="token punctuation">(</span>key<span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>key<span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">__contains__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> key<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token builtin">str</span><span class="token punctuation">(</span>key<span class="token punctuation">)</span> <span class="token keyword">in</span> self<span class="token punctuation">.</span>data    <span class="token keyword">def</span> <span class="token function">__setitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> key<span class="token punctuation">,</span> item<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>key<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> item<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 字典的变种</span><span class="token comment"># collections.OrderedDict，键值会排序，可以用popitem</span><span class="token comment"># collections.ChainMap，可以容纳多个dict，一并查询</span><span class="token comment"># collections.Counter,计数器</span><span class="token comment"># collection.UserDict 纯python实现的dict，更方便我们做dict扩展</span><span class="token keyword">from</span> collections <span class="token keyword">import</span> Counterct <span class="token operator">=</span> Counter<span class="token punctuation">(</span><span class="token string">"sdsadcsadsadaaasdsac"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>ct<span class="token punctuation">)</span>ct<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token string">'wewqewq'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>ct<span class="token punctuation">)</span>ct<span class="token punctuation">.</span>most_common<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment">#排序</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>Counter({‘a’: 7, ‘s’: 6, ‘d’: 5, ‘c’: 2})<br>Counter({‘a’: 7, ‘s’: 6, ‘d’: 5, ‘w’: 3, ‘c’: 2, ‘e’: 2, ‘q’: 2})</p><p>[(‘a’, 7), (‘s’, 6), (‘d’, 5), (‘w’, 3), (‘c’, 2), (‘e’, 2), (‘q’, 2)]</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 不可变映射类型</span><span class="token keyword">from</span> types <span class="token keyword">import</span> MappingProxyTyped <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token number">1</span><span class="token punctuation">:</span> <span class="token string">'A'</span><span class="token punctuation">}</span>d_proxy <span class="token operator">=</span> MappingProxyType<span class="token punctuation">(</span>d<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>d_proxy<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>d_proxy<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>A</p><hr><p>KeyError Traceback (most recent call last)</p><p><ipython-input-12-d4ae1d057cd0> in <module>       4 d_proxy = MappingProxyType(d)<br>      5 print(d_proxy[1])<br>——&gt; 6 print(d_proxy[2])</module></ipython-input-12-d4ae1d057cd0></p><p>KeyError: 2</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python">d<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'B'</span><span class="token keyword">print</span><span class="token punctuation">(</span>d_proxy<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><blockquote><p>B</p></blockquote><h2 id="3-2-set"><a href="#3-2-set" class="headerlink" title="3.2 set"></a>3.2 set</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#集合的本质是许多唯一对象的聚集，可以用于去重</span>l <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token string">'c'</span><span class="token punctuation">,</span> <span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token string">'b'</span><span class="token punctuation">]</span><span class="token builtin">set</span><span class="token punctuation">(</span>l<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>&gt;</p><blockquote><p>{‘a’, ‘b’, ‘c’}</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#集合的运算符</span>a <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">}</span>b <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">}</span><span class="token keyword">print</span><span class="token punctuation">(</span>a <span class="token operator">|</span> b<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a <span class="token operator">&amp;</span> b<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a <span class="token operator">-</span> b<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a <span class="token operator">^</span> b<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>{1, 2, 3, 4, 5, 6}<br>{3, 4}<br>{1, 2}<br>{1, 2, 5, 6}</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> unicodedata <span class="token keyword">import</span> name<span class="token punctuation">{</span><span class="token builtin">chr</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token number">236</span><span class="token punctuation">)</span> <span class="token keyword">if</span> <span class="token string">'SIGN'</span> <span class="token keyword">in</span> name<span class="token punctuation">(</span><span class="token builtin">chr</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><blockquote><p>{‘#’,<br>‘$’,<br>‘%’,<br>‘+’,<br>‘&lt;’,<br>‘=’,<br>‘&gt;’,<br>‘¢’,<br>‘£’,<br>‘¤’,<br>‘¥’,<br>‘§’,<br>‘©’,<br>‘¬’,<br>‘®’,<br>‘°’,<br>‘±’,<br>‘µ’,<br>‘¶’,<br>‘×’}</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> FluentPython </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Chapter_02_An_Array_of_Sequences</title>
      <link href="2020/12/02/python/fluentpython/chapter-02-an-array-of-sequences/"/>
      <url>2020/12/02/python/fluentpython/chapter-02-an-array-of-sequences/</url>
      
        <content type="html"><![CDATA[<h1 id="Chapter-02-An-Array-of-Sequences"><a href="#Chapter-02-An-Array-of-Sequences" class="headerlink" title="Chapter_02_An_Array_of_Sequences"></a>Chapter_02_An_Array_of_Sequences</h1><h2 id="2-1-tuple"><a href="#2-1-tuple" class="headerlink" title="2.1 tuple"></a>2.1 tuple</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 列表推导式</span>colors <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'black'</span><span class="token punctuation">,</span> <span class="token string">'white'</span><span class="token punctuation">]</span>sizes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'S'</span><span class="token punctuation">,</span> <span class="token string">'M'</span><span class="token punctuation">,</span> <span class="token string">'L'</span><span class="token punctuation">]</span>tshirts<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span>color<span class="token punctuation">,</span> size<span class="token punctuation">)</span> <span class="token keyword">for</span> color <span class="token keyword">in</span> colors <span class="token keyword">for</span> size <span class="token keyword">in</span> sizes<span class="token punctuation">]</span>tshirts<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>[(‘black’, ‘S’),<br> (‘black’, ‘M’),<br> (‘black’, ‘L’),<br> (‘white’, ‘S’),<br> (‘white’, ‘M’),<br> (‘white’, ‘L’)]</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 列表生成器 就是吧[]变成()可以调用next生成</span>tshirts <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">{</span>color<span class="token punctuation">}</span></span><span class="token string"> </span><span class="token interpolation"><span class="token punctuation">{</span>size<span class="token punctuation">}</span></span><span class="token string">"</span></span> <span class="token keyword">for</span> color <span class="token keyword">in</span> colors <span class="token keyword">for</span> size <span class="token keyword">in</span> sizes<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>tshirts<span class="token punctuation">)</span><span class="token keyword">for</span> tshirt <span class="token keyword">in</span> tshirts<span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>tshirt<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>&lt; generator object &lt; genexpr &gt; at 0x7fb438de0228 &gt; black S<br>black M<br>black L<br>white S<br>white M<br>white L</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 元组的用法</span>lax_coordinates <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">33.9425</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">118.408056</span><span class="token punctuation">)</span>city<span class="token punctuation">,</span> year<span class="token punctuation">,</span> pop<span class="token punctuation">,</span> chg<span class="token punctuation">,</span> area <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">'Tokyo'</span><span class="token punctuation">,</span> <span class="token number">2003</span><span class="token punctuation">,</span> <span class="token number">32450</span><span class="token punctuation">,</span> <span class="token number">0.66</span><span class="token punctuation">,</span> <span class="token number">8014</span><span class="token punctuation">)</span>traveler_ids <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'USA'</span><span class="token punctuation">,</span> <span class="token string">'31195855'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'BRA'</span><span class="token punctuation">,</span> <span class="token string">'CE342567'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token string">'ESP'</span><span class="token punctuation">,</span> <span class="token string">'XDA205856'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token keyword">for</span> passport <span class="token keyword">in</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span>traveler_ids<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'%s/%s'</span> <span class="token operator">%</span> passport<span class="token punctuation">)</span><span class="token keyword">for</span> country<span class="token punctuation">,</span> _ <span class="token keyword">in</span> traveler_ids<span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>country<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>BRA/CE342567<br>ESP/XDA205856<br>USA/31195855<br>USA<br>BRA<br>ESP</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 元组拆包</span>lax_coordinates <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">33.9425</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">118.408056</span><span class="token punctuation">)</span>latitude<span class="token punctuation">,</span> longitude <span class="token operator">=</span> lax_coordinates <span class="token comment"># tuple unpacking</span><span class="token keyword">print</span><span class="token punctuation">(</span>latitude<span class="token punctuation">,</span> longitude<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>33.9425 -118.408056</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python">t <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">)</span>quotient<span class="token punctuation">,</span> remainder <span class="token operator">=</span> <span class="token builtin">divmod</span><span class="token punctuation">(</span><span class="token operator">*</span>t<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>quotient<span class="token punctuation">,</span> remainder<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><blockquote><p>2 4</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> os_<span class="token punctuation">,</span> filename <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'/home/ctios/.ssh/idrsa.pub'</span><span class="token punctuation">)</span>filename<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><blockquote><p>‘idrsa.pub’</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python">a<span class="token punctuation">,</span> <span class="token operator">*</span>body<span class="token punctuation">,</span> c<span class="token punctuation">,</span> d <span class="token operator">=</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span>a<span class="token punctuation">,</span> body<span class="token punctuation">,</span> c<span class="token punctuation">,</span> d<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><blockquote><p>(0, [1, 2, 3, 4, 5, 6, 7], 8, 9)</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python">metro_areas <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token punctuation">(</span><span class="token string">'Tokyo'</span><span class="token punctuation">,</span><span class="token string">'JP'</span><span class="token punctuation">,</span><span class="token number">36.933</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">35.689722</span><span class="token punctuation">,</span><span class="token number">139.691667</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token string">'Delhi NCR'</span><span class="token punctuation">,</span> <span class="token string">'IN'</span><span class="token punctuation">,</span> <span class="token number">21.935</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">28.613889</span><span class="token punctuation">,</span> <span class="token number">77.208889</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>     <span class="token punctuation">(</span><span class="token string">'Mexico City'</span><span class="token punctuation">,</span> <span class="token string">'MX'</span><span class="token punctuation">,</span> <span class="token number">20.142</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">19.433333</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">99.133333</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token string">'New York-Newark'</span><span class="token punctuation">,</span> <span class="token string">'US'</span><span class="token punctuation">,</span> <span class="token number">20.104</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">40.808611</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">74.020386</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token string">'Sao Paulo'</span><span class="token punctuation">,</span> <span class="token string">'BR'</span><span class="token punctuation">,</span> <span class="token number">19.649</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">23.547778</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">46.635833</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'{:15} | {:^9} | {:^9}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token string">''</span><span class="token punctuation">,</span> <span class="token string">'lat.'</span><span class="token punctuation">,</span> <span class="token string">'long.'</span><span class="token punctuation">)</span><span class="token punctuation">)</span> fmt <span class="token operator">=</span> <span class="token string">'{:15} | {:9.4f} | {:9.4f}'</span><span class="token keyword">for</span> name<span class="token punctuation">,</span> cc<span class="token punctuation">,</span> pop<span class="token punctuation">,</span> <span class="token punctuation">(</span>latitude<span class="token punctuation">,</span> longitude<span class="token punctuation">)</span> <span class="token keyword">in</span> metro_areas<span class="token punctuation">:</span>     <span class="token keyword">if</span> longitude <span class="token operator">&lt;=</span> <span class="token number">0</span><span class="token punctuation">:</span>         <span class="token keyword">print</span><span class="token punctuation">(</span>fmt<span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span> latitude<span class="token punctuation">,</span> longitude<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="table-container"><table><thead><tr><th></th><th>lat.</th><th>long.</th></tr></thead><tbody><tr><td>Mexico City</td><td>19.4333</td><td>-99.1333</td></tr><tr><td>New York-Newark</td><td>40.8086</td><td>-74.0204</td></tr><tr><td>Sao Paulo</td><td>-23.5478</td><td>-46.6358</td></tr></tbody></table></div><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 具名元组 namedtuple</span><span class="token keyword">from</span> collections <span class="token keyword">import</span> namedtupleCity <span class="token operator">=</span> namedtuple<span class="token punctuation">(</span><span class="token string">'City'</span><span class="token punctuation">,</span> <span class="token string">'name country population coordinates'</span><span class="token punctuation">)</span>tokyo <span class="token operator">=</span> City<span class="token punctuation">(</span><span class="token string">'Tokyo'</span><span class="token punctuation">,</span> <span class="token string">'JP'</span><span class="token punctuation">,</span> <span class="token number">36.933</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">35.689722</span><span class="token punctuation">,</span> <span class="token number">139.691667</span><span class="token punctuation">)</span><span class="token punctuation">)</span>City<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">'Tokyo'</span><span class="token punctuation">,</span> country<span class="token operator">=</span><span class="token string">'JP'</span><span class="token punctuation">,</span> population<span class="token operator">=</span><span class="token number">36.933</span><span class="token punctuation">,</span> coordinates<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">35.689722</span><span class="token punctuation">,</span> <span class="token number">139.691667</span><span class="token punctuation">)</span><span class="token punctuation">)</span>tokyo<span class="token punctuation">,</span> tokyo<span class="token punctuation">.</span>population<span class="token punctuation">,</span> tokyo<span class="token punctuation">.</span>coordinates<span class="token punctuation">,</span> tokyo<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>(City(name=’Tokyo’, country=’JP’, population=36.933, coordinates=(35.689722, 139.691667)),<br> 36.933,<br> (35.689722, 139.691667),<br> ‘JP’)</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 具名元组的属性和方法</span>City<span class="token punctuation">.</span>_fields<span class="token punctuation">(</span><span class="token string">'name'</span><span class="token punctuation">,</span> <span class="token string">'country'</span><span class="token punctuation">,</span> <span class="token string">'population'</span><span class="token punctuation">,</span> <span class="token string">'coordinates'</span><span class="token punctuation">)</span>LatLong <span class="token operator">=</span> namedtuple<span class="token punctuation">(</span><span class="token string">'LatLong'</span><span class="token punctuation">,</span> <span class="token string">'lat long'</span><span class="token punctuation">)</span>delhi_data <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">'Delhi NCR'</span><span class="token punctuation">,</span> <span class="token string">'IN'</span><span class="token punctuation">,</span> <span class="token number">21.935</span><span class="token punctuation">,</span> LatLong<span class="token punctuation">(</span><span class="token number">28.613889</span><span class="token punctuation">,</span> <span class="token number">77.208889</span><span class="token punctuation">)</span><span class="token punctuation">)</span>delhi <span class="token operator">=</span> City<span class="token punctuation">.</span>_make<span class="token punctuation">(</span>delhi_data<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>delhi<span class="token punctuation">)</span><span class="token keyword">for</span> key<span class="token punctuation">,</span> value <span class="token keyword">in</span> delhi<span class="token punctuation">.</span>_asdict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>key <span class="token operator">+</span> <span class="token string">':'</span><span class="token punctuation">,</span> value<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>City(name=’Delhi NCR’, country=’IN’, population=21.935, coordinates=LatLong(lat=28.613889, long=77.208889))<br>name: Delhi NCR<br>country: IN<br>population: 21.935<br>coordinates: LatLong(lat=28.613889, long=77.208889)</p></blockquote><h2 id="2-2-slice"><a href="#2-2-slice" class="headerlink" title="2.2 slice"></a>2.2 slice</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 切片</span>s <span class="token operator">=</span> <span class="token string">'bicycle'</span>s<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> s<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><blockquote><p>(‘bye’, ‘elcycib’)</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 切片slice对象，传入seq.__getitem__()</span>invoice <span class="token operator">=</span> <span class="token triple-quoted-string string">"""0.....6.................................40........52...55........1909 Pimoroni PiBrella                      $17.50    3    $52.501489 6mm Tactile Switch x20                 $4.95     2     $9.901510 Panavise Jr. - PV-201                  $28.00    1    $28.001601 PiTFT Mini Kit 320x240                 $34.95    1    $34.95"""</span>SKU <span class="token operator">=</span> <span class="token builtin">slice</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span>DESCRIPTION <span class="token operator">=</span> <span class="token builtin">slice</span><span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">40</span><span class="token punctuation">)</span>UNIT_PRICE <span class="token operator">=</span> <span class="token builtin">slice</span><span class="token punctuation">(</span><span class="token number">40</span><span class="token punctuation">,</span> <span class="token number">52</span><span class="token punctuation">)</span>QUANTITY <span class="token operator">=</span> <span class="token builtin">slice</span><span class="token punctuation">(</span><span class="token number">52</span><span class="token punctuation">,</span> <span class="token number">55</span><span class="token punctuation">)</span>ITEM_TOTAL <span class="token operator">=</span> <span class="token builtin">slice</span><span class="token punctuation">(</span><span class="token number">55</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>line_items <span class="token operator">=</span> invoice<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token keyword">for</span> item <span class="token keyword">in</span> line_items<span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>item<span class="token punctuation">[</span>UNIT_PRICE<span class="token punctuation">]</span><span class="token punctuation">,</span> item<span class="token punctuation">[</span>DESCRIPTION<span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>$17.50 imoroni PiBrella<br>$4.95 6mm Tactile Switch x20<br>$28.00 anavise Jr. - PV-201<br>$34.95 iTFT Mini Kit 320x240             </p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 给切片赋值</span>l <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>l<span class="token punctuation">)</span>l<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span>l<span class="token punctuation">)</span><span class="token keyword">del</span> l<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">:</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span>l<span class="token punctuation">)</span>l<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">11</span><span class="token punctuation">,</span> <span class="token number">22</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span>l<span class="token punctuation">)</span>l<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">100</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span>l<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]<br>[0, 1, 20, 30, 5, 6, 7, 8, 9]<br>[0, 1, 20, 30, 5, 8, 9]<br>[0, 1, 20, 11, 5, 22, 9]<br>[0, 1, 100, 22, 9]</p></blockquote><h2 id="2-3"><a href="#2-3" class="headerlink" title="2.3 +,,+=,="></a>2.3 +,<em>,+=,</em>=</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 对序列使用+，*</span><span class="token comment"># 当a*n 这个语句中，当序列a里面的元素是对其他可变对象的引用，乘法只是赋值多个引用，其中的这些引用都是指向同一个列表，例如</span>board <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">'_'</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">3</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span>board<span class="token punctuation">)</span>board<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'X'</span><span class="token keyword">print</span><span class="token punctuation">(</span>board<span class="token punctuation">)</span>weird_board <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">'_'</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">3</span>weird_board<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'0'</span><span class="token keyword">print</span><span class="token punctuation">(</span>weird_board<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>[[‘_’, ‘_’, ‘_’], [‘_’, ‘_’, ‘_’], [‘_’, ‘_’, ‘_’]]<br>[[‘_’, ‘_’, ‘_’], [‘_’, ‘_’, ‘X’], [‘_’, ‘_’, ‘_’]]<br>[[‘_’, ‘_’, ‘0’], [‘_’, ‘_’, ‘0’], [‘_’, ‘_’, ‘0’]]</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 增量赋值 +=,*=</span><span class="token comment"># 如果对象实现了__iadd__,__imul__的话，就是在原基础上增加</span><span class="token comment"># 否则就是调用__add__，__mul__，得到一个新对象，例如</span>l <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'l={}, id={}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>l<span class="token punctuation">,</span> <span class="token builtin">id</span><span class="token punctuation">(</span>l<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>l <span class="token operator">*=</span> <span class="token number">2</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'l={}, id={}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>l<span class="token punctuation">,</span> <span class="token builtin">id</span><span class="token punctuation">(</span>l<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>t<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'t={}, id={}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>t<span class="token punctuation">,</span><span class="token builtin">id</span><span class="token punctuation">(</span>t<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>t<span class="token operator">*=</span><span class="token number">2</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'t={}, id={}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>t<span class="token punctuation">,</span><span class="token builtin">id</span><span class="token punctuation">(</span>t<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># l的id没变，t的id变了</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>l=[1, 2, 3], id=140216592790216<br>l=[1, 2, 3, 1, 2, 3], id=140216592790216<br>t=(1, 2, 3), id=140216592745168<br>t=(1, 2, 3, 1, 2, 3), id=140216592237672</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 关于+=的谜题</span>t <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">30</span><span class="token punctuation">,</span> <span class="token number">40</span><span class="token punctuation">]</span><span class="token punctuation">)</span>t<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token punctuation">[</span><span class="token number">50</span><span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><blockquote><hr><p>TypeError raceback (most recent call last)</p><p>&lt; ipython-input-6-b2fc2439c603 &gt; in &lt; module &gt;   1 # 关于+=的谜题<br>  2 t = (1, 2, [30, 40])<br>——&gt; 3 t[2] += [50, 60]</p><p>TypeError: ‘tuple’ object does not support item assignment</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><blockquote><p>(1, 2, [30, 40, 50, 60])</p></blockquote><h2 id="2-4-sort-and-bisect"><a href="#2-4-sort-and-bisect" class="headerlink" title="2.4 sort and bisect"></a>2.4 sort and bisect</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># list.sort()和sorted()</span>fruits <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'grape'</span><span class="token punctuation">,</span> <span class="token string">'raspberry'</span><span class="token punctuation">,</span> <span class="token string">'apple'</span><span class="token punctuation">,</span> <span class="token string">'banana'</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">sorted</span><span class="token punctuation">(</span>fruits<span class="token punctuation">)</span><span class="token punctuation">,</span> fruits<span class="token punctuation">)</span>fruits<span class="token punctuation">.</span>sort<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>fruits<span class="token punctuation">)</span>fruits<span class="token punctuation">.</span>sort<span class="token punctuation">(</span>key<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>fruits<span class="token punctuation">)</span><span class="token comment"># python中的sort是稳定的，大小一致时，位置不变</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># bisect_left, bisect, bisect_right是查找位置，不插入</span><span class="token keyword">from</span> bisect <span class="token keyword">import</span> bisect_left<span class="token punctuation">,</span> bisect<span class="token punctuation">,</span> bisect_righta <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span>index <span class="token operator">=</span> bisect_left<span class="token punctuation">(</span>a<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> index<span class="token punctuation">)</span>a <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span>index <span class="token operator">=</span> bisect<span class="token punctuation">(</span>a<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> index<span class="token punctuation">)</span>a <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span>index <span class="token operator">=</span> bisect_right<span class="token punctuation">(</span>a<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> index<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>[1, 2, 3, 4] 1<br>[1, 2, 3, 4] 2<br>[1, 2, 3, 4] 2</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># bisect 可以作为索引查询表格</span><span class="token keyword">import</span> bisect<span class="token keyword">def</span> <span class="token function">grade</span><span class="token punctuation">(</span>score<span class="token punctuation">,</span> breakpoints<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">60</span><span class="token punctuation">,</span> <span class="token number">70</span><span class="token punctuation">,</span> <span class="token number">80</span><span class="token punctuation">,</span> <span class="token number">90</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grades<span class="token operator">=</span><span class="token string">'FDCBA'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    i <span class="token operator">=</span> bisect<span class="token punctuation">.</span>bisect<span class="token punctuation">(</span>breakpoints<span class="token punctuation">,</span> score<span class="token punctuation">)</span>    <span class="token keyword">return</span> grades<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>grade<span class="token punctuation">(</span>score<span class="token punctuation">)</span> <span class="token keyword">for</span> score <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token number">33</span><span class="token punctuation">,</span> <span class="token number">99</span><span class="token punctuation">,</span> <span class="token number">77</span><span class="token punctuation">,</span> <span class="token number">70</span><span class="token punctuation">,</span> <span class="token number">89</span><span class="token punctuation">,</span> <span class="token number">90</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>[‘F’, ‘A’, ‘C’, ‘C’, ‘B’, ‘A’, ‘A’]</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 用bisect.insort插入新元素</span><span class="token keyword">import</span> bisect<span class="token keyword">import</span> randomSIZE <span class="token operator">=</span> <span class="token number">7</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">1997</span><span class="token punctuation">)</span>my_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>SIZE<span class="token punctuation">)</span><span class="token punctuation">:</span>    new_item <span class="token operator">=</span> random<span class="token punctuation">.</span>randrange<span class="token punctuation">(</span>SIZE <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">)</span>    bisect<span class="token punctuation">.</span>insort<span class="token punctuation">(</span>my_list<span class="token punctuation">,</span> new_item<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'%2d -&gt;'</span> <span class="token operator">%</span> new_item<span class="token punctuation">,</span> my_list<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>12 -&gt; [12]<br> 3 -&gt; [3, 12]<br> 1 -&gt; [1, 3, 12]<br> 3 -&gt; [1, 3, 3, 12]<br> 2 -&gt; [1, 2, 3, 3, 12]<br>10 -&gt; [1, 2, 3, 3, 10, 12]<br> 3 -&gt; [1, 2, 3, 3, 3, 10, 12]</p></blockquote><h2 id="2-5-array-and-memoryview"><a href="#2-5-array-and-memoryview" class="headerlink" title="2.5 array and memoryview"></a>2.5 array and memoryview</h2><p>此模块定义了一种对象类型，可以紧凑地表示基本类型值的数组：字符、整数、浮点数等。 数组属于序列类型，其行为与列表非常相似，不同之处在于其中存储的对象类型是受限的。 类型在对象创建时使用单个字符的 <em>类型码</em> 来指定。 已定义的类型码如下：</p><div class="table-container"><table><thead><tr><th style="text-align:left">类型码</th><th style="text-align:left">C 类型</th><th style="text-align:left">Python 类型</th><th style="text-align:left">以字节表示的最小尺寸</th><th style="text-align:left">注释</th></tr></thead><tbody><tr><td style="text-align:left"><code>'b'</code></td><td style="text-align:left">signed char</td><td style="text-align:left">int</td><td style="text-align:left">1</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><code>'B'</code></td><td style="text-align:left">unsigned char</td><td style="text-align:left">int</td><td style="text-align:left">1</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><code>'u'</code></td><td style="text-align:left">Py_UNICODE</td><td style="text-align:left">Unicode 字符</td><td style="text-align:left">2</td><td style="text-align:left">(1)</td></tr><tr><td style="text-align:left"><code>'h'</code></td><td style="text-align:left">signed short</td><td style="text-align:left">int</td><td style="text-align:left">2</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><code>'H'</code></td><td style="text-align:left">unsigned short</td><td style="text-align:left">int</td><td style="text-align:left">2</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><code>'i'</code></td><td style="text-align:left">signed int</td><td style="text-align:left">int</td><td style="text-align:left">2</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><code>'I'</code></td><td style="text-align:left">unsigned int</td><td style="text-align:left">int</td><td style="text-align:left">2</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><code>'l'</code></td><td style="text-align:left">signed long</td><td style="text-align:left">int</td><td style="text-align:left">4</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><code>'L'</code></td><td style="text-align:left">unsigned long</td><td style="text-align:left">int</td><td style="text-align:left">4</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><code>'q'</code></td><td style="text-align:left">signed long long</td><td style="text-align:left">int</td><td style="text-align:left">8</td><td style="text-align:left">(2)</td></tr><tr><td style="text-align:left"><code>'Q'</code></td><td style="text-align:left">unsigned long long</td><td style="text-align:left">int</td><td style="text-align:left">8</td><td style="text-align:left">(2)</td></tr><tr><td style="text-align:left"><code>'f'</code></td><td style="text-align:left">float</td><td style="text-align:left">float</td><td style="text-align:left">4</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><code>'d'</code></td><td style="text-align:left">double</td><td style="text-align:left">float</td><td style="text-align:left">8</td></tr></tbody></table></div><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> array <span class="token keyword">import</span> array<span class="token keyword">from</span> random <span class="token keyword">import</span> randomfloats<span class="token operator">=</span>array<span class="token punctuation">(</span><span class="token string">'d'</span><span class="token punctuation">,</span><span class="token punctuation">(</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token operator">**</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>floats<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>0.4585058847207508</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python">fp<span class="token operator">=</span><span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'floats.bin'</span><span class="token punctuation">,</span><span class="token string">'wb+'</span><span class="token punctuation">)</span>floats<span class="token punctuation">.</span>tofile<span class="token punctuation">(</span>fp<span class="token punctuation">)</span>fp<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>floats2<span class="token operator">=</span>array<span class="token punctuation">(</span><span class="token string">'d'</span><span class="token punctuation">)</span>fp<span class="token operator">=</span><span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'floats.bin'</span><span class="token punctuation">,</span><span class="token string">'rb'</span><span class="token punctuation">)</span>floats2<span class="token punctuation">.</span>fromfile<span class="token punctuation">(</span>fp<span class="token punctuation">,</span><span class="token number">10</span><span class="token operator">**</span><span class="token number">7</span><span class="token punctuation">)</span>fp<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>floats2<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>0.4585058847207508</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python">floats<span class="token operator">=</span>array<span class="token punctuation">(</span>floats<span class="token punctuation">.</span>typecode<span class="token punctuation">,</span><span class="token builtin">sorted</span><span class="token punctuation">(</span>floats<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>floats<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><blockquote><p>array(‘d’, [3.67431575165611e-08, 3.475699940080901e-07, 3.7822653908836656e-07, 4.061706841973489e-07, 6.074053917615174e-07, 6.484837163922563e-07, 6.683984088429185e-07, 7.039884853954348e-07, 7.172446871983595e-07, 7.357147590036917e-07])</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># memeryview</span><span class="token keyword">import</span> this<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><blockquote><p>The Zen of Python, by Tim Peters</p><p>Beautiful is better than ugly.<br>Explicit is better than implicit.<br>Simple is better than complex.<br>Complex is better than complicated.<br>Flat is better than nested.<br>Sparse is better than dense.<br>Readability counts.<br>Special cases aren’t special enough to break the rules.<br>Although practicality beats purity.<br>Errors should never pass silently.<br>Unless explicitly silenced.<br>In the face of ambiguity, refuse the temptation to guess.<br>There should be one— and preferably only one —obvious way to do it.<br>Although that way may not be obvious at first unless you’re Dutch.<br>Now is better than never.<br>Although never is often better than <em>right</em> now.<br>If the implementation is hard to explain, it’s a bad idea.<br>If the implementation is easy to explain, it may be a good idea.<br>Namespaces are one honking great idea — let’s do more of those!</p></blockquote><h2 id="2-6-queue"><a href="#2-6-queue" class="headerlink" title="2.6 queue"></a>2.6 queue</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># deque 双向队列</span><span class="token keyword">from</span> collections <span class="token keyword">import</span> dequedq <span class="token operator">=</span> deque<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span> maxlen<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>dq<span class="token punctuation">)</span>dq<span class="token punctuation">.</span>rotate<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>dq<span class="token punctuation">)</span>dq<span class="token punctuation">.</span>rotate<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>dq<span class="token punctuation">)</span>dq<span class="token punctuation">.</span>appendleft<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>dq<span class="token punctuation">)</span>dq<span class="token punctuation">.</span>extend<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">,</span> <span class="token number">40</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>dq<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>deque([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], maxlen=10)<br>deque([7, 8, 9, 0, 1, 2, 3, 4, 5, 6], maxlen=10)<br>deque([1, 2, 3, 4, 5, 6, 7, 8, 9, 0], maxlen=10)<br>deque([-1, 1, 2, 3, 4, 5, 6, 7, 8, 9], maxlen=10)<br>deque([3, 4, 5, 6, 7, 8, 9, 20, 30, 40], maxlen=10)</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 优先队列</span><span class="token keyword">import</span> heapq<span class="token keyword">import</span> randoma <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>heapq<span class="token punctuation">.</span>heapify<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    heapq<span class="token punctuation">.</span>heappush<span class="token punctuation">(</span>a<span class="token punctuation">,</span> random<span class="token punctuation">.</span>randrange<span class="token punctuation">(</span><span class="token number">10</span><span class="token operator">**</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">while</span> <span class="token builtin">len</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>    x <span class="token operator">=</span> heapq<span class="token punctuation">.</span>heappop<span class="token punctuation">(</span>a<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>[403, 433, 459, 476, 541, 985, 821, 855, 822, 855]<br>403<br>433<br>459<br>476<br>541<br>821<br>822<br>855<br>855<br>985</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> FluentPython </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Chapter_01_The_Python_Data_Model</title>
      <link href="2020/12/01/python/fluentpython/chapter-01-the-python-data-model/"/>
      <url>2020/12/01/python/fluentpython/chapter-01-the-python-data-model/</url>
      
        <content type="html"><![CDATA[<h1 id="Chapter-01-The-Python-Data-Model"><a href="#Chapter-01-The-Python-Data-Model" class="headerlink" title="Chapter_01_The_Python_Data_Model"></a>Chapter_01_The_Python_Data_Model</h1><h2 id="1-1-Playing-Cards"><a href="#1-1-Playing-Cards" class="headerlink" title="1.1 Playing Cards"></a>1.1 Playing Cards</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> collectionsCard <span class="token operator">=</span> collections<span class="token punctuation">.</span>namedtuple<span class="token punctuation">(</span><span class="token string">'Card'</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">'rank'</span><span class="token punctuation">,</span> <span class="token string">'suit'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">FrenchDesk</span><span class="token punctuation">:</span>    ranks <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span> <span class="token keyword">for</span> n <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token operator">+</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token string">'JQKA'</span><span class="token punctuation">)</span>    suits <span class="token operator">=</span> <span class="token string">'spades diamonds clubs hearts'</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>_cards <span class="token operator">=</span> <span class="token punctuation">[</span>Card<span class="token punctuation">(</span>rank<span class="token punctuation">,</span> suit<span class="token punctuation">)</span>                       <span class="token keyword">for</span> suit <span class="token keyword">in</span> self<span class="token punctuation">.</span>suits <span class="token keyword">for</span> rank <span class="token keyword">in</span> self<span class="token punctuation">.</span>ranks<span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>_cards<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> position<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>_cards<span class="token punctuation">[</span>position<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#访问纸牌对象</span>beer_card <span class="token operator">=</span> Card<span class="token punctuation">(</span><span class="token string">'7'</span><span class="token punctuation">,</span> <span class="token string">'diamonds'</span><span class="token punctuation">)</span>beer_card<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><blockquote><p>   Card(rank=’7’, suit=’diamonds’)</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 定义了len之后可以查看len</span>desk <span class="token operator">=</span> FrenchDesk<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token builtin">len</span><span class="token punctuation">(</span>desk<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><blockquote><p>   52</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 定义了getitem可以让对象变成可迭代的</span>desk<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><blockquote><p>   Card(rank=’2’, suit=’spades’)</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python">desk<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><blockquote><p>   Card(rank=’A’, suit=’hearts’)</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> random <span class="token keyword">import</span> choicechoice<span class="token punctuation">(</span>desk<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><blockquote><p>   Card(rank=’6’, suit=’clubs’)</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python">desk<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><blockquote><p>[Card(rank=’2’, suit=’spades’),<br> Card(rank=’3’, suit=’spades’),<br> Card(rank=’4’, suit=’spades’)]</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> card <span class="token keyword">in</span> desk<span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>card<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><blockquote><p>Card(rank=’2’, suit=’spades’)<br>Card(rank=’3’, suit=’spades’)<br>Card(rank=’4’, suit=’spades’)<br>Card(rank=’5’, suit=’spades’)<br>Card(rank=’6’, suit=’spades’)<br>Card(rank=’7’, suit=’spades’)<br>Card(rank=’8’, suit=’spades’)<br>Card(rank=’9’, suit=’spades’)<br>Card(rank=’10’, suit=’spades’)<br>Card(rank=’J’, suit=’spades’)<br>Card(rank=’Q’, suit=’spades’)<br>Card(rank=’K’, suit=’spades’)<br>Card(rank=’A’, suit=’spades’)<br>Card(rank=’2’, suit=’diamonds’)<br>Card(rank=’3’, suit=’diamonds’)<br>Card(rank=’4’, suit=’diamonds’)<br>Card(rank=’5’, suit=’diamonds’)<br>Card(rank=’6’, suit=’diamonds’)<br>Card(rank=’7’, suit=’diamonds’)<br>Card(rank=’8’, suit=’diamonds’)<br>Card(rank=’9’, suit=’diamonds’)<br>Card(rank=’10’, suit=’diamonds’)<br>Card(rank=’J’, suit=’diamonds’)<br>Card(rank=’Q’, suit=’diamonds’)<br>Card(rank=’K’, suit=’diamonds’)<br>Card(rank=’A’, suit=’diamonds’)<br>Card(rank=’2’, suit=’clubs’)<br>Card(rank=’3’, suit=’clubs’)<br>Card(rank=’4’, suit=’clubs’)<br>Card(rank=’5’, suit=’clubs’)<br>Card(rank=’6’, suit=’clubs’)<br>Card(rank=’7’, suit=’clubs’)<br>Card(rank=’8’, suit=’clubs’)<br>Card(rank=’9’, suit=’clubs’)<br>Card(rank=’10’, suit=’clubs’)<br>Card(rank=’J’, suit=’clubs’)<br>Card(rank=’Q’, suit=’clubs’)<br>Card(rank=’K’, suit=’clubs’)<br>Card(rank=’A’, suit=’clubs’)<br>Card(rank=’2’, suit=’hearts’)<br>Card(rank=’3’, suit=’hearts’)<br>Card(rank=’4’, suit=’hearts’)<br>Card(rank=’5’, suit=’hearts’)<br>Card(rank=’6’, suit=’hearts’)<br>Card(rank=’7’, suit=’hearts’)<br>Card(rank=’8’, suit=’hearts’)<br>Card(rank=’9’, suit=’hearts’)<br>Card(rank=’10’, suit=’hearts’)<br>Card(rank=’J’, suit=’hearts’)<br>Card(rank=’Q’, suit=’hearts’)<br>Card(rank=’K’, suit=’hearts’)<br>Card(rank=’A’, suit=’hearts’)</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python">Card<span class="token punctuation">(</span><span class="token string">'Q'</span><span class="token punctuation">,</span> <span class="token string">'hearts'</span><span class="token punctuation">)</span> <span class="token keyword">in</span> desk<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><blockquote><p>True</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 加上排序的key</span>suits_value <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>spades<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> hearts<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> diamonds<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> clubs<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">spades_high</span><span class="token punctuation">(</span>card<span class="token punctuation">)</span><span class="token punctuation">:</span>    rank_value <span class="token operator">=</span> FrenchDesk<span class="token punctuation">.</span>ranks<span class="token punctuation">.</span>index<span class="token punctuation">(</span>card<span class="token punctuation">.</span>rank<span class="token punctuation">)</span>    <span class="token keyword">return</span> rank_value<span class="token operator">*</span><span class="token builtin">len</span><span class="token punctuation">(</span>suits_value<span class="token punctuation">)</span><span class="token operator">+</span>suits_value<span class="token punctuation">[</span>card<span class="token punctuation">.</span>suit<span class="token punctuation">]</span><span class="token keyword">for</span> card <span class="token keyword">in</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span>desk<span class="token punctuation">,</span> key<span class="token operator">=</span>spades_high<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>card<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>Card(rank=’2’, suit=’clubs’)<br>Card(rank=’2’, suit=’diamonds’)<br>Card(rank=’2’, suit=’hearts’)<br>Card(rank=’2’, suit=’spades’)<br>Card(rank=’3’, suit=’clubs’)<br>Card(rank=’3’, suit=’diamonds’)<br>Card(rank=’3’, suit=’hearts’)<br>Card(rank=’3’, suit=’spades’)<br>Card(rank=’4’, suit=’clubs’)<br>Card(rank=’4’, suit=’diamonds’)<br>Card(rank=’4’, suit=’hearts’)<br>Card(rank=’4’, suit=’spades’)<br>Card(rank=’5’, suit=’clubs’)<br>Card(rank=’5’, suit=’diamonds’)<br>Card(rank=’5’, suit=’hearts’)<br>Card(rank=’5’, suit=’spades’)<br>Card(rank=’6’, suit=’clubs’)<br>Card(rank=’6’, suit=’diamonds’)<br>Card(rank=’6’, suit=’hearts’)<br>Card(rank=’6’, suit=’spades’)<br>Card(rank=’7’, suit=’clubs’)<br>Card(rank=’7’, suit=’diamonds’)<br>Card(rank=’7’, suit=’hearts’)<br>Card(rank=’7’, suit=’spades’)<br>Card(rank=’8’, suit=’clubs’)<br>Card(rank=’8’, suit=’diamonds’)<br>Card(rank=’8’, suit=’hearts’)<br>Card(rank=’8’, suit=’spades’)<br>Card(rank=’9’, suit=’clubs’)<br>Card(rank=’9’, suit=’diamonds’)<br>Card(rank=’9’, suit=’hearts’)<br>Card(rank=’9’, suit=’spades’)<br>Card(rank=’10’, suit=’clubs’)<br>Card(rank=’10’, suit=’diamonds’)<br>Card(rank=’10’, suit=’hearts’)<br>Card(rank=’10’, suit=’spades’)<br>Card(rank=’J’, suit=’clubs’)<br>Card(rank=’J’, suit=’diamonds’)<br>Card(rank=’J’, suit=’hearts’)<br>Card(rank=’J’, suit=’spades’)<br>Card(rank=’Q’, suit=’clubs’)<br>Card(rank=’Q’, suit=’diamonds’)<br>Card(rank=’Q’, suit=’hearts’)<br>Card(rank=’Q’, suit=’spades’)<br>Card(rank=’K’, suit=’clubs’)<br>Card(rank=’K’, suit=’diamonds’)<br>Card(rank=’K’, suit=’hearts’)<br>Card(rank=’K’, suit=’spades’)<br>Card(rank=’A’, suit=’clubs’)<br>Card(rank=’A’, suit=’diamonds’)<br>Card(rank=’A’, suit=’hearts’)<br>Card(rank=’A’, suit=’spades’)</p></blockquote><h2 id="1-2-A-simple-two-dimensional-vector-class"><a href="#1-2-A-simple-two-dimensional-vector-class" class="headerlink" title="1.2 A simple two-dimensional vector class"></a>1.2 A simple two-dimensional vector class</h2><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206232143.png" alt="Special method names (operators excluded)"><br><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206232143.png" alt="Special method names for operators"></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> math <span class="token keyword">import</span> hypot<span class="token keyword">class</span> <span class="token class-name">Vector</span><span class="token punctuation">:</span>        <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>x <span class="token operator">=</span> x        self<span class="token punctuation">.</span>y <span class="token operator">=</span> y        <span class="token keyword">def</span> <span class="token function">__repr__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token string-interpolation"><span class="token string">f'Vector(</span><span class="token interpolation"><span class="token punctuation">{</span>self<span class="token punctuation">.</span>x<span class="token punctuation">}</span></span><span class="token string">, </span><span class="token interpolation"><span class="token punctuation">{</span>self<span class="token punctuation">.</span>y<span class="token punctuation">}</span></span><span class="token string">)'</span></span>        <span class="token keyword">def</span> <span class="token function">__abs__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> hypot<span class="token punctuation">(</span>self<span class="token punctuation">.</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>y<span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">__bool__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token builtin">bool</span><span class="token punctuation">(</span><span class="token builtin">abs</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">__add__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> other<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>x <span class="token operator">+</span> other<span class="token punctuation">.</span>x        y <span class="token operator">=</span> self<span class="token punctuation">.</span>y <span class="token operator">+</span> other<span class="token punctuation">.</span>y        <span class="token keyword">return</span> Vector<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">__mul__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> scalar<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> Vector<span class="token punctuation">(</span>self<span class="token punctuation">.</span>x<span class="token operator">*</span>scalar<span class="token punctuation">,</span> self<span class="token punctuation">.</span>y<span class="token operator">*</span>scalar<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python">v1 <span class="token operator">=</span> Vector<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>v2 <span class="token operator">=</span> Vector<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span>v1 <span class="token operator">+</span> v2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><blockquote><p>Vector(3, 6)</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token builtin">abs</span><span class="token punctuation">(</span>v1<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><blockquote><p>2.23606797749979</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python">v1<span class="token operator">*</span><span class="token number">3</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><blockquote><p>Vector(3, 6)</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> FluentPython </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Chapter_04_Text_versus_Bytes</title>
      <link href="2020/12/01/python/fluentpython/chapter-04-text-versus-bytes/"/>
      <url>2020/12/01/python/fluentpython/chapter-04-text-versus-bytes/</url>
      
        <content type="html"><![CDATA[<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 编码和解码</span>s <span class="token operator">=</span> <span class="token string">'caf中国'</span><span class="token keyword">print</span><span class="token punctuation">(</span>s<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">)</span>b <span class="token operator">=</span> s<span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span>s <span class="token operator">=</span> b<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>s<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>caf中国 5<br>b’caf\xe4\xb8\xad\xe5\x9b\xbd’ 9<br>caf中国 5</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python">s1 <span class="token operator">=</span> <span class="token string">'café'</span><span class="token keyword">print</span><span class="token punctuation">(</span>s1<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>s1<span class="token punctuation">)</span><span class="token punctuation">)</span>b <span class="token operator">=</span> s1<span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span>s2 <span class="token operator">=</span> <span class="token string">'café'</span><span class="token keyword">print</span><span class="token punctuation">(</span>s2<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>s2<span class="token punctuation">)</span><span class="token punctuation">)</span>b <span class="token operator">=</span> s2<span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>café 5<br>b’cafe\xcc\x81’ 6<br>café 4<br>b’caf\xc3\xa9’ 5</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> unicodedata <span class="token keyword">import</span> normalize<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>normalize<span class="token punctuation">(</span><span class="token string">'NFC'</span><span class="token punctuation">,</span>s1<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>normalize<span class="token punctuation">(</span><span class="token string">'NFC'</span><span class="token punctuation">,</span>s2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><blockquote><p>4 4</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># bytes和bytearray，bytes存储的是整数</span><span class="token comment"># bytes切片还是bytes对象，bytearray同样</span>cafe <span class="token operator">=</span> <span class="token builtin">bytes</span><span class="token punctuation">(</span><span class="token string">'café'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>cafe<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>cafe<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">for</span> b <span class="token keyword">in</span> cafe<span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span>cafe_arr <span class="token operator">=</span> <span class="token builtin">bytearray</span><span class="token punctuation">(</span>cafe<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>cafe_arr<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>cafe_arr<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>b’caf\xc3\xa9’<br>b’c’<br>99<br>97<br>102<br>195<br>169<br>bytearray(b’caf\xc3\xa9’)<br>bytearray(b’\xa9’)</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> arraynumbers<span class="token operator">=</span>array<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token string">'h'</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>octets<span class="token operator">=</span><span class="token builtin">bytes</span><span class="token punctuation">(</span>numbers<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>octets<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>b'\xfe\xff\xff\xff\x00\x00\x01\x00\x02\x00'</code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 默认编码</span><span class="token keyword">import</span> sys<span class="token punctuation">,</span> localeexpressions <span class="token operator">=</span> <span class="token triple-quoted-string string">"""        locale.getpreferredencoding()        type(my_file)        my_file.encoding        sys.stdout.isatty()        sys.stdout.encoding        sys.stdin.isatty()        sys.stdin.encoding        sys.stderr.isatty()        sys.stderr.encoding        sys.getdefaultencoding()        sys.getfilesystemencoding()"""</span>my_file <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'dummy'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span><span class="token keyword">for</span> expression <span class="token keyword">in</span> expressions<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    value <span class="token operator">=</span> <span class="token builtin">eval</span><span class="token punctuation">(</span>expression<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>expression<span class="token punctuation">.</span>rjust<span class="token punctuation">(</span><span class="token number">30</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'-&gt;'</span><span class="token punctuation">,</span> <span class="token builtin">repr</span><span class="token punctuation">(</span>value<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p> locale.getpreferredencoding() -&gt; ‘UTF-8’<br>                 type(my_file) -&gt; <class '_io.textiowrapper'=""><br>              my_file.encoding -&gt; ‘UTF-8’<br>           sys.stdout.isatty() -&gt; False<br>           sys.stdout.encoding -&gt; ‘UTF-8’<br>            sys.stdin.isatty() -&gt; False<br>            sys.stdin.encoding -&gt; ‘UTF-8’<br>           sys.stderr.isatty() -&gt; False<br>           sys.stderr.encoding -&gt; ‘UTF-8’<br>      sys.getdefaultencoding() -&gt; ‘utf-8’<br>   sys.getfilesystemencoding() -&gt; ‘utf-8’</class></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> FluentPython </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
