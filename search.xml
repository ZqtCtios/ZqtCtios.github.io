<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>02_逻辑回归_Logistic_Regression</title>
      <link href="2020/12/03/machinelearning/wu-en-da-ji-qi-xue-xi/logistic-regression/"/>
      <url>2020/12/03/machinelearning/wu-en-da-ji-qi-xue-xi/logistic-regression/</url>
      
        <content type="html"><![CDATA[<h1 id="逻辑回归（Logistic-Regression）"><a href="#逻辑回归（Logistic-Regression）" class="headerlink" title="逻辑回归（Logistic Regression）"></a>逻辑回归（Logistic Regression）</h1><h2 id="1-分类问题（Classification）"><a href="#1-分类问题（Classification）" class="headerlink" title="1.分类问题（Classification）"></a>1.分类问题（Classification）</h2><p>分类问题中，需要预测的变量y是离散的值</p><h2 id="2-假说表示"><a href="#2-假说表示" class="headerlink" title="2.假说表示"></a>2.假说表示</h2><p>逻辑回归模型的假设是: $\quad h_{\theta}(x)=g\left(\theta^{T} X\right)$ 其中: $\quad X$ 代表特征向量 $g$ 代表逻辑函数（logistic function)是一个常用的逻辑函数为 S 形函数(Sigmoid function)，公式为:</p><script type="math/tex; mode=display">g(z)=\frac{1}{1+e^{-z}}</script><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np <span class="token keyword">def</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>z<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">return</span> <span class="token number">1</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>z<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>$h_{\theta}(x)$ 的作用是，对于给定的输入变量，根据选择的参数计算输出变量=1 的可能性(estimated probablity) 即 $h_{\theta}(x)=P(y=1 \mid x ; \theta)$<br>例如，如果对于给定的 $x,$ 通过已经确定的参数计算得出 $h_{\theta}(x)=0.7,$ 则表示有 $70 \%$ 的几率 $y$ 为正向类，相应地 $y$ 为负向类的几率为 1-0.7=0.3。</p><h2 id="3-判定边界Decision-Boundary"><a href="#3-判定边界Decision-Boundary" class="headerlink" title="3.判定边界Decision Boundary"></a>3.判定边界Decision Boundary</h2><p>现在假设我们有一个模型:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201205224807.png" alt="image-20201205224807126"></p><p>可以选直线：</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201205224836.png" alt="image-20201205224836663"></p><p>但其他的比如边界的圆形：</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201205224903.png" alt="image-20201205224903704"></p><p>我们可以用非常复杂的模型来适应非常复杂形状的判定边界</p><h2 id="4-代价函数Cost-Function"><a href="#4-代价函数Cost-Function" class="headerlink" title="4.代价函数Cost Function"></a>4.代价函数Cost Function</h2><p>如何拟合逻辑回归模型的参数𝜃。具体来说，拟合参数的优化目标或者叫代价函数，这便是监督学习问题中的逻辑回归模型的拟合问题。</p><p>对于线性回归模型，我们定义的代价函数是所有模型误差的平方和。理论上来说，我们也可以对逻辑回归模型沿用这个定义，但是问题在于，当我们将 $h_{\theta}(x)=\frac{1}{1+e^{-\theta} T_{X}}$ 带入到这样定义了的代价函数中时，我们得到的代价函数将是一个非凸函数（non-convexfunction），这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201205225126.png" alt="image-20201205225126378"></p><p>线性回归的代价函数为:</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{m} \sum_{i=1}^{m} \frac{1}{2}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}</script><p>我们重新定义逻辑回归的代价函数为:</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{m} \sum_{i=1}^{m} \operatorname{cost}\left(h_{\theta}\left(x^{(i)}\right), y^{(i)}\right)</script><p>其中，</p><script type="math/tex; mode=display">\operatorname{cost}\left(h_{\theta}(x), y\right)=\left\{\begin{aligned}-\log \left(h_{\theta}(x)\right) & \text { if } y=1 \\-\log \left(1-h_{\theta}(x)\right) & \text { if } y=0\end{aligned}\right.</script><p>$h_{\theta}(x)$ 与 $\operatorname{cost}\left(h_{\theta}(x), y\right)$ 之间的关系如下图所示:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201205225317.png" alt="image-20201205225317462"></p><p>这样构建的 $\operatorname{cost}\left(h_{\theta}(x), y\right)$ 函数的特点是：当实际的 $y=1$ 且 $h_{\theta}(x)$ 也为 1 时误差为 0 ,当 $y=1$ 但 $h_{\theta}(x)$ 不为 1 时误差随着 $h_{\theta}(x)$ 变小而变大; 当实际的 $y=0$ 且 $h_{\theta}(x)$ 也为 0 时代价为 $0,$ 当 $y=0$ 但 $h_{\theta}(x)$ 不为 0 时误差随着 $h_{\theta}(x)$ 的变大而变大。将构建的 $\operatorname{cost}\left(h_{\theta}(x), y\right)$ 简化如下:</p><script type="math/tex; mode=display">\operatorname{cost}\left(h_{\theta}(x), y\right)=-y \times \log \left(h_{\theta}(x)\right)-(1-y) \times \log \left(1-h_{\theta}(x)\right)</script><p>带入代价函数得到:</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{m} \sum_{i=1}^{m}\left[-y^{(i)} \log \left(h_{\theta}\left(x^{(i)}\right)\right)-\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]</script><p>即: </p><script type="math/tex; mode=display">J(\theta)=-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)} \log \left(h_{\theta}\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]</script><p>Python代码实现为：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np <span class="token keyword">def</span> <span class="token function">cost</span><span class="token punctuation">(</span>theta<span class="token punctuation">,</span> X<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>  theta <span class="token operator">=</span> np<span class="token punctuation">.</span>matrix<span class="token punctuation">(</span>theta<span class="token punctuation">)</span>  X <span class="token operator">=</span> np<span class="token punctuation">.</span>matrix<span class="token punctuation">(</span>X<span class="token punctuation">)</span>  y <span class="token operator">=</span> np<span class="token punctuation">.</span>matrix<span class="token punctuation">(</span>y<span class="token punctuation">)</span>  first <span class="token operator">=</span> np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span><span class="token operator">-</span>y<span class="token punctuation">,</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>sigmoid<span class="token punctuation">(</span>X<span class="token operator">*</span> theta<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  second <span class="token operator">=</span> np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> y<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> sigmoid<span class="token punctuation">(</span>X<span class="token operator">*</span> theta<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">return</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>first <span class="token operator">-</span> second<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在得到这样一个代价函数以后，我们便可以用梯度下降算法来求得能使代价函数最小的参数了。算法为:</p><script type="math/tex; mode=display">\theta_{j}:=\theta_{j}-\alpha \frac{\partial}{\partial \theta_{j}} J(\theta)</script><p>求导后：</p><script type="math/tex; mode=display">\theta_{j}:=\theta_{j}-\alpha \frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(\mathrm{x}^{(i)}\right)-\mathrm{y}^{(i)}\right) \mathrm{x}_{j}^{(i)}</script><p>代价函数𝐽(𝜃)会是一 个凸函数，并且没有局部最优值。推导过程:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201205230522.png" alt="image-20201205230522918"></p><p>所以：</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201205230500.png" alt="image-20201205230500427"></p><p>虽然得到的梯度下降算法表面上看上去与线性回归的梯度下降算法一样，但是这里的$h_𝜃(𝑥) = 𝑔(𝜃^𝑇𝑋)$与线性回归中不同，所以实际上是不一样的。另外，在运行梯度下降算法 之前，进行特征缩放依旧是非常必要的。一些梯度下降算法之外的选择: 除了梯度下降算法以外，还有一些常被用来令代价函数最小的算法，这些算法更加复杂和优越，而且通常不需要人工选择学习率，通常比梯度下降算法要更加快速。这些算法有:共轭梯度（Conjugate Gradient），局部优化法（Broyden fletcher goldfarb shann,BFGS）和有限内存局部优化法（LBFGS）</p><h2 id="5-梯度下降"><a href="#5-梯度下降" class="headerlink" title="5.梯度下降"></a>5.梯度下降</h2><p>和之前的线性回归类似:</p><script type="math/tex; mode=display">\theta_{j}:=\theta_{j}-\alpha \frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}</script><p>但是现在逻辑函数假设函数: $h_{\theta}(x)=\frac{1}{1+e^{-\theta} T_{X}}$，即使更新参数的规则看起来基本相同，但由于假设的定义发生了变化，所以逻辑函数的梯度下降，跟线性回归的梯度下降实际上是两个完全不同的东西。</p><h2 id="6-高级优化"><a href="#6-高级优化" class="headerlink" title="6.高级优化"></a>6.高级优化</h2><p>然而梯度下降并不是我们可以使用的唯一算法，还有其他一些算法，更高级、更复杂。如果我们能用这些方法来计算代价函数 $J(\theta)$ 和偏导数项 $\frac{\partial}{\partial \theta_{j}} J(\theta)$ 两个项的话，那么这些算法就是为我们优化代价函数的不同方法，共轩梯度法 BFGS （变尺度法）和 L-BFGS （限制变尺度法）就是其中一些更高级的优化算法，它们需要有一种方法来计算 𝐽(𝜃)，以及需要一种方法 计算导数项，然后使用比梯度下降更复杂的算法来最小化代价函数。</p><h2 id="7-多类别分类-一对多"><a href="#7-多类别分类-一对多" class="headerlink" title="7.多类别分类:一对多"></a>7.多类别分类:一对多</h2><p>然而对于之前的一个，二元分类问题，我们的数据看起来可能是像这样:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206000542.png" alt="image-20201206000542702"></p><p>对于一个多类分类问题，我们的数据集或许看起来像这样:</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206000556.png" alt="image-20201206000556885"></p><p>我们现在已经知道如何进行二元分类，可以使用逻辑回归，对于直线或许你也知道，可以将数据集一分为二为正类和负类。用一对多的分类思想，我们可以将其用在多类分类问题 上。下面将介绍如何进行一对多的分类工作，有时这个方法也被称为”一对余”方法。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206000949.png" alt="image-20201206000949232"></p><p>方法就是将多分类问题转化成多个二分类问题，例如上图先把除了三角形的全视为一类，转化成一个二分类问题。为了能实现这样的转变，我们将多个类中的一个类标记为正向类（ $y=1$） ，然后将其他所有类都标记为负向类，这个模型记作 $h_{\theta}^{(1)}(x)_{\circ}$ 接着，类似地第我们选择另一个类标记为正向类 $(y=2)$ ，再将其它类都标记为负向类，将这个模型记作 $h_{\theta}^{(2)}(x)$,依此类推。最后我们得到一系列的模型简记为: $\quad h_{\theta}^{(i)}(x)=p(y=i \mid x ; \theta)$ 其中: $i=(1,2,3 \ldots k)$</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201206001229.png" alt="image-20201206001229747"></p><p>最后，将所有的分类机都运行一遍，然后对每一个输入变量， 都选择最高可能性的输出变量。</p>]]></content>
      
      
      <categories>
          
          <category> 吴恩达——机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 吴恩达 </tag>
            
            <tag> 逻辑回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>01_线性回归_Linear_Regression</title>
      <link href="2020/12/02/machinelearning/wu-en-da-ji-qi-xue-xi/linear-regression/"/>
      <url>2020/12/02/machinelearning/wu-en-da-ji-qi-xue-xi/linear-regression/</url>
      
        <content type="html"><![CDATA[<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>回归本质上是利用离散的值来试着推测出一个连续值的结果，对其他值进行预测</p><h2 id="1-单变量线性回归（Linear-Regression-with-One-Variable）"><a href="#1-单变量线性回归（Linear-Regression-with-One-Variable）" class="headerlink" title="1 单变量线性回归（Linear Regression with One Variable）"></a>1 单变量线性回归（Linear Regression with One Variable）</h2><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201202232210.png" alt="image-20201202232210402"></p><h3 id="1-1-例子房价预测问题："><a href="#1-1-例子房价预测问题：" class="headerlink" title="1.1 例子房价预测问题："></a>1.1 例子房价预测问题：</h3><p>假使回归问题的训练集(Training Set)如下表所示</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201202232330.png" alt="image-20201202232330802"></p><p>我们将要用来描述这个回归问题的标记如下: </p><ul><li><p>$𝑚$代表训练集中实例的数量</p></li><li><p>$𝑥$ 代表特征/输入变量</p></li><li>$𝑦$ 代表目标变量/输出变量</li><li>$(𝑥,𝑦)$ 代表训练集中的实例</li><li>$(𝑥^{(𝑖)},𝑦^{(𝑖)})$代表第$𝑖$ 个观察实例</li><li>$h$ 代表学习算法的解决方案或函数也称为假设(hypothesis)</li></ul><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201202232830.png" alt="image-20201202232830677"></p><p>我们该如何表达 h?</p><p>一种可能的表达方式为:$h_𝜃(𝑥) = 𝜃_0 + 𝜃_1𝑥$，因为只含有一个特征/输入变量，因此这样 的问题叫作单变量线性回归问题。</p><h3 id="1-2-代价函数"><a href="#1-2-代价函数" class="headerlink" title="1.2 代价函数"></a>1.2 代价函数</h3><h4 id="误差定义："><a href="#误差定义：" class="headerlink" title="误差定义："></a>误差定义：</h4><p>预测值与实际值的的差距。$h_𝜃(x)-y$</p><h4 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h4><p>对于$h_𝜃(𝑥) = 𝜃_0 + 𝜃_1𝑥$，目的是找到两个变量$𝜃_0,𝜃_1$来使得整体误差的值最小，即误差平方和最小，代价公式为：</p><script type="math/tex; mode=display">MSE=J\left(\theta_{0}, \theta_{1}\right)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}</script><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201202233929.png" alt="image-20201202233929481"></p><h3 id="1-3-梯度下降"><a href="#1-3-梯度下降" class="headerlink" title="1.3 梯度下降"></a>1.3 梯度下降</h3><p>批量梯度下降(batch gradient descent):</p><script type="math/tex; mode=display">\begin{aligned}&\text { repeat until convergence \{}\\&\theta_{j}:=\theta_{j}-\alpha \frac{\partial}{\partial \theta_{j}} J\left(\theta_{0}, \theta_{1}\right) \quad(\text { for } j=0 \text { and } j=1)\\&\text{\}}\end{aligned}</script><p>其中$𝑎$是学习率(learning rate)，它决定了我们沿着能让代价函数下降程度最大的方向 向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数，参数更新的步骤如下</p><script type="math/tex; mode=display">\begin{array}{l}\text { temp } 0:=\theta_{0}-\alpha \frac{\partial}{\partial \theta_{0}} J\left(\theta_{0}, \theta_{1}\right) \\\text { templ }:=\theta_{1}-\alpha \frac{\partial}{\partial \theta_{1}} J\left(\theta_{0}, \theta_{1}\right) \\\theta_{0}:=\text { temp } 0 \\\theta_{1}:=\text { temp } 1\end{array}</script><ul><li>$a$太小收敛速度太慢</li><li>$a$太大可能不收敛</li></ul><h3 id="1-4-梯度下降的线性回归"><a href="#1-4-梯度下降的线性回归" class="headerlink" title="1.4 梯度下降的线性回归"></a>1.4 梯度下降的线性回归</h3><script type="math/tex; mode=display">\begin{array}{c}\frac{\partial}{\partial \theta_{j}} J\left(\theta_{0}, \theta_{1}\right)=\frac{\partial}{\partial \theta_{j}} \frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2} \\j=0 \text { 时: } \frac{\partial}{\partial \theta_{0}} J\left(\theta_{0}, \theta_{1}\right)=\frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) \\j=1 \text { 时: } \quad \frac{\partial}{\partial \theta_{1}} J\left(\theta_{0}, \theta_{1}\right)=\frac{1}{m} \sum_{i=1}^{m}\left(\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) \cdot x^{(i)}\right)\end{array}</script><h2 id="2-多变量线性回归-Linear-Regression-with-Multiple-Variables"><a href="#2-多变量线性回归-Linear-Regression-with-Multiple-Variables" class="headerlink" title="2 多变量线性回归(Linear Regression with Multiple Variables)"></a>2 多变量线性回归(Linear Regression with Multiple Variables)</h2><p>对上面的模型增加更多的特征，模型中的特征为$(x_1,x_2,….,x_n)$</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201203163630.png" alt="image-20201203150333545"></p><p>增添更多特征后，我们引入一系列新的注释:</p><ul><li><p>$𝑛$ 代表特征的数量</p></li><li><p>$𝑥^{(𝑖)}$代表第 $𝑖$ 个训练实例，是特征矩阵中的第$𝑖$行，是一个向量(vector)。</p></li><li><p>比方说，上图的</p><script type="math/tex; mode=display">x^{(2)}=\left[\begin{array}{c}1416 \\3 \\2 \\40\end{array}\right]</script></li><li><p>$𝑥^{(𝑖)}_j$代表特征矩阵中第$ 𝑖$ 行的第$𝑗$个特征，也就是第 𝑖 个训练实例的第 𝑗 个特征。</p></li><li><p>如上图的$\mid x_{2}^{(2)}=3, x_{3}^{(2)}=2$</p></li><li><p>支持多变量的假设 h 表示为:$h_{\theta}(x)=\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}+\ldots+\theta_{n} x_{n}$</p></li><li><p>这个公式中有𝑛 + 1个参数和𝑛个变量，为了使得公式能够简化一些，引入$𝑥_0 = 1$，则公 式转化为:$h_{\theta}(x)=\theta_{0} x_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}+\ldots+\theta_{n} x_{n}$</p></li><li><p>此时模型中的参数是一个$𝑛 + 1$维的向量，任何一个训练实例也都是𝑛 + 1维的向量，特 征矩阵𝑋的维度是 $𝑚 ∗ (𝑛 + 1)$。 因此公式可以简化为:$h_𝜃(𝑥) = 𝜃^𝑇𝑋$，其中上标𝑇代表矩阵转置。</p></li></ul><h3 id="2-1-多变量梯度下降Gradient-Descent-for-Multiple-Variables"><a href="#2-1-多变量梯度下降Gradient-Descent-for-Multiple-Variables" class="headerlink" title="2.1 多变量梯度下降Gradient Descent for Multiple Variables"></a>2.1 多变量梯度下降Gradient Descent for Multiple Variables</h3><p>与单变量线性回归类似，在多变量线性回归中，我们也构建一个代价函数，则这个代价 函数是所有建模误差的平方和，即:</p><script type="math/tex; mode=display">J\left(\theta_{0}, \theta_{1} \ldots \theta_{n}\right)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}</script><p>梯度下降算法求导后：</p><p>Repeat：</p><script type="math/tex; mode=display">\theta_{j}:=\theta_{j}-\alpha \frac{1}{m} \sum_{i=1}^{m}\left(\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) \cdot x_{j}^{(i)}\right)</script><p>( simultaneously update  $\theta_{j}$$ \text { for } \mathrm{j}=0,1, \ldots, \mathrm{n})$</p><p>计算Cost函数Python代码：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">computeCost</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> theta<span class="token punctuation">)</span><span class="token punctuation">:</span>  inner <span class="token operator">=</span> np<span class="token punctuation">.</span>power<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">(</span>X <span class="token operator">*</span> theta<span class="token punctuation">.</span>T<span class="token punctuation">)</span> <span class="token operator">-</span> y<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>   <span class="token keyword">return</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>inner<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="2-2-梯度下降法实践-1-特征缩放"><a href="#2-2-梯度下降法实践-1-特征缩放" class="headerlink" title="2.2 梯度下降法实践 1-特征缩放"></a>2.2 梯度下降法实践 1-特征缩放</h3><p>Gradient Descent in Practice I - Feature Scaling</p><p>在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛，解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201203153342.png" alt="image-20201203153342751"></p><p>最简单的方法是令:$x_{n}=\frac{x_{n}-\mu_{n}}{s_{n}},$ 其中 $\mu_{n}$ 是平均值, $s_{n}$ 是标准差。</p><h3 id="2-3-梯度下降法实践-2-学习率"><a href="#2-3-梯度下降法实践-2-学习率" class="headerlink" title="2.3 梯度下降法实践 2-学习率"></a>2.3 梯度下降法实践 2-学习率</h3><p>Gradient Descent in Practice II - Learning Rate</p><p>梯度下降算法的每次迭代受到学习率的影响，如果学习率𝑎过小，则达到收敛所需的迭 代次数会非常高;如果学习率𝑎过大，每次迭代可能不会减小代价函数，可能会越过局部最 小值导致无法收敛。</p><h3 id="2-4-特征和多项式回归"><a href="#2-4-特征和多项式回归" class="headerlink" title="2.4 特征和多项式回归"></a>2.4 特征和多项式回归</h3><p>线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，比如一个二次方模型: $h_{\theta}(x)=\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}^{2}$<br>或者三次方模型: $\quad h_{\theta}(x)=\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}^{2}+\theta_{3} x_{3}^{3}$</p><p><img src="https://cdn.jsdelivr.net/gh/ZqtCtios/Image/img/20201203155639.png" alt="image-20201203155639868"></p><p>通常我们需要先观察数据然后再决定准备尝试怎样的模型。 另外，我们可以令：$x_{2}=x_{2}^{2}, x_{3}=x_{3}^{3},$ 从而将模型转化为线性回归模型。</p><p>从而将模型转化为线性回归模型。</p><h3 id="2-5-正规方程"><a href="#2-5-正规方程" class="headerlink" title="2.5 正规方程"></a>2.5 正规方程</h3><p>正规方程是通过求解下面的方程来找出使得代价函数最小的参数的 $： \frac{\partial}{\partial \theta_{j}} J\left(\theta_{j}\right)=0$，假设我们的训练集特征矩阵为 X（包含了 $x_{0}=1$ ）并且我们的训练集结果为向量 y，则利用正规方程解出向量 $\theta=\left(X^{T} X\right)^{-1} X^{T} y$ 。上标 T 代表矩阵转置，上标-1 代表矩阵的逆。设矩阵 $A=X^{T} X,$ 则: $\left(X^{T} X\right)^{-1}=A^{-1}$</p><p> 梯度下降与正规方程的比较:</p><div class="table-container"><table><thead><tr><th style="text-align:left">梯度下降</th><th>正规方程</th></tr></thead><tbody><tr><td style="text-align:left">需要选择学习率𝛼</td><td>不需要</td></tr><tr><td style="text-align:left">需要多次迭代</td><td>一次运算得出</td></tr><tr><td style="text-align:left">当特征数量𝑛大时也能较好适用</td><td>需要计算 $\left(X^{T} X\right)^{-1}$ 如果特征数量 $n$ 较大则运算代价大, 因为矩阵逆的计算时间复杂度 为 $O\left(n^{3}\right),$ 通常来说当n小于10000 时还是 可以接受的</td></tr><tr><td style="text-align:left">适用于各种类型的模型</td><td>只适用于线性模型，不适合逻辑回归模型等 其他模型</td></tr></tbody></table></div><p>正规方程的python实现：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np <span class="token keyword">def</span> <span class="token function">normalEqn</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>  theta <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>inv<span class="token punctuation">(</span>X<span class="token punctuation">.</span>T@X<span class="token punctuation">)</span>@X<span class="token punctuation">.</span>T@y <span class="token comment">#X.T@X 等价于 X.T.dot(X) </span>  <span class="token keyword">return</span> theta<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 吴恩达——机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 吴恩达 </tag>
            
            <tag> 线性回归 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
